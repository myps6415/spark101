#!/usr/bin/env python3
"""
å³æ™‚ç›£æ§ç³»çµ±

é€™æ˜¯ä¸€å€‹åŸºæ–¼ Apache Spark çš„å³æ™‚ç›£æ§ç³»çµ±ï¼Œç”¨æ–¼ç›£æ§ç³»çµ±æŒ‡æ¨™ã€æª¢æ¸¬ç•°å¸¸ä¸¦ç™¼é€è­¦å ±ã€‚

åŠŸèƒ½ç‰¹è‰²ï¼š
- å³æ™‚ç³»çµ±æŒ‡æ¨™ç›£æ§
- é–¾å€¼ç›£æ§å’Œè­¦å ±
- ç•°å¸¸æª¢æ¸¬å’Œé æ¸¬
- æ­·å²è¶¨å‹¢åˆ†æ
- è‡ªå‹•åŒ–å ±å‘Šç”Ÿæˆ

ä½¿ç”¨æ–¹æ³•ï¼š
    python monitoring_system.py --config config.json --output-path /path/to/output
"""

import argparse
import sys
import os
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
from dataclasses import dataclass
from threading import Thread
import logging

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np


@dataclass
class AlertConfig:
    """è­¦å ±é…ç½®"""
    cpu_threshold: float = 80.0
    memory_threshold: float = 85.0
    disk_threshold: float = 90.0
    network_latency_threshold: float = 100.0
    error_rate_threshold: float = 5.0
    response_time_threshold: float = 1000.0
    request_rate_threshold: int = 1000


@dataclass
class MonitoringConfig:
    """ç›£æ§é…ç½®"""
    input_path: str
    output_path: str
    alert_config: AlertConfig
    monitoring_interval: int = 60  # ç§’
    retention_days: int = 30
    enable_ml_detection: bool = True
    email_alerts: bool = False
    email_recipients: List[str] = None


class MetricsCollector:
    """æŒ‡æ¨™æ”¶é›†å™¨"""
    
    def __init__(self, spark: SparkSession):
        self.spark = spark
        self.schema = StructType([
            StructField("timestamp", TimestampType(), True),
            StructField("server_id", StringType(), True),
            StructField("service_name", StringType(), True),
            StructField("cpu_usage", DoubleType(), True),
            StructField("memory_usage", DoubleType(), True),
            StructField("disk_usage", DoubleType(), True),
            StructField("network_latency", DoubleType(), True),
            StructField("error_rate", DoubleType(), True),
            StructField("request_count", IntegerType(), True),
            StructField("response_time", DoubleType(), True),
            StructField("active_connections", IntegerType(), True),
            StructField("thread_count", IntegerType(), True),
            StructField("heap_usage", DoubleType(), True)
        ])
    
    def load_metrics(self, input_path: str) -> DataFrame:
        """
        è¼‰å…¥æŒ‡æ¨™è³‡æ–™
        
        Args:
            input_path: æŒ‡æ¨™è³‡æ–™è·¯å¾‘
            
        Returns:
            DataFrame: æŒ‡æ¨™è³‡æ–™
        """
        try:
            # æ”¯æ´å¤šç¨®æ ¼å¼
            if input_path.endswith('.json'):
                df = self.spark.read.json(input_path)
            elif input_path.endswith('.csv'):
                df = self.spark.read.csv(input_path, header=True, inferSchema=True)
            elif input_path.endswith('.parquet'):
                df = self.spark.read.parquet(input_path)
            else:
                # é è¨­å˜—è©¦ JSON æ ¼å¼
                df = self.spark.read.json(input_path)
            
            # ç¢ºä¿è³‡æ–™æ ¼å¼æ­£ç¢º
            df = df.withColumn("timestamp", 
                              when(col("timestamp").isNotNull(), col("timestamp"))
                              .otherwise(current_timestamp()))
            
            return df
        
        except Exception as e:
            logging.error(f"è¼‰å…¥æŒ‡æ¨™è³‡æ–™å¤±æ•—: {str(e)}")
            return None
    
    def generate_sample_metrics(self, num_records: int = 10000) -> DataFrame:
        """
        ç”Ÿæˆç¯„ä¾‹æŒ‡æ¨™è³‡æ–™
        
        Args:
            num_records: è¨˜éŒ„æ•¸é‡
            
        Returns:
            DataFrame: ç”Ÿæˆçš„æŒ‡æ¨™è³‡æ–™
        """
        import random
        
        print(f"ç”Ÿæˆ {num_records} æ¢ç¯„ä¾‹æŒ‡æ¨™è³‡æ–™...")
        
        servers = [f"server-{i:03d}" for i in range(1, 21)]
        services = ["web", "api", "database", "cache", "queue", "worker"]
        
        metrics = []
        base_time = datetime.now() - timedelta(hours=24)
        
        for i in range(num_records):
            timestamp = base_time + timedelta(seconds=i * 30)
            server_id = random.choice(servers)
            service_name = random.choice(services)
            
            # æ ¹æ“šæœå‹™é¡å‹ç”Ÿæˆä¸åŒçš„åŸºæº–å€¼
            if service_name == "database":
                cpu_base = 60 + random.gauss(0, 10)
                memory_base = 70 + random.gauss(0, 10)
                response_time_base = 50 + random.gauss(0, 20)
            elif service_name == "cache":
                cpu_base = 20 + random.gauss(0, 5)
                memory_base = 80 + random.gauss(0, 10)
                response_time_base = 5 + random.gauss(0, 2)
            elif service_name == "web":
                cpu_base = 30 + random.gauss(0, 10)
                memory_base = 40 + random.gauss(0, 10)
                response_time_base = 100 + random.gauss(0, 30)
            else:
                cpu_base = 40 + random.gauss(0, 10)
                memory_base = 50 + random.gauss(0, 10)
                response_time_base = 80 + random.gauss(0, 25)
            
            # æ¨¡æ“¬ç•°å¸¸æƒ…æ³
            is_anomaly = random.random() < 0.1
            if is_anomaly:
                cpu_usage = min(100, max(0, cpu_base + random.gauss(30, 10)))
                memory_usage = min(100, max(0, memory_base + random.gauss(25, 10)))
                response_time = max(0, response_time_base + random.gauss(200, 50))
                error_rate = max(0, random.gauss(8, 3))
            else:
                cpu_usage = min(100, max(0, cpu_base))
                memory_usage = min(100, max(0, memory_base))
                response_time = max(0, response_time_base)
                error_rate = max(0, random.gauss(1, 0.5))
            
            disk_usage = min(100, max(0, random.gauss(65, 15)))
            network_latency = max(0, random.gauss(25, 10))
            request_count = max(0, int(random.gauss(100, 30)))
            active_connections = max(0, int(random.gauss(50, 15)))
            thread_count = max(0, int(random.gauss(20, 5)))
            heap_usage = min(100, max(0, random.gauss(45, 15)))
            
            metrics.append((
                timestamp,
                server_id,
                service_name,
                cpu_usage,
                memory_usage,
                disk_usage,
                network_latency,
                error_rate,
                request_count,
                response_time,
                active_connections,
                thread_count,
                heap_usage
            ))
        
        df = self.spark.createDataFrame(metrics, self.schema)
        print(f"æˆåŠŸç”Ÿæˆ {df.count()} æ¢æŒ‡æ¨™è¨˜éŒ„")
        return df


class AlertManager:
    """è­¦å ±ç®¡ç†å™¨"""
    
    def __init__(self, config: AlertConfig):
        self.config = config
        self.alert_history = []
    
    def check_thresholds(self, metrics_df: DataFrame) -> DataFrame:
        """
        æª¢æŸ¥é–¾å€¼ä¸¦ç”Ÿæˆè­¦å ±
        
        Args:
            metrics_df: æŒ‡æ¨™è³‡æ–™
            
        Returns:
            DataFrame: è§¸ç™¼çš„è­¦å ±
        """
        alerts = []
        
        # CPU ä½¿ç”¨ç‡è­¦å ±
        cpu_alerts = metrics_df.filter(
            col("cpu_usage") > self.config.cpu_threshold
        ).withColumn("alert_type", lit("cpu_high")) \
         .withColumn("alert_message", 
                    concat(lit("CPU ä½¿ç”¨ç‡éé«˜: "), col("cpu_usage"), lit("%"))) \
         .withColumn("severity", 
                    when(col("cpu_usage") > 95, "critical")
                    .when(col("cpu_usage") > 90, "high")
                    .otherwise("medium"))
        
        # è¨˜æ†¶é«”ä½¿ç”¨ç‡è­¦å ±
        memory_alerts = metrics_df.filter(
            col("memory_usage") > self.config.memory_threshold
        ).withColumn("alert_type", lit("memory_high")) \
         .withColumn("alert_message", 
                    concat(lit("è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜: "), col("memory_usage"), lit("%"))) \
         .withColumn("severity", 
                    when(col("memory_usage") > 95, "critical")
                    .when(col("memory_usage") > 90, "high")
                    .otherwise("medium"))
        
        # ç£ç¢Ÿä½¿ç”¨ç‡è­¦å ±
        disk_alerts = metrics_df.filter(
            col("disk_usage") > self.config.disk_threshold
        ).withColumn("alert_type", lit("disk_high")) \
         .withColumn("alert_message", 
                    concat(lit("ç£ç¢Ÿä½¿ç”¨ç‡éé«˜: "), col("disk_usage"), lit("%"))) \
         .withColumn("severity", 
                    when(col("disk_usage") > 98, "critical")
                    .when(col("disk_usage") > 95, "high")
                    .otherwise("medium"))
        
        # ç¶²è·¯å»¶é²è­¦å ±
        latency_alerts = metrics_df.filter(
            col("network_latency") > self.config.network_latency_threshold
        ).withColumn("alert_type", lit("latency_high")) \
         .withColumn("alert_message", 
                    concat(lit("ç¶²è·¯å»¶é²éé«˜: "), col("network_latency"), lit("ms"))) \
         .withColumn("severity", 
                    when(col("network_latency") > 500, "critical")
                    .when(col("network_latency") > 200, "high")
                    .otherwise("medium"))
        
        # éŒ¯èª¤ç‡è­¦å ±
        error_alerts = metrics_df.filter(
            col("error_rate") > self.config.error_rate_threshold
        ).withColumn("alert_type", lit("error_rate_high")) \
         .withColumn("alert_message", 
                    concat(lit("éŒ¯èª¤ç‡éé«˜: "), col("error_rate"), lit("%"))) \
         .withColumn("severity", 
                    when(col("error_rate") > 20, "critical")
                    .when(col("error_rate") > 10, "high")
                    .otherwise("medium"))
        
        # éŸ¿æ‡‰æ™‚é–“è­¦å ±
        response_time_alerts = metrics_df.filter(
            col("response_time") > self.config.response_time_threshold
        ).withColumn("alert_type", lit("response_time_high")) \
         .withColumn("alert_message", 
                    concat(lit("éŸ¿æ‡‰æ™‚é–“éé«˜: "), col("response_time"), lit("ms"))) \
         .withColumn("severity", 
                    when(col("response_time") > 5000, "critical")
                    .when(col("response_time") > 2000, "high")
                    .otherwise("medium"))
        
        # åˆä½µæ‰€æœ‰è­¦å ±
        all_alerts = cpu_alerts.union(memory_alerts) \
                              .union(disk_alerts) \
                              .union(latency_alerts) \
                              .union(error_alerts) \
                              .union(response_time_alerts)
        
        # æ·»åŠ è­¦å ± ID å’Œæ™‚é–“æˆ³
        all_alerts = all_alerts.withColumn("alert_id", 
                                         concat(col("server_id"), lit("_"), 
                                               col("alert_type"), lit("_"), 
                                               col("timestamp").cast("string"))) \
                               .withColumn("alert_timestamp", current_timestamp())
        
        return all_alerts
    
    def process_alerts(self, alerts_df: DataFrame) -> Dict[str, int]:
        """
        è™•ç†è­¦å ±
        
        Args:
            alerts_df: è­¦å ±è³‡æ–™
            
        Returns:
            Dict[str, int]: è­¦å ±çµ±è¨ˆ
        """
        alert_counts = {}
        
        if alerts_df.count() > 0:
            # æŒ‰åš´é‡ç¨‹åº¦çµ±è¨ˆ
            severity_counts = alerts_df.groupBy("severity").count().collect()
            for row in severity_counts:
                alert_counts[row['severity']] = row['count']
            
            # æŒ‰é¡å‹çµ±è¨ˆ
            type_counts = alerts_df.groupBy("alert_type").count().collect()
            for row in type_counts:
                alert_counts[row['alert_type']] = row['count']
            
            # è¨˜éŒ„è­¦å ±æ­·å²
            self.alert_history.append({
                'timestamp': datetime.now(),
                'total_alerts': alerts_df.count(),
                'severity_breakdown': dict(severity_counts),
                'type_breakdown': dict(type_counts)
            })
        
        return alert_counts
    
    def send_alert_notification(self, alert_summary: Dict[str, int]) -> None:
        """
        ç™¼é€è­¦å ±é€šçŸ¥
        
        Args:
            alert_summary: è­¦å ±æ‘˜è¦
        """
        # é€™è£¡å¯ä»¥é›†æˆå¯¦éš›çš„é€šçŸ¥æœå‹™
        # å¦‚ Email, Slack, PagerDuty ç­‰
        
        if alert_summary:
            print(f"ğŸš¨ è­¦å ±é€šçŸ¥: {alert_summary}")
            
            # ç¯„ä¾‹ï¼šç™¼é€ Email è­¦å ±
            # if self.config.email_alerts:
            #     self.send_email_alert(alert_summary)


class AnomalyDetector:
    """ç•°å¸¸æª¢æ¸¬å™¨"""
    
    def __init__(self, spark: SparkSession):
        self.spark = spark
        self.models = {}
    
    def detect_statistical_anomalies(self, metrics_df: DataFrame) -> DataFrame:
        """
        ä½¿ç”¨çµ±è¨ˆæ–¹æ³•æª¢æ¸¬ç•°å¸¸
        
        Args:
            metrics_df: æŒ‡æ¨™è³‡æ–™
            
        Returns:
            DataFrame: ç•°å¸¸æª¢æ¸¬çµæœ
        """
        print("æ­£åœ¨é€²è¡Œçµ±è¨ˆç•°å¸¸æª¢æ¸¬...")
        
        # è¨ˆç®—ç§»å‹•å¹³å‡å’Œæ¨™æº–å·®
        window_spec = Window.partitionBy("server_id", "service_name") \
                           .orderBy("timestamp") \
                           .rowsBetween(-5, 0)
        
        with_stats = metrics_df.withColumn(
            "cpu_moving_avg", avg("cpu_usage").over(window_spec)
        ).withColumn(
            "cpu_moving_std", stddev("cpu_usage").over(window_spec)
        ).withColumn(
            "memory_moving_avg", avg("memory_usage").over(window_spec)
        ).withColumn(
            "memory_moving_std", stddev("memory_usage").over(window_spec)
        ).withColumn(
            "response_time_moving_avg", avg("response_time").over(window_spec)
        ).withColumn(
            "response_time_moving_std", stddev("response_time").over(window_spec)
        )
        
        # æª¢æ¸¬ç•°å¸¸ï¼ˆè¶…é 2 å€‹æ¨™æº–å·®ï¼‰
        anomalies = with_stats.filter(
            (abs(col("cpu_usage") - col("cpu_moving_avg")) > 2 * col("cpu_moving_std")) |
            (abs(col("memory_usage") - col("memory_moving_avg")) > 2 * col("memory_moving_std")) |
            (abs(col("response_time") - col("response_time_moving_avg")) > 2 * col("response_time_moving_std"))
        ).withColumn("anomaly_type", lit("statistical")) \
         .withColumn("anomaly_score", 
                    greatest(
                        abs(col("cpu_usage") - col("cpu_moving_avg")) / col("cpu_moving_std"),
                        abs(col("memory_usage") - col("memory_moving_avg")) / col("memory_moving_std"),
                        abs(col("response_time") - col("response_time_moving_avg")) / col("response_time_moving_std")
                    ))
        
        return anomalies
    
    def detect_ml_anomalies(self, metrics_df: DataFrame) -> DataFrame:
        """
        ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æ–¹æ³•æª¢æ¸¬ç•°å¸¸
        
        Args:
            metrics_df: æŒ‡æ¨™è³‡æ–™
            
        Returns:
            DataFrame: ç•°å¸¸æª¢æ¸¬çµæœ
        """
        print("æ­£åœ¨é€²è¡Œæ©Ÿå™¨å­¸ç¿’ç•°å¸¸æª¢æ¸¬...")
        
        # ç‰¹å¾µå·¥ç¨‹
        feature_cols = ["cpu_usage", "memory_usage", "disk_usage", 
                       "network_latency", "error_rate", "response_time"]
        
        assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
        feature_df = assembler.transform(metrics_df)
        
        # ä½¿ç”¨ K-means èšé¡é€²è¡Œç•°å¸¸æª¢æ¸¬
        kmeans = KMeans(k=3, featuresCol="features", predictionCol="cluster")
        
        try:
            model = kmeans.fit(feature_df)
            predictions = model.transform(feature_df)
            
            # è¨ˆç®—åˆ°èšé¡ä¸­å¿ƒçš„è·é›¢
            def calculate_distance(features, centers):
                # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è©²è¨ˆç®—åˆ°å„èšé¡ä¸­å¿ƒçš„è·é›¢
                return 1.0  # å ä½ç¬¦
            
            # æ¨™è¨˜ç•°å¸¸ï¼ˆè·é›¢èšé¡ä¸­å¿ƒè¼ƒé çš„é»ï¼‰
            anomalies = predictions.filter(col("cluster") == 0)  # ç°¡åŒ–è™•ç†
            
            # æ·»åŠ ç•°å¸¸è³‡è¨Š
            anomalies = anomalies.withColumn("anomaly_type", lit("ml_clustering")) \
                               .withColumn("anomaly_score", lit(1.0))
            
            return anomalies
            
        except Exception as e:
            print(f"æ©Ÿå™¨å­¸ç¿’ç•°å¸¸æª¢æ¸¬å¤±æ•—: {str(e)}")
            return metrics_df.limit(0)  # è¿”å›ç©º DataFrame
    
    def detect_pattern_anomalies(self, metrics_df: DataFrame) -> DataFrame:
        """
        æª¢æ¸¬æ¨¡å¼ç•°å¸¸
        
        Args:
            metrics_df: æŒ‡æ¨™è³‡æ–™
            
        Returns:
            DataFrame: æ¨¡å¼ç•°å¸¸æª¢æ¸¬çµæœ
        """
        print("æ­£åœ¨é€²è¡Œæ¨¡å¼ç•°å¸¸æª¢æ¸¬...")
        
        # æª¢æ¸¬åŒä¸€æœå‹™å™¨å¤šå€‹æŒ‡æ¨™åŒæ™‚ç•°å¸¸
        threshold_conditions = [
            col("cpu_usage") > 80,
            col("memory_usage") > 80,
            col("disk_usage") > 80,
            col("network_latency") > 100,
            col("error_rate") > 5,
            col("response_time") > 1000
        ]
        
        # è¨ˆç®—ç•°å¸¸æŒ‡æ¨™æ•¸é‡
        anomaly_count = metrics_df.withColumn(
            "anomaly_count",
            sum([when(condition, 1).otherwise(0) for condition in threshold_conditions])
        )
        
        # å¤šå€‹æŒ‡æ¨™åŒæ™‚ç•°å¸¸è¦–ç‚ºæ¨¡å¼ç•°å¸¸
        pattern_anomalies = anomaly_count.filter(col("anomaly_count") >= 3) \
                                       .withColumn("anomaly_type", lit("pattern")) \
                                       .withColumn("anomaly_score", col("anomaly_count") / 6.0)
        
        return pattern_anomalies


class MonitoringSystem:
    """ç›£æ§ç³»çµ±ä¸»é¡"""
    
    def __init__(self, config: MonitoringConfig, spark: SparkSession):
        self.config = config
        self.spark = spark
        self.metrics_collector = MetricsCollector(spark)
        self.alert_manager = AlertManager(config.alert_config)
        self.anomaly_detector = AnomalyDetector(spark)
        
        # è¨­å®šæ—¥èªŒ
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def run_monitoring_cycle(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œä¸€å€‹ç›£æ§é€±æœŸ
        
        Returns:
            Dict[str, Any]: ç›£æ§çµæœ
        """
        self.logger.info("é–‹å§‹ç›£æ§é€±æœŸ")
        
        # è¼‰å…¥æŒ‡æ¨™è³‡æ–™
        if os.path.exists(self.config.input_path):
            metrics_df = self.metrics_collector.load_metrics(self.config.input_path)
        else:
            # ä½¿ç”¨ç¯„ä¾‹è³‡æ–™
            metrics_df = self.metrics_collector.generate_sample_metrics()
        
        if metrics_df is None:
            self.logger.error("ç„¡æ³•è¼‰å…¥æŒ‡æ¨™è³‡æ–™")
            return {}
        
        # æª¢æŸ¥è­¦å ±
        alerts_df = self.alert_manager.check_thresholds(metrics_df)
        alert_summary = self.alert_manager.process_alerts(alerts_df)
        
        # ç•°å¸¸æª¢æ¸¬
        anomalies = {}
        if self.config.enable_ml_detection:
            anomalies['statistical'] = self.anomaly_detector.detect_statistical_anomalies(metrics_df)
            anomalies['ml'] = self.anomaly_detector.detect_ml_anomalies(metrics_df)
            anomalies['pattern'] = self.anomaly_detector.detect_pattern_anomalies(metrics_df)
        
        # ç”Ÿæˆå ±å‘Š
        report = self.generate_monitoring_report(metrics_df, alerts_df, anomalies)
        
        # ç™¼é€è­¦å ±
        if alert_summary:
            self.alert_manager.send_alert_notification(alert_summary)
        
        self.logger.info("ç›£æ§é€±æœŸå®Œæˆ")
        return report
    
    def generate_monitoring_report(self, metrics_df: DataFrame, 
                                 alerts_df: DataFrame, 
                                 anomalies: Dict[str, DataFrame]) -> Dict[str, Any]:
        """
        ç”Ÿæˆç›£æ§å ±å‘Š
        
        Args:
            metrics_df: æŒ‡æ¨™è³‡æ–™
            alerts_df: è­¦å ±è³‡æ–™
            anomalies: ç•°å¸¸æª¢æ¸¬çµæœ
            
        Returns:
            Dict[str, Any]: ç›£æ§å ±å‘Š
        """
        self.logger.info("ç”Ÿæˆç›£æ§å ±å‘Š")
        
        # åŸºæœ¬çµ±è¨ˆ
        total_metrics = metrics_df.count()
        total_alerts = alerts_df.count()
        unique_servers = metrics_df.select("server_id").distinct().count()
        
        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        avg_metrics = metrics_df.agg(
            avg("cpu_usage").alias("avg_cpu"),
            avg("memory_usage").alias("avg_memory"),
            avg("disk_usage").alias("avg_disk"),
            avg("network_latency").alias("avg_latency"),
            avg("error_rate").alias("avg_error_rate"),
            avg("response_time").alias("avg_response_time")
        ).collect()[0]
        
        # ç³»çµ±å¥åº·åº¦è©•åˆ†
        health_score = self.calculate_health_score(avg_metrics)
        
        # ç•°å¸¸çµ±è¨ˆ
        anomaly_counts = {}
        for anomaly_type, anomaly_df in anomalies.items():
            anomaly_counts[anomaly_type] = anomaly_df.count()
        
        # è¶¨å‹¢åˆ†æ
        trends = self.analyze_trends(metrics_df)
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_metrics": total_metrics,
                "total_alerts": total_alerts,
                "unique_servers": unique_servers,
                "health_score": health_score,
                "status": "healthy" if health_score > 80 else "warning" if health_score > 60 else "critical"
            },
            "metrics": {
                "avg_cpu": float(avg_metrics['avg_cpu']),
                "avg_memory": float(avg_metrics['avg_memory']),
                "avg_disk": float(avg_metrics['avg_disk']),
                "avg_latency": float(avg_metrics['avg_latency']),
                "avg_error_rate": float(avg_metrics['avg_error_rate']),
                "avg_response_time": float(avg_metrics['avg_response_time'])
            },
            "anomalies": anomaly_counts,
            "trends": trends,
            "recommendations": self.generate_recommendations(avg_metrics, alert_summary)
        }
        
        # ä¿å­˜å ±å‘Š
        self.save_report(report)
        
        return report
    
    def calculate_health_score(self, avg_metrics) -> float:
        """
        è¨ˆç®—ç³»çµ±å¥åº·åº¦è©•åˆ†
        
        Args:
            avg_metrics: å¹³å‡æŒ‡æ¨™
            
        Returns:
            float: å¥åº·åº¦è©•åˆ† (0-100)
        """
        cpu_score = max(0, 100 - avg_metrics['avg_cpu'])
        memory_score = max(0, 100 - avg_metrics['avg_memory'])
        disk_score = max(0, 100 - avg_metrics['avg_disk'])
        latency_score = max(0, 100 - avg_metrics['avg_latency'])
        error_score = max(0, 100 - avg_metrics['avg_error_rate'] * 10)
        response_score = max(0, 100 - avg_metrics['avg_response_time'] / 10)
        
        return (cpu_score + memory_score + disk_score + latency_score + error_score + response_score) / 6
    
    def analyze_trends(self, metrics_df: DataFrame) -> Dict[str, Any]:
        """
        åˆ†æè¶¨å‹¢
        
        Args:
            metrics_df: æŒ‡æ¨™è³‡æ–™
            
        Returns:
            Dict[str, Any]: è¶¨å‹¢åˆ†æçµæœ
        """
        # æŒ‰å°æ™‚åˆ†æè¶¨å‹¢
        hourly_trends = metrics_df.withColumn("hour", hour("timestamp")) \
                                 .groupBy("hour") \
                                 .agg(
                                     avg("cpu_usage").alias("avg_cpu"),
                                     avg("memory_usage").alias("avg_memory"),
                                     avg("response_time").alias("avg_response_time"),
                                     count("*").alias("metric_count")
                                 ) \
                                 .orderBy("hour")
        
        trends_data = hourly_trends.collect()
        
        # è¨ˆç®—è¶¨å‹¢æ–¹å‘
        cpu_trend = "stable"
        memory_trend = "stable"
        response_time_trend = "stable"
        
        if len(trends_data) >= 2:
            cpu_values = [row['avg_cpu'] for row in trends_data]
            memory_values = [row['avg_memory'] for row in trends_data]
            response_time_values = [row['avg_response_time'] for row in trends_data]
            
            # ç°¡å–®çš„è¶¨å‹¢è¨ˆç®—
            if cpu_values[-1] > cpu_values[0] * 1.1:
                cpu_trend = "increasing"
            elif cpu_values[-1] < cpu_values[0] * 0.9:
                cpu_trend = "decreasing"
            
            if memory_values[-1] > memory_values[0] * 1.1:
                memory_trend = "increasing"
            elif memory_values[-1] < memory_values[0] * 0.9:
                memory_trend = "decreasing"
            
            if response_time_values[-1] > response_time_values[0] * 1.1:
                response_time_trend = "increasing"
            elif response_time_values[-1] < response_time_values[0] * 0.9:
                response_time_trend = "decreasing"
        
        return {
            "cpu_trend": cpu_trend,
            "memory_trend": memory_trend,
            "response_time_trend": response_time_trend,
            "hourly_data": [row.asDict() for row in trends_data]
        }
    
    def generate_recommendations(self, avg_metrics, alert_summary) -> List[str]:
        """
        ç”Ÿæˆå»ºè­°
        
        Args:
            avg_metrics: å¹³å‡æŒ‡æ¨™
            alert_summary: è­¦å ±æ‘˜è¦
            
        Returns:
            List[str]: å»ºè­°åˆ—è¡¨
        """
        recommendations = []
        
        if avg_metrics['avg_cpu'] > 70:
            recommendations.append("CPU ä½¿ç”¨ç‡åé«˜ï¼Œå»ºè­°æª¢æŸ¥ç¨‹å¼æ•ˆèƒ½æˆ–å¢åŠ è³‡æº")
        
        if avg_metrics['avg_memory'] > 80:
            recommendations.append("è¨˜æ†¶é«”ä½¿ç”¨ç‡åé«˜ï¼Œå»ºè­°æª¢æŸ¥è¨˜æ†¶é«”æ´©æ¼æˆ–å¢åŠ è¨˜æ†¶é«”")
        
        if avg_metrics['avg_disk'] > 85:
            recommendations.append("ç£ç¢Ÿä½¿ç”¨ç‡åé«˜ï¼Œå»ºè­°æ¸…ç†ç£ç¢Ÿæˆ–æ“´å±•å„²å­˜")
        
        if avg_metrics['avg_latency'] > 50:
            recommendations.append("ç¶²è·¯å»¶é²è¼ƒé«˜ï¼Œå»ºè­°æª¢æŸ¥ç¶²è·¯é…ç½®")
        
        if avg_metrics['avg_error_rate'] > 3:
            recommendations.append("éŒ¯èª¤ç‡åé«˜ï¼Œå»ºè­°æª¢æŸ¥æ‡‰ç”¨ç¨‹å¼æ—¥èªŒ")
        
        if avg_metrics['avg_response_time'] > 500:
            recommendations.append("éŸ¿æ‡‰æ™‚é–“è¼ƒæ…¢ï¼Œå»ºè­°å„ªåŒ–æ‡‰ç”¨ç¨‹å¼æˆ–è³‡æ–™åº«æŸ¥è©¢")
        
        if alert_summary and alert_summary.get('critical', 0) > 0:
            recommendations.append("æª¢æ¸¬åˆ°åš´é‡è­¦å ±ï¼Œå»ºè­°ç«‹å³è™•ç†")
        
        return recommendations
    
    def save_report(self, report: Dict[str, Any]) -> None:
        """
        ä¿å­˜å ±å‘Š
        
        Args:
            report: ç›£æ§å ±å‘Š
        """
        os.makedirs(self.config.output_path, exist_ok=True)
        
        # ä¿å­˜ JSON å ±å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join(self.config.output_path, f"monitoring_report_{timestamp}.json")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ç›£æ§å ±å‘Šå·²ä¿å­˜è‡³: {report_path}")
    
    def generate_dashboard(self, metrics_df: DataFrame) -> None:
        """
        ç”Ÿæˆç›£æ§å„€è¡¨æ¿
        
        Args:
            metrics_df: æŒ‡æ¨™è³‡æ–™
        """
        self.logger.info("ç”Ÿæˆç›£æ§å„€è¡¨æ¿")
        
        # æŒ‰æœå‹™åˆ†çµ„åˆ†æ
        service_metrics = metrics_df.groupBy("service_name").agg(
            avg("cpu_usage").alias("avg_cpu"),
            avg("memory_usage").alias("avg_memory"),
            avg("disk_usage").alias("avg_disk"),
            avg("network_latency").alias("avg_latency"),
            avg("error_rate").alias("avg_error_rate"),
            avg("response_time").alias("avg_response_time")
        ).toPandas()
        
        # æ™‚é–“è¶¨å‹¢åˆ†æ
        hourly_metrics = metrics_df.withColumn("hour", hour("timestamp")) \
                                  .groupBy("hour") \
                                  .agg(
                                      avg("cpu_usage").alias("avg_cpu"),
                                      avg("memory_usage").alias("avg_memory"),
                                      avg("response_time").alias("avg_response_time")
                                  ) \
                                  .orderBy("hour") \
                                  .toPandas()
        
        # ç”Ÿæˆåœ–è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # å„æœå‹™ CPU ä½¿ç”¨ç‡
        axes[0, 0].bar(service_metrics['service_name'], service_metrics['avg_cpu'])
        axes[0, 0].set_title('å„æœå‹™å¹³å‡ CPU ä½¿ç”¨ç‡')
        axes[0, 0].set_ylabel('CPU ä½¿ç”¨ç‡ (%)')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # å„æœå‹™è¨˜æ†¶é«”ä½¿ç”¨ç‡
        axes[0, 1].bar(service_metrics['service_name'], service_metrics['avg_memory'], color='orange')
        axes[0, 1].set_title('å„æœå‹™å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨ç‡')
        axes[0, 1].set_ylabel('è¨˜æ†¶é«”ä½¿ç”¨ç‡ (%)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # å„æœå‹™éŸ¿æ‡‰æ™‚é–“
        axes[0, 2].bar(service_metrics['service_name'], service_metrics['avg_response_time'], color='green')
        axes[0, 2].set_title('å„æœå‹™å¹³å‡éŸ¿æ‡‰æ™‚é–“')
        axes[0, 2].set_ylabel('éŸ¿æ‡‰æ™‚é–“ (ms)')
        axes[0, 2].tick_params(axis='x', rotation=45)
        
        # æ¯å°æ™‚ CPU è¶¨å‹¢
        axes[1, 0].plot(hourly_metrics['hour'], hourly_metrics['avg_cpu'], marker='o')
        axes[1, 0].set_title('æ¯å°æ™‚ CPU ä½¿ç”¨ç‡è¶¨å‹¢')
        axes[1, 0].set_xlabel('å°æ™‚')
        axes[1, 0].set_ylabel('CPU ä½¿ç”¨ç‡ (%)')
        axes[1, 0].grid(True)
        
        # æ¯å°æ™‚è¨˜æ†¶é«”è¶¨å‹¢
        axes[1, 1].plot(hourly_metrics['hour'], hourly_metrics['avg_memory'], marker='o', color='orange')
        axes[1, 1].set_title('æ¯å°æ™‚è¨˜æ†¶é«”ä½¿ç”¨ç‡è¶¨å‹¢')
        axes[1, 1].set_xlabel('å°æ™‚')
        axes[1, 1].set_ylabel('è¨˜æ†¶é«”ä½¿ç”¨ç‡ (%)')
        axes[1, 1].grid(True)
        
        # æ¯å°æ™‚éŸ¿æ‡‰æ™‚é–“è¶¨å‹¢
        axes[1, 2].plot(hourly_metrics['hour'], hourly_metrics['avg_response_time'], marker='o', color='green')
        axes[1, 2].set_title('æ¯å°æ™‚éŸ¿æ‡‰æ™‚é–“è¶¨å‹¢')
        axes[1, 2].set_xlabel('å°æ™‚')
        axes[1, 2].set_ylabel('éŸ¿æ‡‰æ™‚é–“ (ms)')
        axes[1, 2].grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        dashboard_path = os.path.join(self.config.output_path, 'monitoring_dashboard.png')
        plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"ç›£æ§å„€è¡¨æ¿å·²ä¿å­˜è‡³: {dashboard_path}")
    
    def run_continuous_monitoring(self) -> None:
        """
        åŸ·è¡ŒæŒçºŒç›£æ§
        """
        self.logger.info("é–‹å§‹æŒçºŒç›£æ§")
        
        try:
            while True:
                report = self.run_monitoring_cycle()
                
                # ç”Ÿæˆå„€è¡¨æ¿
                if os.path.exists(self.config.input_path):
                    metrics_df = self.metrics_collector.load_metrics(self.config.input_path)
                else:
                    metrics_df = self.metrics_collector.generate_sample_metrics()
                
                if metrics_df is not None:
                    self.generate_dashboard(metrics_df)
                
                # ç­‰å¾…ä¸‹ä¸€å€‹ç›£æ§é€±æœŸ
                time.sleep(self.config.monitoring_interval)
                
        except KeyboardInterrupt:
            self.logger.info("ç›£æ§ç³»çµ±å·²åœæ­¢")
        except Exception as e:
            self.logger.error(f"ç›£æ§ç³»çµ±ç™¼ç”ŸéŒ¯èª¤: {str(e)}")


def load_config(config_path: str) -> MonitoringConfig:
    """
    è¼‰å…¥é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾‘
        
    Returns:
        MonitoringConfig: ç›£æ§é…ç½®
    """
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        alert_config = AlertConfig(**config_data.get('alert_config', {}))
        
        return MonitoringConfig(
            input_path=config_data.get('input_path', ''),
            output_path=config_data.get('output_path', './monitoring_output'),
            alert_config=alert_config,
            monitoring_interval=config_data.get('monitoring_interval', 60),
            retention_days=config_data.get('retention_days', 30),
            enable_ml_detection=config_data.get('enable_ml_detection', True),
            email_alerts=config_data.get('email_alerts', False),
            email_recipients=config_data.get('email_recipients', [])
        )
    else:
        # ä½¿ç”¨é è¨­é…ç½®
        return MonitoringConfig(
            input_path='',
            output_path='./monitoring_output',
            alert_config=AlertConfig()
        )


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='å³æ™‚ç›£æ§ç³»çµ±')
    parser.add_argument('--config', default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--mode', choices=['single', 'continuous'], default='single', 
                       help='åŸ·è¡Œæ¨¡å¼ï¼šsingle(å–®æ¬¡åŸ·è¡Œ) æˆ– continuous(æŒçºŒç›£æ§)')
    parser.add_argument('--app-name', default='MonitoringSystem', help='Spark æ‡‰ç”¨åç¨±')
    
    args = parser.parse_args()
    
    # è¼‰å…¥é…ç½®
    config = load_config(args.config)
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    os.makedirs(config.output_path, exist_ok=True)
    
    # å»ºç«‹ Spark æœƒè©±
    spark = SparkSession.builder \
        .appName(args.app_name) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    try:
        # å»ºç«‹ç›£æ§ç³»çµ±
        monitoring_system = MonitoringSystem(config, spark)
        
        if args.mode == 'single':
            # å–®æ¬¡åŸ·è¡Œ
            report = monitoring_system.run_monitoring_cycle()
            print(f"ç›£æ§å®Œæˆï¼Œå ±å‘Šå·²ä¿å­˜è‡³: {config.output_path}")
        else:
            # æŒçºŒç›£æ§
            monitoring_system.run_continuous_monitoring()
    
    except Exception as e:
        print(f"éŒ¯èª¤: {str(e)}")
        sys.exit(1)
    
    finally:
        # é—œé–‰ Spark æœƒè©±
        spark.stop()


if __name__ == "__main__":
    main()