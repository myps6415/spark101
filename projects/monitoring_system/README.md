# å³æ™‚ç›£æ§ç³»çµ±

ä¸€å€‹åŸºæ–¼ Apache Spark çš„å³æ™‚ç›£æ§ç³»çµ±ï¼Œç”¨æ–¼ç›£æ§ç³»çµ±æŒ‡æ¨™ã€æª¢æ¸¬ç•°å¸¸è¡Œç‚ºä¸¦è‡ªå‹•ç™¼é€è­¦å ±ã€‚

## åŠŸèƒ½ç‰¹è‰²

### ğŸ“Š å³æ™‚ç›£æ§
- **ç³»çµ±æŒ‡æ¨™æ”¶é›†**: CPUã€è¨˜æ†¶é«”ã€ç£ç¢Ÿã€ç¶²è·¯å»¶é²ç­‰
- **æ‡‰ç”¨ç¨‹å¼æŒ‡æ¨™**: éŸ¿æ‡‰æ™‚é–“ã€éŒ¯èª¤ç‡ã€è«‹æ±‚æ•¸ç­‰
- **å¤šæœå‹™ç›£æ§**: æ”¯æ´ç›£æ§å¤šå€‹æœå‹™å’Œä¼ºæœå™¨
- **å³æ™‚è™•ç†**: ä½¿ç”¨ Spark Streaming é€²è¡Œå³æ™‚è³‡æ–™è™•ç†

### ğŸš¨ æ™ºèƒ½è­¦å ±
- **é–¾å€¼ç›£æ§**: å¯è‡ªè¨‚å„é …æŒ‡æ¨™çš„è­¦å ±é–¾å€¼
- **å¤šç´šè­¦å ±**: æ”¯æ´ criticalã€highã€medium ä¸‰ç´šè­¦å ±
- **è­¦å ±æŠ‘åˆ¶**: é¿å…é‡è¤‡è­¦å ±çš„æ™ºèƒ½æŠ‘åˆ¶æ©Ÿåˆ¶
- **å¤šé€šé“é€šçŸ¥**: æ”¯æ´ Emailã€Slackã€Webhook ç­‰é€šçŸ¥æ–¹å¼

### ğŸ” ç•°å¸¸æª¢æ¸¬
- **çµ±è¨ˆç•°å¸¸æª¢æ¸¬**: åŸºæ–¼ç§»å‹•å¹³å‡å’Œæ¨™æº–å·®çš„ç•°å¸¸æª¢æ¸¬
- **æ©Ÿå™¨å­¸ç¿’æª¢æ¸¬**: ä½¿ç”¨ K-means èšé¡é€²è¡Œç•°å¸¸æª¢æ¸¬
- **æ¨¡å¼ç•°å¸¸æª¢æ¸¬**: æª¢æ¸¬å¤šå€‹æŒ‡æ¨™åŒæ™‚ç•°å¸¸çš„æ¨¡å¼
- **è¶¨å‹¢åˆ†æ**: åˆ†ææŒ‡æ¨™çš„é•·æœŸè¶¨å‹¢è®ŠåŒ–

### ğŸ“ˆ å¯è¦–åŒ–å ±å‘Š
- **å³æ™‚å„€è¡¨æ¿**: å‹•æ…‹ç”Ÿæˆç›£æ§å„€è¡¨æ¿
- **æ­·å²è¶¨å‹¢åœ–**: å±•ç¤ºæŒ‡æ¨™çš„æ­·å²è®ŠåŒ–è¶¨å‹¢
- **å¥åº·åº¦è©•åˆ†**: è‡ªå‹•è¨ˆç®—ç³»çµ±æ•´é«”å¥åº·åº¦
- **è©³ç´°å ±å‘Š**: ç”Ÿæˆ JSON æ ¼å¼çš„è©³ç´°ç›£æ§å ±å‘Š

## å®‰è£éœ€æ±‚

```bash
# Python ä¾è³´
pip install pyspark pandas matplotlib seaborn numpy scikit-learn

# æˆ–ä½¿ç”¨ poetryï¼ˆæ¨è–¦ï¼‰
poetry install
```

## å¿«é€Ÿé–‹å§‹

### 1. æº–å‚™é…ç½®æ–‡ä»¶

```bash
# è¤‡è£½ç¯„ä¾‹é…ç½®æ–‡ä»¶
cp config.json my_config.json

# æ ¹æ“šéœ€è¦ä¿®æ”¹é…ç½®
vim my_config.json
```

### 2. åŸºæœ¬ä½¿ç”¨

```bash
# å–®æ¬¡ç›£æ§åŸ·è¡Œ
python monitoring_system.py --config my_config.json --mode single

# æŒçºŒç›£æ§æ¨¡å¼
python monitoring_system.py --config my_config.json --mode continuous
```

## é…ç½®èªªæ˜

### åŸºæœ¬é…ç½®

```json
{
  "input_path": "./sample_metrics.json",
  "output_path": "./monitoring_output",
  "monitoring_interval": 60,
  "retention_days": 30,
  "enable_ml_detection": true
}
```

### è­¦å ±é…ç½®

```json
{
  "alert_config": {
    "cpu_threshold": 80.0,
    "memory_threshold": 85.0,
    "disk_threshold": 90.0,
    "network_latency_threshold": 100.0,
    "error_rate_threshold": 5.0,
    "response_time_threshold": 1000.0
  }
}
```

### é€šçŸ¥é…ç½®

```json
{
  "notification_channels": {
    "email": {
      "enabled": true,
      "smtp_server": "smtp.gmail.com",
      "smtp_port": 587,
      "username": "your_email@gmail.com",
      "password": "your_app_password"
    },
    "slack": {
      "enabled": true,
      "webhook_url": "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
    }
  }
}
```

## è³‡æ–™æ ¼å¼

### æŒ‡æ¨™è³‡æ–™æ ¼å¼

ç›£æ§ç³»çµ±æ”¯æ´ä»¥ä¸‹æ ¼å¼çš„æŒ‡æ¨™è³‡æ–™ï¼š

```json
{
  "timestamp": "2024-01-01T12:00:00Z",
  "server_id": "server-001",
  "service_name": "web",
  "cpu_usage": 75.5,
  "memory_usage": 68.2,
  "disk_usage": 45.8,
  "network_latency": 25.3,
  "error_rate": 2.1,
  "request_count": 1250,
  "response_time": 180.5,
  "active_connections": 45,
  "thread_count": 20,
  "heap_usage": 60.3
}
```

### æ”¯æ´çš„è³‡æ–™ä¾†æº

- **JSON æ–‡ä»¶**: é©ç”¨æ–¼æ—¥èªŒæ–‡ä»¶æˆ–æ‰¹æ¬¡è™•ç†
- **CSV æ–‡ä»¶**: é©ç”¨æ–¼çµæ§‹åŒ–è³‡æ–™
- **Parquet æ–‡ä»¶**: é©ç”¨æ–¼å¤§è¦æ¨¡è³‡æ–™è™•ç†
- **Kafka**: é©ç”¨æ–¼å³æ™‚è³‡æ–™æµ
- **è³‡æ–™åº«**: é©ç”¨æ–¼ç¾æœ‰ç›£æ§ç³»çµ±æ•´åˆ

## ä½¿ç”¨ç¯„ä¾‹

### 1. ç”Ÿæˆæ¸¬è©¦è³‡æ–™

```bash
# ç”Ÿæˆç¯„ä¾‹æŒ‡æ¨™è³‡æ–™
python -c "
import json
import random
from datetime import datetime, timedelta

# ç”Ÿæˆ 1000 æ¢æ¸¬è©¦æŒ‡æ¨™
metrics = []
for i in range(1000):
    timestamp = datetime.now() - timedelta(seconds=i*30)
    metrics.append({
        'timestamp': timestamp.isoformat(),
        'server_id': f'server-{random.randint(1, 10):03d}',
        'service_name': random.choice(['web', 'api', 'database']),
        'cpu_usage': random.uniform(20, 90),
        'memory_usage': random.uniform(30, 95),
        'disk_usage': random.uniform(40, 80),
        'network_latency': random.uniform(10, 150),
        'error_rate': random.uniform(0, 10),
        'request_count': random.randint(50, 500),
        'response_time': random.uniform(50, 2000),
        'active_connections': random.randint(10, 100),
        'thread_count': random.randint(5, 50),
        'heap_usage': random.uniform(20, 80)
    })

with open('sample_metrics.json', 'w') as f:
    for metric in metrics:
        f.write(json.dumps(metric) + '\\n')
"

echo "æ¸¬è©¦è³‡æ–™å·²ç”Ÿæˆ: sample_metrics.json"
```

### 2. åŸ·è¡Œç›£æ§

```bash
# åŸ·è¡Œå–®æ¬¡ç›£æ§
python monitoring_system.py --config config.json --mode single

# æª¢æŸ¥çµæœ
ls -la monitoring_output/
```

### 3. æŸ¥çœ‹ç›£æ§å ±å‘Š

```bash
# æŸ¥çœ‹æœ€æ–°çš„ç›£æ§å ±å‘Š
cat monitoring_output/monitoring_report_*.json | jq '.'

# æŸ¥çœ‹ç”Ÿæˆçš„åœ–è¡¨
open monitoring_output/monitoring_dashboard.png
```

## é€²éšåŠŸèƒ½

### 1. è‡ªè¨‚ç•°å¸¸æª¢æ¸¬

```python
# è‡ªè¨‚ç•°å¸¸æª¢æ¸¬é‚è¼¯
class CustomAnomalyDetector:
    def detect_business_anomalies(self, metrics_df):
        # æ¥­å‹™é‚è¼¯ç›¸é—œçš„ç•°å¸¸æª¢æ¸¬
        # ä¾‹å¦‚ï¼šè¨‚å–®é‡çªç„¶ä¸‹é™ã€ç”¨æˆ¶æ´»èºåº¦ç•°å¸¸ç­‰
        pass
    
    def detect_seasonal_anomalies(self, metrics_df):
        # å­£ç¯€æ€§ç•°å¸¸æª¢æ¸¬
        # è€ƒæ…®æ™‚é–“å› ç´ çš„ç•°å¸¸æª¢æ¸¬
        pass
```

### 2. æ•´åˆå¤–éƒ¨ç³»çµ±

```python
# æ•´åˆ Grafana
def export_to_grafana(metrics_df):
    # å°‡æŒ‡æ¨™è³‡æ–™åŒ¯å‡ºåˆ° Grafana
    pass

# æ•´åˆ Prometheus
def export_to_prometheus(metrics_df):
    # å°‡æŒ‡æ¨™è³‡æ–™åŒ¯å‡ºåˆ° Prometheus
    pass
```

### 3. æ©Ÿå™¨å­¸ç¿’æ¨¡å‹

```python
# ä½¿ç”¨ LSTM é€²è¡Œæ™‚é–“åºåˆ—é æ¸¬
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler

def train_prediction_model(historical_data):
    # è¨“ç·´é æ¸¬æ¨¡å‹
    assembler = VectorAssembler(inputCols=["cpu_usage", "memory_usage"], outputCol="features")
    feature_data = assembler.transform(historical_data)
    
    lr = LinearRegression(featuresCol="features", labelCol="response_time")
    model = lr.fit(feature_data)
    
    return model
```

## è­¦å ±è¦å‰‡

### 1. åŸºæœ¬è­¦å ±è¦å‰‡

```python
# CPU ä½¿ç”¨ç‡è­¦å ±
if cpu_usage > 80:
    severity = "high"
    message = f"CPU ä½¿ç”¨ç‡éé«˜: {cpu_usage}%"

# è¨˜æ†¶é«”ä½¿ç”¨ç‡è­¦å ±
if memory_usage > 85:
    severity = "high"
    message = f"è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜: {memory_usage}%"

# éŸ¿æ‡‰æ™‚é–“è­¦å ±
if response_time > 1000:
    severity = "medium"
    message = f"éŸ¿æ‡‰æ™‚é–“éé•·: {response_time}ms"
```

### 2. è¤‡åˆè­¦å ±è¦å‰‡

```python
# ç³»çµ±è² è¼‰éé«˜è­¦å ±
if cpu_usage > 80 and memory_usage > 80 and response_time > 500:
    severity = "critical"
    message = "ç³»çµ±è² è¼‰éé«˜ï¼Œå¤šå€‹æŒ‡æ¨™ç•°å¸¸"

# æœå‹™ä¸å¯ç”¨è­¦å ±
if error_rate > 50 and response_time > 5000:
    severity = "critical"
    message = "æœå‹™å¯èƒ½ä¸å¯ç”¨"
```

### 3. è¶¨å‹¢è­¦å ±

```python
# CPU ä½¿ç”¨ç‡æŒçºŒä¸Šå‡è­¦å ±
if cpu_trend == "increasing" and cpu_usage > 70:
    severity = "medium"
    message = "CPU ä½¿ç”¨ç‡æŒçºŒä¸Šå‡ï¼Œéœ€è¦é—œæ³¨"
```

## æ•ˆèƒ½å„ªåŒ–

### 1. Spark é…ç½®å„ªåŒ–

```python
spark = SparkSession.builder \
    .appName("MonitoringSystem") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.sql.adaptive.skewJoin.enabled", "true") \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "2") \
    .config("spark.sql.shuffle.partitions", "200") \
    .getOrCreate()
```

### 2. è³‡æ–™åˆ†å€ç­–ç•¥

```python
# æŒ‰æ™‚é–“åˆ†å€
df = df.repartition(col("timestamp"))

# æŒ‰æœå‹™åˆ†å€
df = df.repartition(col("service_name"))
```

### 3. å¿«å–ç­–ç•¥

```python
# å¿«å–ç¶“å¸¸ä½¿ç”¨çš„è³‡æ–™
metrics_df.cache()

# å¿«å–ä¸­é–“çµæœ
processed_df.persist(StorageLevel.MEMORY_AND_DISK)
```

## éƒ¨ç½²æŒ‡å—

### 1. æœ¬åœ°éƒ¨ç½²

```bash
# å…‹éš†å°ˆæ¡ˆ
git clone https://github.com/your-org/monitoring-system.git
cd monitoring-system

# å®‰è£ä¾è³´
pip install -r requirements.txt

# å•Ÿå‹•ç›£æ§
python monitoring_system.py --mode continuous
```

### 2. Docker éƒ¨ç½²

```dockerfile
FROM python:3.8-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["python", "monitoring_system.py", "--mode", "continuous"]
```

```bash
# å»ºç«‹ Docker æ˜ åƒ
docker build -t monitoring-system .

# åŸ·è¡Œå®¹å™¨
docker run -d -v $(pwd)/config.json:/app/config.json monitoring-system
```

### 3. Kubernetes éƒ¨ç½²

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: monitoring-system
  template:
    metadata:
      labels:
        app: monitoring-system
    spec:
      containers:
      - name: monitoring-system
        image: monitoring-system:latest
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
        volumeMounts:
        - name: config
          mountPath: /app/config.json
          subPath: config.json
      volumes:
      - name: config
        configMap:
          name: monitoring-config
```

## ç›£æ§æœ€ä½³å¯¦è¸

### 1. æŒ‡æ¨™é¸æ“‡
- é¸æ“‡é—œéµæ¥­å‹™æŒ‡æ¨™
- é¿å…ç›£æ§éå¤šç„¡ç”¨æŒ‡æ¨™
- è€ƒæ…®æŒ‡æ¨™çš„ç›¸é—œæ€§

### 2. è­¦å ±è¨­å®š
- è¨­å®šåˆç†çš„è­¦å ±é–¾å€¼
- é¿å…è­¦å ±é¢¨æš´
- å»ºç«‹è­¦å ±å‡ç´šæ©Ÿåˆ¶

### 3. è³‡æ–™ä¿ç•™
- æ ¹æ“šéœ€æ±‚è¨­å®šè³‡æ–™ä¿ç•™æœŸé™
- è€ƒæ…®è³‡æ–™å£“ç¸®å’Œæ­¸æª”
- å®šæœŸæ¸…ç†æ­·å²è³‡æ–™

### 4. ç³»çµ±ç¶­è­·
- å®šæœŸæª¢æŸ¥ç³»çµ±å¥åº·ç‹€æ…‹
- æ›´æ–°è­¦å ±è¦å‰‡
- å„ªåŒ–æŸ¥è©¢æ•ˆèƒ½

## æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **è¨˜æ†¶é«”ä¸è¶³**
   ```bash
   # å¢åŠ  Spark è¨˜æ†¶é«”é…ç½®
   export SPARK_EXECUTOR_MEMORY=4g
   export SPARK_DRIVER_MEMORY=2g
   ```

2. **è³‡æ–™è¼‰å…¥å¤±æ•—**
   ```python
   # æª¢æŸ¥è³‡æ–™æ ¼å¼
   df.printSchema()
   df.show(5)
   ```

3. **è­¦å ±æœªè§¸ç™¼**
   ```python
   # æª¢æŸ¥è­¦å ±é…ç½®
   print(f"ç•¶å‰ CPU ä½¿ç”¨ç‡: {current_cpu}")
   print(f"CPU è­¦å ±é–¾å€¼: {cpu_threshold}")
   ```

### æ—¥èªŒåˆ†æ

```bash
# æŸ¥çœ‹æ‡‰ç”¨ç¨‹å¼æ—¥èªŒ
tail -f monitoring_system.log

# æŸ¥çœ‹ Spark æ—¥èªŒ
ls -la $SPARK_HOME/logs/
```

## æ“´å±•åŠŸèƒ½

### 1. æ–°å¢æŒ‡æ¨™é¡å‹

```python
# æ–°å¢è‡ªè¨‚æŒ‡æ¨™
class CustomMetric:
    def __init__(self, name, value, unit):
        self.name = name
        self.value = value
        self.unit = unit
    
    def to_dict(self):
        return {
            'name': self.name,
            'value': self.value,
            'unit': self.unit,
            'timestamp': datetime.now().isoformat()
        }
```

### 2. æ–°å¢é€šçŸ¥é€šé“

```python
# æ–°å¢ Teams é€šçŸ¥
class TeamsNotifier:
    def __init__(self, webhook_url):
        self.webhook_url = webhook_url
    
    def send_notification(self, message):
        # ç™¼é€ Teams é€šçŸ¥
        pass
```

### 3. æ–°å¢è³‡æ–™ä¾†æº

```python
# æ–°å¢ InfluxDB è³‡æ–™ä¾†æº
class InfluxDBSource:
    def __init__(self, host, port, database):
        self.host = host
        self.port = port
        self.database = database
    
    def read_metrics(self, query):
        # å¾ InfluxDB è®€å–æŒ‡æ¨™
        pass
```

## æˆæ¬Š

æœ¬å°ˆæ¡ˆä½¿ç”¨ MIT æˆæ¬Šæ¢æ¬¾ã€‚

## è²¢ç»

æ­¡è¿æäº¤ Issues å’Œ Pull Requestsï¼

## è¯çµ¡æ–¹å¼

å¦‚æœ‰å•é¡Œï¼Œè«‹è¯çµ¡ï¼š[myps6415@gmail.com](mailto:myps6415@gmail.com)