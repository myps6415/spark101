# æ—¥èªŒåˆ†æç³»çµ±

ä¸€å€‹åŸºæ–¼ Apache Spark çš„ç¶²ç«™æ—¥èªŒåˆ†æç³»çµ±ï¼Œç”¨æ–¼åˆ†æç¶²ç«™è¨ªå•æ—¥èªŒã€æª¢æ¸¬å®‰å…¨å¨è„…å’Œç•°å¸¸è¡Œç‚ºã€‚

## åŠŸèƒ½ç‰¹è‰²

### ğŸ” æ—¥èªŒè§£æ
- æ”¯æ´ Apache Common Log Format
- è‡ªå‹•è§£æ IP åœ°å€ã€æ™‚é–“æˆ³ã€è«‹æ±‚æ–¹æ³•ã€è·¯å¾‘ã€ç‹€æ…‹ç¢¼ç­‰
- æ™ºèƒ½è™•ç†è§£æå¤±æ•—çš„è¨˜éŒ„

### ğŸ›¡ï¸ å®‰å…¨å¨è„…æª¢æ¸¬
- **SQL æ³¨å…¥æª¢æ¸¬**: è­˜åˆ¥å¯èƒ½çš„ SQL æ³¨å…¥å˜—è©¦
- **ç›®éŒ„éæ­·æ”»æ“Š**: æª¢æ¸¬è·¯å¾‘éæ­·æ”»æ“Šæ¨¡å¼
- **æš´åŠ›ç ´è§£æª¢æ¸¬**: è­˜åˆ¥å°ç™»å…¥ç«¯é»çš„æš´åŠ›ç ´è§£å˜—è©¦
- **ç•°å¸¸é«˜é »è¨ªå•**: æª¢æ¸¬å¯èƒ½çš„ DDoS æ”»æ“Š
- **æ•æ„Ÿè·¯å¾‘ç›£æ§**: ç›£æ§å°ç®¡ç†é é¢çš„è¨ªå•
- **å¯ç–‘çˆ¬èŸ²æª¢æ¸¬**: è­˜åˆ¥ç•°å¸¸çš„çˆ¬èŸ²è¡Œç‚º

### ğŸ“Š æµé‡æ¨¡å¼åˆ†æ
- æ™‚é–“åºåˆ—åˆ†æï¼ˆæ¯å°æ™‚ã€æ¯æ—¥æµé‡ï¼‰
- ç†±é–€è·¯å¾‘åˆ†æ
- HTTP ç‹€æ…‹ç¢¼åˆ†ä½ˆ
- ç”¨æˆ¶ä»£ç†åˆ†æ
- éŸ¿æ‡‰æ™‚é–“åˆ†æ

### ğŸš¨ ç•°å¸¸è¡Œç‚ºæª¢æ¸¬
- éŸ¿æ‡‰æ™‚é–“ç•°å¸¸æª¢æ¸¬
- è«‹æ±‚é »ç‡ç•°å¸¸æª¢æ¸¬
- éŒ¯èª¤æ¨¡å¼åˆ†æ
- è·¯å¾‘è¨ªå•ç•°å¸¸æª¢æ¸¬

### ğŸ“ˆ å¯è¦–åŒ–å ±å‘Š
- æµé‡è¶¨å‹¢åœ–è¡¨
- ç‹€æ…‹ç¢¼åˆ†ä½ˆåœ–
- ç†±é–€è·¯å¾‘åœ–è¡¨
- è‡ªå‹•ç”Ÿæˆ JSON æ ¼å¼å®‰å…¨å ±å‘Š

## å®‰è£éœ€æ±‚

```bash
# Python ä¾è³´
pip install pyspark pandas matplotlib seaborn

# æˆ–ä½¿ç”¨ poetryï¼ˆæ¨è–¦ï¼‰
poetry install
```

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨

```bash
python log_analyzer.py --input-path /path/to/access.log --output-path /path/to/output
```

### åƒæ•¸èªªæ˜

- `--input-path`: æ—¥èªŒæ–‡ä»¶è·¯å¾‘ï¼ˆå¿…éœ€ï¼‰
- `--output-path`: è¼¸å‡ºç›®éŒ„è·¯å¾‘ï¼ˆå¿…éœ€ï¼‰
- `--app-name`: Spark æ‡‰ç”¨åç¨±ï¼ˆé¸å¡«ï¼Œé»˜èªç‚º LogAnalyzerï¼‰

### æ—¥èªŒæ ¼å¼

ç³»çµ±æ”¯æ´æ¨™æº–çš„ Apache Common Log Formatï¼š

```
IP - - [timestamp] "method path protocol" status_code response_size "referer" "user_agent" response_time
```

ç¯„ä¾‹ï¼š
```
192.168.1.100 - - [01/Jan/2024:12:00:00 +0000] "GET /index.html HTTP/1.1" 200 1234 "https://example.com" "Mozilla/5.0..." 0.123
```

## è¼¸å‡ºçµæœ

### 1. å®‰å…¨å ±å‘Š (security_report.json)

```json
{
  "report_timestamp": "2024-01-01T12:00:00",
  "summary": {
    "total_requests": 100000,
    "unique_ips": 5000,
    "date_range": {
      "start": "2024-01-01T00:00:00",
      "end": "2024-01-01T23:59:59"
    }
  },
  "threats": {
    "sql_injection": {
      "count": 15,
      "severity": "high"
    },
    "brute_force": {
      "count": 5,
      "severity": "medium"
    }
  },
  "anomalies": {
    "slow_requests": {
      "count": 200,
      "severity": "medium"
    }
  },
  "recommendations": [
    "æª¢æ¸¬åˆ° SQL æ³¨å…¥å˜—è©¦ï¼Œå»ºè­°åŠ å¼·è¼¸å…¥é©—è­‰å’Œä½¿ç”¨åƒæ•¸åŒ–æŸ¥è©¢",
    "æª¢æ¸¬åˆ°æš´åŠ›ç ´è§£å˜—è©¦ï¼Œå»ºè­°å¯¦æ–½å¸³æˆ¶é–å®šç­–ç•¥"
  ]
}
```

### 2. å¯è¦–åŒ–åœ–è¡¨ (visualizations/)

- `hourly_traffic_analysis.png`: æ¯å°æ™‚æµé‡åˆ†æ
- `status_code_distribution.png`: HTTP ç‹€æ…‹ç¢¼åˆ†ä½ˆ
- `popular_paths.png`: ç†±é–€è·¯å¾‘åˆ†æ

## ç¯„ä¾‹ç”¨æ³•

### ç”Ÿæˆæ¸¬è©¦è³‡æ–™

```python
# å»ºç«‹æ¸¬è©¦æ—¥èªŒæ–‡ä»¶
python -c "
import random
from datetime import datetime, timedelta

# ç”Ÿæˆ 1000 æ¢æ¸¬è©¦æ—¥èªŒ
with open('test_access.log', 'w') as f:
    for i in range(1000):
        timestamp = datetime.now() - timedelta(hours=random.randint(0, 24))
        ip = f'192.168.1.{random.randint(1, 100)}'
        method = random.choice(['GET', 'POST', 'PUT'])
        path = random.choice(['/', '/login', '/admin', '/api/users'])
        status = random.choice([200, 404, 500])
        size = random.randint(100, 10000)
        response_time = random.uniform(0.1, 2.0)
        
        log_line = f'{ip} - - [{timestamp.strftime(\"%d/%b/%Y:%H:%M:%S +0000\")}] \"{method} {path} HTTP/1.1\" {status} {size} \"-\" \"Mozilla/5.0\" {response_time:.3f}'
        f.write(log_line + '\\n')
"
```

### åŸ·è¡Œåˆ†æ

```bash
# åˆ†ææ¸¬è©¦æ—¥èªŒ
python log_analyzer.py --input-path test_access.log --output-path ./results

# æŸ¥çœ‹çµæœ
ls -la results/
cat results/security_report.json
```

## é€²éšé…ç½®

### è‡ªå®šç¾©å¨è„…æª¢æ¸¬åƒæ•¸

```python
# ä¿®æ”¹ LogAnalyzer é…ç½®
analyzer = LogAnalyzer(spark)
analyzer.config.update({
    'suspicious_ips': ['192.168.1.100', '10.0.0.1'],
    'sensitive_paths': ['/admin', '/config', '/api/admin'],
    'error_threshold': 50,
    'high_freq_threshold': 500,
    'response_time_threshold': 1.5
})
```

### è‡ªå®šç¾© Spark é…ç½®

```python
spark = SparkSession.builder \
    .appName("LogAnalyzer") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.sql.adaptive.skewJoin.enabled", "true") \
    .config("spark.executor.memory", "2g") \
    .config("spark.executor.cores", "2") \
    .getOrCreate()
```

## æ•ˆèƒ½å„ªåŒ–å»ºè­°

### 1. è³‡æ–™åˆ†å€
```python
# å°æ–¼å¤§å‹æ—¥èªŒæ–‡ä»¶ï¼Œå»ºè­°æŒ‰æ™‚é–“åˆ†å€
df = df.repartition(col("timestamp"))
```

### 2. å¿«å–ç­–ç•¥
```python
# å¿«å–ç¶“å¸¸ä½¿ç”¨çš„è³‡æ–™
df.cache()
```

### 3. è³‡æ–™å£“ç¸®
```bash
# å£“ç¸®æ—¥èªŒæ–‡ä»¶ä»¥ç¯€çœç©ºé–“
gzip access.log
```

## ç›£æ§å’Œå‘Šè­¦

### çµåˆç›£æ§ç³»çµ±

```python
# çµåˆ Grafana æˆ–å…¶ä»–ç›£æ§ç³»çµ±
def send_alert(threat_type, count, severity):
    if severity == "high":
        # ç™¼é€å³æ™‚è­¦å ±
        send_notification(f"æª¢æ¸¬åˆ° {threat_type}: {count} æ¬¡")
```

### è‡ªå‹•åŒ–åŸ·è¡Œ

```bash
# è¨­å®š cron ä»»å‹™æ¯å°æ™‚åŸ·è¡Œä¸€æ¬¡
0 * * * * /usr/bin/python /path/to/log_analyzer.py --input-path /var/log/apache2/access.log --output-path /var/log/analysis/
```

## æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **è¨˜æ†¶é«”ä¸è¶³**
   ```bash
   # å¢åŠ  Spark è¨˜æ†¶é«”é…ç½®
   export SPARK_EXECUTOR_MEMORY=4g
   export SPARK_DRIVER_MEMORY=2g
   ```

2. **æ—¥èªŒè§£æå¤±æ•—**
   ```python
   # æª¢æŸ¥æ—¥èªŒæ ¼å¼æ˜¯å¦æ­£ç¢º
   df.filter(col("ip").isNull()).show()
   ```

3. **æ•ˆèƒ½å•é¡Œ**
   ```python
   # æª¢æŸ¥åˆ†å€æ•¸é‡
   print(f"åˆ†å€æ•¸: {df.rdd.getNumPartitions()}")
   ```

## æ“´å±•åŠŸèƒ½

### 1. å³æ™‚ç›£æ§
```python
# ä½¿ç”¨ Structured Streaming é€²è¡Œå³æ™‚åˆ†æ
stream = spark \
    .readStream \
    .format("text") \
    .option("path", "/var/log/apache2/") \
    .load()
```

### 2. æ©Ÿå™¨å­¸ç¿’ç•°å¸¸æª¢æ¸¬
```python
# ä½¿ç”¨ MLlib é€²è¡Œç•°å¸¸æª¢æ¸¬
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler

# ç‰¹å¾µå·¥ç¨‹
assembler = VectorAssembler(inputCols=["request_count", "response_time"], outputCol="features")
features = assembler.transform(df)

# èšé¡åˆ†æ
kmeans = KMeans(k=3)
model = kmeans.fit(features)
```

### 3. åœ°ç†ä½ç½®åˆ†æ
```python
# çµåˆ IP åœ°ç†ä½ç½®è³‡æ–™åº«
def get_location(ip):
    # ä½¿ç”¨ GeoIP è³‡æ–™åº«
    return lookup_ip_location(ip)
```

## æˆæ¬Š

æœ¬å°ˆæ¡ˆä½¿ç”¨ MIT æˆæ¬Šæ¢æ¬¾ã€‚

## è²¢ç»

æ­¡è¿æäº¤ Issues å’Œ Pull Requestsï¼

## è¯çµ¡æ–¹å¼

å¦‚æœ‰å•é¡Œï¼Œè«‹è¯çµ¡ï¼š[myps6415@gmail.com](mailto:myps6415@gmail.com)