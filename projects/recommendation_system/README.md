# æ¨è–¦ç³»çµ±

ä¸€å€‹åŸºæ–¼ Apache Spark MLlib çš„æ¨è–¦ç³»çµ±ï¼Œä½¿ç”¨å”åŒéæ¿¾æ¼”ç®—æ³• (Collaborative Filtering) å’Œ ALS (Alternating Least Squares) çŸ©é™£åˆ†è§£æŠ€è¡“ç‚ºç”¨æˆ¶æ¨è–¦å•†å“ã€‚

## åŠŸèƒ½ç‰¹è‰²

### ğŸ¯ æ¨è–¦æ¼”ç®—æ³•
- **å”åŒéæ¿¾**: åŸºæ–¼ç”¨æˆ¶è¡Œç‚ºçš„æ¨è–¦
- **çŸ©é™£åˆ†è§£ (ALS)**: é«˜æ•ˆçš„å¤§è¦æ¨¡æ¨è–¦æ¼”ç®—æ³•
- **éš±å¼åé¥‹**: æ”¯æ´é»æ“Šã€ç€è¦½ç­‰éš±å¼åé¥‹
- **å†·å•Ÿå‹•è™•ç†**: é‡å°æ–°ç”¨æˆ¶å’Œæ–°å•†å“çš„æ¨è–¦ç­–ç•¥

### ğŸ“Š æ¨è–¦å“è³ª
- **è¦†è“‹ç‡åˆ†æ**: æ¨è–¦ç³»çµ±è¦†è“‹çš„å•†å“æ¯”ä¾‹
- **å¤šæ¨£æ€§è©•ä¼°**: æ¨è–¦çµæœçš„å¤šæ¨£æ€§åˆ†æ
- **æ–°ç©æ€§è¨ˆç®—**: æ¨è–¦å•†å“çš„æ–°ç©æ€§è©•ä¼°
- **æº–ç¢ºæ€§æŒ‡æ¨™**: RMSEã€MAEã€RÂ² ç­‰è©•ä¼°æŒ‡æ¨™

### ğŸ”§ ç³»çµ±åŠŸèƒ½
- **æ‰¹æ¬¡æ¨è–¦**: ç‚ºå¤§é‡ç”¨æˆ¶æ‰¹æ¬¡ç”Ÿæˆæ¨è–¦
- **å³æ™‚æ¨è–¦**: ç‚ºå–®ä¸€ç”¨æˆ¶å³æ™‚ç”Ÿæˆæ¨è–¦
- **ç›¸ä¼¼å•†å“**: åŸºæ–¼å•†å“ç‰¹å¾µçš„ç›¸ä¼¼æ¨è–¦
- **è¶…åƒæ•¸èª¿å„ª**: è‡ªå‹•åŒ–è¶…åƒæ•¸æœ€ä½³åŒ–

### ğŸ“ˆ åˆ†æåŠŸèƒ½
- **è³‡æ–™å“è³ªåˆ†æ**: è©•åˆ†è³‡æ–™çš„å“è³ªè©•ä¼°
- **ç”¨æˆ¶ç•«åƒç”Ÿæˆ**: è©³ç´°çš„ç”¨æˆ¶åå¥½åˆ†æ
- **å•†å“åˆ†æ**: å•†å“å—æ­¡è¿ç¨‹åº¦å’Œé¡åˆ¥åˆ†æ
- **æ¨è–¦æ•ˆæœåˆ†æ**: æ¨è–¦çµæœçš„è©³ç´°åˆ†æ

## å®‰è£éœ€æ±‚

```bash
# Python ä¾è³´
pip install pyspark pandas matplotlib seaborn numpy scikit-learn

# æˆ–ä½¿ç”¨ poetryï¼ˆæ¨è–¦ï¼‰
poetry install
```

## å¿«é€Ÿé–‹å§‹

### 1. æº–å‚™è³‡æ–™

æ¨è–¦ç³»çµ±éœ€è¦ä»¥ä¸‹è³‡æ–™ï¼š

**è©•åˆ†è³‡æ–™ (ratings.csv)**
```csv
user_id,item_id,rating,timestamp
1,101,4.5,1609459200
1,102,3.0,1609459300
2,101,5.0,1609459400
```

**å•†å“è³‡æ–™ (products.csvï¼Œå¯é¸)**
```csv
item_id,title,category,price,brand,description
101,iPhone 13,Electronics,999.99,Apple,Latest iPhone model
102,MacBook Pro,Electronics,1999.99,Apple,Professional laptop
```

**ç”¨æˆ¶è³‡æ–™ (users.csvï¼Œå¯é¸)**
```csv
user_id,age,gender,occupation,location
1,25,M,Engineer,New York
2,32,F,Teacher,Los Angeles
```

### 2. åŸºæœ¬ä½¿ç”¨

```bash
# è¨“ç·´æ¨è–¦æ¨¡å‹
python recommendation_system.py --ratings-path ratings.csv --products-path products.csv --mode train

# ç‚ºç‰¹å®šç”¨æˆ¶ç”Ÿæˆæ¨è–¦
python recommendation_system.py --ratings-path ratings.csv --products-path products.csv --mode recommend --user-id 1

# ä½¿ç”¨ç¯„ä¾‹è³‡æ–™è¨“ç·´
python recommendation_system.py --ratings-path ./sample_ratings.csv --mode train
```

### 3. æŸ¥çœ‹çµæœ

```bash
# æŸ¥çœ‹è¨“ç·´å ±å‘Š
cat recommendation_output/training_report.json

# æŸ¥çœ‹å¯è¦–åŒ–åœ–è¡¨
open recommendation_output/rating_distribution.png
open recommendation_output/model_metrics.png
```

## è©³ç´°ä½¿ç”¨æŒ‡å—

### è³‡æ–™æ ¼å¼è¦æ±‚

#### è©•åˆ†è³‡æ–™
- **user_id**: ç”¨æˆ¶å”¯ä¸€æ¨™è­˜ç¬¦ (æ•´æ•¸)
- **item_id**: å•†å“å”¯ä¸€æ¨™è­˜ç¬¦ (æ•´æ•¸)
- **rating**: è©•åˆ†å€¼ (æµ®é»æ•¸ï¼Œé€šå¸¸ 1-5)
- **timestamp**: æ™‚é–“æˆ³ (æ•´æ•¸ï¼Œå¯é¸)

#### å•†å“è³‡æ–™
- **item_id**: å•†å“å”¯ä¸€æ¨™è­˜ç¬¦ (æ•´æ•¸)
- **title**: å•†å“æ¨™é¡Œ (å­—ä¸²)
- **category**: å•†å“é¡åˆ¥ (å­—ä¸²)
- **price**: å•†å“åƒ¹æ ¼ (æµ®é»æ•¸)
- **brand**: å“ç‰Œ (å­—ä¸²)
- **description**: å•†å“æè¿° (å­—ä¸²)

#### ç”¨æˆ¶è³‡æ–™
- **user_id**: ç”¨æˆ¶å”¯ä¸€æ¨™è­˜ç¬¦ (æ•´æ•¸)
- **age**: å¹´é½¡ (æ•´æ•¸)
- **gender**: æ€§åˆ¥ (å­—ä¸²)
- **occupation**: è·æ¥­ (å­—ä¸²)
- **location**: åœ°å€ (å­—ä¸²)

### åƒæ•¸é…ç½®

```python
# å»ºç«‹è‡ªè¨‚é…ç½®
config = RecommendationConfig(
    ratings_path="./ratings.csv",
    products_path="./products.csv",
    users_path="./users.csv",
    output_path="./output",
    model_path="./model",
    num_recommendations=10,
    min_ratings_per_user=5,
    min_ratings_per_item=5,
    test_ratio=0.2,
    als_config={
        "rank": 50,          # æ½›åœ¨å› å­æ•¸é‡
        "maxIter": 10,       # æœ€å¤§è¿­ä»£æ¬¡æ•¸
        "regParam": 0.01,    # æ­£å‰‡åŒ–åƒæ•¸
        "alpha": 1.0,        # éš±å¼åé¥‹çš„ä¿¡å¿ƒåƒæ•¸
        "coldStartStrategy": "drop",  # å†·å•Ÿå‹•ç­–ç•¥
        "nonnegative": True  # éè² ç´„æŸ
    }
)
```

### ALS è¶…åƒæ•¸èªªæ˜

- **rank**: æ½›åœ¨å› å­çš„æ•¸é‡ï¼Œå½±éŸ¿æ¨¡å‹çš„è¤‡é›œåº¦
- **maxIter**: æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼Œå½±éŸ¿æ”¶æ–‚é€Ÿåº¦
- **regParam**: æ­£å‰‡åŒ–åƒæ•¸ï¼Œé˜²æ­¢éæ“¬åˆ
- **alpha**: éš±å¼åé¥‹çš„ä¿¡å¿ƒåƒæ•¸
- **coldStartStrategy**: è™•ç†æ–°ç”¨æˆ¶/å•†å“çš„ç­–ç•¥

## ä½¿ç”¨ç¯„ä¾‹

### 1. ç”Ÿæˆç¯„ä¾‹è³‡æ–™

```bash
# ç”Ÿæˆç¯„ä¾‹è©•åˆ†è³‡æ–™
python -c "
import pandas as pd
import numpy as np
import random

# ç”Ÿæˆ 1000 ç”¨æˆ¶ï¼Œ500 å•†å“ï¼Œ50000 è©•åˆ†
users = range(1, 1001)
items = range(1, 501)
ratings = []

for _ in range(50000):
    user_id = random.choice(users)
    item_id = random.choice(items)
    rating = random.uniform(1, 5)
    timestamp = random.randint(1577836800, 1609459200)  # 2020å¹´
    ratings.append([user_id, item_id, rating, timestamp])

df = pd.DataFrame(ratings, columns=['user_id', 'item_id', 'rating', 'timestamp'])
df.to_csv('sample_ratings.csv', index=False)
print('ç¯„ä¾‹è©•åˆ†è³‡æ–™å·²ç”Ÿæˆ: sample_ratings.csv')
"

# ç”Ÿæˆç¯„ä¾‹å•†å“è³‡æ–™
python -c "
import pandas as pd
import random

categories = ['Electronics', 'Books', 'Clothing', 'Sports', 'Home']
brands = ['Apple', 'Samsung', 'Nike', 'Adidas', 'Sony']

products = []
for i in range(1, 501):
    products.append([
        i,
        f'Product {i}',
        random.choice(categories),
        round(random.uniform(10, 1000), 2),
        random.choice(brands),
        f'Description for product {i}'
    ])

df = pd.DataFrame(products, columns=['item_id', 'title', 'category', 'price', 'brand', 'description'])
df.to_csv('sample_products.csv', index=False)
print('ç¯„ä¾‹å•†å“è³‡æ–™å·²ç”Ÿæˆ: sample_products.csv')
"
```

### 2. è¨“ç·´æ¨¡å‹

```bash
# ä½¿ç”¨ç¯„ä¾‹è³‡æ–™è¨“ç·´
python recommendation_system.py \
    --ratings-path sample_ratings.csv \
    --products-path sample_products.csv \
    --mode train \
    --output-path ./results \
    --model-path ./model

# æŸ¥çœ‹è¨“ç·´çµæœ
cat results/training_report.json | jq '.model_metrics'
```

### 3. ç”Ÿæˆæ¨è–¦

```bash
# ç‚ºç”¨æˆ¶ 1 ç”Ÿæˆ 10 å€‹æ¨è–¦
python recommendation_system.py \
    --ratings-path sample_ratings.csv \
    --products-path sample_products.csv \
    --mode recommend \
    --user-id 1 \
    --num-recommendations 10

# ç‚ºç”¨æˆ¶ 100 ç”Ÿæˆ 5 å€‹æ¨è–¦
python recommendation_system.py \
    --ratings-path sample_ratings.csv \
    --products-path sample_products.csv \
    --mode recommend \
    --user-id 100 \
    --num-recommendations 5
```

### 4. æ‰¹æ¬¡æ¨è–¦

```python
# ä½¿ç”¨ Python API é€²è¡Œæ‰¹æ¬¡æ¨è–¦
from pyspark.sql import SparkSession
from recommendation_system import RecommendationSystem, RecommendationConfig

# å»ºç«‹ Spark æœƒè©±
spark = SparkSession.builder.appName("BatchRecommendation").getOrCreate()

# å»ºç«‹é…ç½®
config = RecommendationConfig(
    ratings_path="sample_ratings.csv",
    products_path="sample_products.csv",
    model_path="./model"
)

# å»ºç«‹æ¨è–¦ç³»çµ±
rec_system = RecommendationSystem(config, spark)

# ç‚ºå¤šå€‹ç”¨æˆ¶ç”Ÿæˆæ¨è–¦
user_ids = [1, 2, 3, 4, 5]
for user_id in user_ids:
    recommendations = rec_system.generate_recommendations_for_user(user_id, 10)
    print(f"ç”¨æˆ¶ {user_id} çš„æ¨è–¦:")
    for rec in recommendations["recommendations"]:
        print(f"  {rec['title']} - é æ¸¬è©•åˆ†: {rec['predicted_rating']:.2f}")
```

## é€²éšåŠŸèƒ½

### 1. è¶…åƒæ•¸èª¿å„ª

```python
# è‡ªå‹•è¶…åƒæ•¸èª¿å„ª
def tune_hyperparameters():
    # è¼‰å…¥è³‡æ–™
    ratings_df = spark.read.csv("ratings.csv", header=True, inferSchema=True)
    
    # åˆ†å‰²è³‡æ–™
    train_df, validation_df = ratings_df.randomSplit([0.8, 0.2])
    
    # åŸ·è¡Œèª¿å„ª
    best_params = rec_engine.tune_hyperparameters(train_df, validation_df)
    
    return best_params
```

### 2. å³æ™‚æ¨è–¦æœå‹™

```python
# å»ºç«‹å³æ™‚æ¨è–¦æœå‹™
class RecommendationService:
    def __init__(self, model_path):
        self.spark = SparkSession.builder.getOrCreate()
        self.model = ALSModel.load(model_path)
    
    def get_recommendations(self, user_id, num_recommendations=10):
        user_df = self.spark.createDataFrame([(user_id,)], ["user_id"])
        recommendations = self.model.recommendForUserSubset(user_df, num_recommendations)
        return recommendations.collect()
```

### 3. æ··åˆæ¨è–¦

```python
# çµåˆå”åŒéæ¿¾å’ŒåŸºæ–¼å…§å®¹çš„æ¨è–¦
class HybridRecommendation:
    def __init__(self, cf_model, content_model):
        self.cf_model = cf_model
        self.content_model = content_model
    
    def recommend(self, user_id, num_recommendations=10):
        # å”åŒéæ¿¾æ¨è–¦
        cf_recs = self.cf_model.recommend(user_id, num_recommendations)
        
        # åŸºæ–¼å…§å®¹çš„æ¨è–¦
        content_recs = self.content_model.recommend(user_id, num_recommendations)
        
        # æ··åˆæ¨è–¦çµæœ
        hybrid_recs = self.combine_recommendations(cf_recs, content_recs)
        
        return hybrid_recs
```

## è©•ä¼°æŒ‡æ¨™

### 1. æº–ç¢ºæ€§æŒ‡æ¨™

```python
# RMSE (Root Mean Square Error)
rmse = evaluator.setMetricName("rmse").evaluate(predictions)

# MAE (Mean Absolute Error)
mae = evaluator.setMetricName("mae").evaluate(predictions)

# RÂ² (æ±ºå®šä¿‚æ•¸)
r2 = evaluator.setMetricName("r2").evaluate(predictions)
```

### 2. æ’åæŒ‡æ¨™

```python
# è¨ˆç®— Precision@K
def precision_at_k(predictions, k=10):
    # å¯¦ç¾ Precision@K è¨ˆç®—
    pass

# è¨ˆç®— Recall@K
def recall_at_k(predictions, k=10):
    # å¯¦ç¾ Recall@K è¨ˆç®—
    pass

# è¨ˆç®— NDCG@K
def ndcg_at_k(predictions, k=10):
    # å¯¦ç¾ NDCG@K è¨ˆç®—
    pass
```

### 3. å¤šæ¨£æ€§æŒ‡æ¨™

```python
# è¨ˆç®—æ¨è–¦å¤šæ¨£æ€§
def calculate_diversity(recommendations, products_df):
    # è¨ˆç®—é¡åˆ¥å¤šæ¨£æ€§
    categories = recommendations.join(products_df, "item_id").select("category").distinct().count()
    total_categories = products_df.select("category").distinct().count()
    
    return categories / total_categories
```

## æ•ˆèƒ½å„ªåŒ–

### 1. Spark é…ç½®å„ªåŒ–

```python
# å„ªåŒ– Spark é…ç½®
spark = SparkSession.builder \
    .appName("RecommendationSystem") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.sql.adaptive.skewJoin.enabled", "true") \
    .config("spark.executor.memory", "8g") \
    .config("spark.executor.cores", "4") \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()
```

### 2. è³‡æ–™åˆ†å€ç­–ç•¥

```python
# æŒ‰ç”¨æˆ¶åˆ†å€
ratings_df = ratings_df.repartition("user_id")

# æŒ‰å•†å“åˆ†å€
ratings_df = ratings_df.repartition("item_id")

# ä½¿ç”¨ Bucketing
ratings_df.write \
    .bucketBy(10, "user_id") \
    .sortBy("timestamp") \
    .saveAsTable("ratings_bucketed")
```

### 3. å¿«å–ç­–ç•¥

```python
# å¿«å–ç¶“å¸¸ä½¿ç”¨çš„è³‡æ–™
ratings_df.cache()

# ä½¿ç”¨ä¸åŒçš„å„²å­˜ç´šåˆ¥
from pyspark.storagelevel import StorageLevel
ratings_df.persist(StorageLevel.MEMORY_AND_DISK_SER)
```

## éƒ¨ç½²æŒ‡å—

### 1. æœ¬åœ°éƒ¨ç½²

```bash
# å•Ÿå‹•æ¨è–¦æœå‹™
python recommendation_service.py --model-path ./model --port 8080

# æ¸¬è©¦æ¨è–¦æœå‹™
curl -X POST http://localhost:8080/recommend \
  -H "Content-Type: application/json" \
  -d '{"user_id": 1, "num_recommendations": 10}'
```

### 2. Docker éƒ¨ç½²

```dockerfile
FROM python:3.8-slim

WORKDIR /app

# å®‰è£ Java (Spark éœ€è¦)
RUN apt-get update && \
    apt-get install -y openjdk-11-jdk && \
    rm -rf /var/lib/apt/lists/*

# è¨­å®š Java ç’°å¢ƒè®Šæ•¸
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# è¤‡è£½ä¾è³´æ–‡ä»¶
COPY requirements.txt .
RUN pip install -r requirements.txt

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8080

# å•Ÿå‹•æœå‹™
CMD ["python", "recommendation_service.py", "--model-path", "./model", "--port", "8080"]
```

### 3. Kubernetes éƒ¨ç½²

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recommendation-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: recommendation-system
  template:
    metadata:
      labels:
        app: recommendation-system
    spec:
      containers:
      - name: recommendation-system
        image: recommendation-system:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
        env:
        - name: SPARK_EXECUTOR_MEMORY
          value: "4g"
        - name: SPARK_EXECUTOR_CORES
          value: "2"
---
apiVersion: v1
kind: Service
metadata:
  name: recommendation-service
spec:
  selector:
    app: recommendation-system
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
```

## ç›£æ§å’Œç¶­è­·

### 1. æ¨¡å‹æ€§èƒ½ç›£æ§

```python
# ç›£æ§æ¨è–¦å“è³ª
def monitor_recommendation_quality():
    # è¨ˆç®—å³æ™‚è©•ä¼°æŒ‡æ¨™
    current_rmse = calculate_current_rmse()
    
    # ç›£æ§æ¨è–¦è¦†è“‹ç‡
    coverage = calculate_coverage()
    
    # ç›£æ§æ¨è–¦å¤šæ¨£æ€§
    diversity = calculate_diversity()
    
    # ç™¼é€è­¦å ±
    if current_rmse > threshold:
        send_alert("Model performance degraded")
```

### 2. A/B æ¸¬è©¦

```python
# å¯¦æ–½ A/B æ¸¬è©¦
def ab_test_recommendations():
    # éš¨æ©Ÿåˆ†é…ç”¨æˆ¶åˆ°ä¸åŒçš„æ¨è–¦ç­–ç•¥
    if user_id % 2 == 0:
        return strategy_a.recommend(user_id)
    else:
        return strategy_b.recommend(user_id)
```

### 3. æ¨¡å‹æ›´æ–°

```python
# å®šæœŸæ›´æ–°æ¨¡å‹
def update_model():
    # è¼‰å…¥æ–°çš„è©•åˆ†è³‡æ–™
    new_ratings = load_new_ratings()
    
    # é‡æ–°è¨“ç·´æ¨¡å‹
    new_model = train_model(new_ratings)
    
    # è©•ä¼°æ–°æ¨¡å‹
    if evaluate_model(new_model) > current_model_performance:
        deploy_model(new_model)
```

## å¸¸è¦‹å•é¡Œå’Œè§£æ±ºæ–¹æ¡ˆ

### 1. è¨˜æ†¶é«”ä¸è¶³

```bash
# å¢åŠ  Spark è¨˜æ†¶é«”
export SPARK_EXECUTOR_MEMORY=8g
export SPARK_DRIVER_MEMORY=4g

# æˆ–åœ¨ç¨‹å¼ç¢¼ä¸­è¨­å®š
spark.conf.set("spark.executor.memory", "8g")
spark.conf.set("spark.driver.memory", "4g")
```

### 2. å†·å•Ÿå‹•å•é¡Œ

```python
# è™•ç†æ–°ç”¨æˆ¶
def handle_new_user(user_id):
    # ä½¿ç”¨ç†±é–€å•†å“æ¨è–¦
    popular_items = get_popular_items(limit=10)
    
    # æˆ–ä½¿ç”¨åŸºæ–¼å…§å®¹çš„æ¨è–¦
    content_based_recs = get_content_based_recommendations(user_id)
    
    return popular_items + content_based_recs
```

### 3. è³‡æ–™ç¨€ç–æ€§

```python
# è™•ç†ç¨€ç–è³‡æ–™
def handle_sparse_data():
    # ä½¿ç”¨éš±å¼åé¥‹
    implicit_feedback = generate_implicit_feedback()
    
    # çµåˆé¡¯å¼å’Œéš±å¼åé¥‹
    combined_data = combine_feedback(explicit_ratings, implicit_feedback)
    
    return combined_data
```

## æ“´å±•åŠŸèƒ½

### 1. æ·±åº¦å­¸ç¿’æ¨è–¦

```python
# ä½¿ç”¨æ·±åº¦å­¸ç¿’æ¨¡å‹
from pyspark.ml.recommendation import DeepFM

# å»ºç«‹ DeepFM æ¨¡å‹
deep_fm = DeepFM(
    userCol="user_id",
    itemCol="item_id",
    ratingCol="rating",
    featuresCol="features"
)

# è¨“ç·´æ¨¡å‹
deep_model = deep_fm.fit(training_data)
```

### 2. åœ–ç¥ç¶“ç¶²çµ¡

```python
# ä½¿ç”¨åœ–ç¥ç¶“ç¶²çµ¡
from pyspark.ml.recommendation import GraphSAGE

# å»ºç«‹ç”¨æˆ¶-å•†å“åœ–
user_item_graph = build_user_item_graph(ratings_df)

# è¨“ç·´ GraphSAGE æ¨¡å‹
graph_model = GraphSAGE().fit(user_item_graph)
```

### 3. å¤šç›®æ¨™æ¨è–¦

```python
# å¤šç›®æ¨™æ¨è–¦ï¼ˆé»æ“Šç‡ + è½‰æ›ç‡ï¼‰
class MultiObjectiveRecommendation:
    def __init__(self):
        self.ctr_model = build_ctr_model()
        self.cvr_model = build_cvr_model()
    
    def recommend(self, user_id):
        # çµåˆå¤šå€‹ç›®æ¨™
        ctr_scores = self.ctr_model.predict(user_id)
        cvr_scores = self.cvr_model.predict(user_id)
        
        # åŠ æ¬Šçµ„åˆ
        final_scores = 0.7 * ctr_scores + 0.3 * cvr_scores
        
        return final_scores
```

## æˆæ¬Š

æœ¬å°ˆæ¡ˆä½¿ç”¨ MIT æˆæ¬Šæ¢æ¬¾ã€‚

## è²¢ç»

æ­¡è¿æäº¤ Issues å’Œ Pull Requestsï¼

## è¯çµ¡æ–¹å¼

å¦‚æœ‰å•é¡Œï¼Œè«‹è¯çµ¡ï¼š[your-email@example.com](mailto:your-email@example.com)