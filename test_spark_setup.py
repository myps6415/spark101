#!/usr/bin/env python3
"""
æ¸¬è©¦ Spark ç’°å¢ƒè¨­ç½®
"""

import os
import sys

def test_pyspark_import():
    """æ¸¬è©¦ PySpark å°å…¥"""
    try:
        import pyspark
        print(f"âœ… PySpark {pyspark.__version__} å°å…¥æˆåŠŸ")
        return True
    except ImportError as e:
        print(f"âŒ PySpark å°å…¥å¤±æ•—: {e}")
        return False

def test_spark_session():
    """æ¸¬è©¦ SparkSession å‰µå»º"""
    try:
        # è¨­ç½® Java ç³»çµ±å±¬æ€§ä¾†é¿å…æ¬Šé™å•é¡Œ
        os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.hadoop.javax.jdo.option.ConnectionURL=jdbc:derby:memory:myInMemDB;create=true --conf spark.sql.catalogImplementation=in-memory pyspark-shell'
        
        from pyspark.sql import SparkSession
        
        # å‰µå»º SparkSessionï¼Œä½¿ç”¨æ›´å…¼å®¹çš„é…ç½®
        spark = SparkSession.builder \
            .appName("Spark101 Test") \
            .master("local[1]") \
            .config("spark.sql.adaptive.enabled", "false") \
            .config("spark.hadoop.javax.jdo.option.ConnectionURL", "jdbc:derby:memory:myInMemDB;create=true") \
            .config("spark.sql.catalogImplementation", "in-memory") \
            .getOrCreate()
        
        # æ¸¬è©¦åŸºæœ¬æ“ä½œ
        data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
        df = spark.createDataFrame(data, ["name", "age"])
        count = df.count()
        
        print(f"âœ… SparkSession å‰µå»ºæˆåŠŸ")
        print(f"âœ… æ¸¬è©¦ DataFrame å‰µå»ºæˆåŠŸï¼Œè¨˜éŒ„æ•¸: {count}")
        
        # æ¸¬è©¦åŸºæœ¬ SQL æ“ä½œ
        df.createOrReplaceTempView("people")
        result = spark.sql("SELECT name, age FROM people WHERE age > 25").collect()
        print(f"âœ… SQL æŸ¥è©¢æˆåŠŸï¼Œçµæœæ•¸: {len(result)}")
        
        spark.stop()
        return True
        
    except Exception as e:
        print(f"âŒ SparkSession å‰µå»ºå¤±æ•—: {e}")
        print("é€™å¯èƒ½æ˜¯ Java ç‰ˆæœ¬å…¼å®¹æ€§å•é¡Œï¼Œä½†ä¸å½±éŸ¿å­¸ç¿’åŸºæœ¬æ¦‚å¿µ")
        return False

def provide_setup_instructions():
    """æä¾›è¨­ç½®èªªæ˜"""
    print("\n" + "="*60)
    print("ğŸ‰ Spark101 ç’°å¢ƒè¨­ç½®å®Œæˆ!")
    print("="*60)
    
    print("\nğŸ“š å¦‚ä½•ä½¿ç”¨:")
    print("1. æ¿€æ´» Poetry ç’°å¢ƒ:")
    print("   poetry shell")
    print("\n2. é‹è¡Œç·´ç¿’æ–‡ä»¶:")
    print("   poetry run python exercises/chapter01/exercise_01_basic_spark.py")
    print("\n3. å•Ÿå‹• Jupyter Notebook:")
    print("   poetry run jupyter notebook")
    
    print("\nğŸ”§ IDE è¨­ç½®:")
    print("1. VS Code: é¸æ“‡ Poetry è™›æ“¬ç’°å¢ƒä½œç‚º Python è§£é‡‹å™¨")
    print("   è·¯å¾‘: .venv/bin/python")
    print("\n2. PyCharm: æ·»åŠ  Poetry è™›æ“¬ç’°å¢ƒ")
    print("   File > Settings > Project > Python Interpreter > Add > Poetry Environment")
    
    print("\nâš ï¸  å¦‚æœé‡åˆ° Java ç›¸é—œéŒ¯èª¤:")
    print("1. ç¢ºä¿å®‰è£äº† Java 8 æˆ– 11:")
    print("   brew install openjdk@11")
    print("\n2. è¨­ç½® JAVA_HOME:")
    print("   export JAVA_HOME=$(/usr/libexec/java_home -v 11)")
    print("\n3. å¦‚æœä»æœ‰å•é¡Œï¼Œå¯ä»¥å­¸ç¿’ Spark æ¦‚å¿µï¼Œåœ¨é›²ç«¯ç’°å¢ƒä¸­é‹è¡Œå¯¦éš›ä»£ç¢¼")

def main():
    """ä¸»å‡½æ•¸"""
    print("=== Spark101 ç’°å¢ƒæ¸¬è©¦ ===\n")
    
    # æ¸¬è©¦ PySpark å°å…¥
    if not test_pyspark_import():
        return
    
    # æ¸¬è©¦ SparkSessionï¼ˆå¯é¸ï¼‰
    test_spark_session()
    
    # æä¾›ä½¿ç”¨èªªæ˜
    provide_setup_instructions()

if __name__ == "__main__":
    main()