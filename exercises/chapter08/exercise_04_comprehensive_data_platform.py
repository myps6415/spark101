#!/usr/bin/env python3
"""
ç¬¬8ç«  ç·´ç¿’4ï¼šç¶œåˆæ•¸æ“šå¹³å°æ§‹å»º
=================================

åœ¨é€™å€‹ç¶œåˆç·´ç¿’ä¸­ï¼Œä½ å°‡æ§‹å»ºä¸€å€‹å®Œæ•´çš„æ•¸æ“šå¹³å°ï¼Œ
æ•´åˆå‰é¢æ‰€å­¸çš„æ‰€æœ‰æŠ€è¡“ï¼šæ‰¹è™•ç†ã€æµè™•ç†ã€æ©Ÿå™¨å­¸ç¿’ã€æ€§èƒ½èª¿å„ªã€‚

å­¸ç¿’ç›®æ¨™ï¼š
- è¨­è¨ˆå’Œå¯¦ç¾ç«¯åˆ°ç«¯çš„æ•¸æ“šè™•ç†ç®¡é“
- æ•´åˆæ‰¹è™•ç†å’Œæµè™•ç†
- å¯¦ç¾å¯¦æ™‚æ©Ÿå™¨å­¸ç¿’é æ¸¬
- æ‡‰ç”¨æ€§èƒ½èª¿å„ªæœ€ä½³å¯¦è¸
- æ§‹å»ºç›£æ§å’Œå‘Šè­¦ç³»çµ±
"""

import json
import os
import sys
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)
sys.path.insert(0, project_root)

from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import (BinaryClassificationEvaluator,
                                   RegressionEvaluator)
from pyspark.ml.feature import StandardScaler, StringIndexer, VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.sql import SparkSession
from pyspark.sql.functions import (array_contains, avg, broadcast, coalesce,
                                   col, collect_list, count, current_timestamp,
                                   date_format, dense_rank, explode, isnan,
                                   isnull, lag, lead, lit)
from pyspark.sql.functions import max as spark_max
from pyspark.sql.functions import min as spark_min
from pyspark.sql.functions import (ntile, rank, regexp_extract, row_number,
                                   size, split, struct)
from pyspark.sql.functions import sum as spark_sum
from pyspark.sql.functions import to_timestamp, udf, when, window
from pyspark.sql.streaming import StreamingQuery
from pyspark.sql.types import (ArrayType, BooleanType, DoubleType, IntegerType,
                               MapType, StringType, StructField, StructType,
                               TimestampType)
from pyspark.sql.window import Window
from pyspark.streaming import StreamingContext


class ComprehensiveDataPlatform:
    """ç¶œåˆæ•¸æ“šå¹³å°é¡"""

    def __init__(self, app_name: str = "ComprehensiveDataPlatform"):
        """åˆå§‹åŒ–æ•¸æ“šå¹³å°"""
        self.spark = (
            SparkSession.builder.appName(app_name)
            .master("local[*]")
            .config("spark.sql.adaptive.enabled", "true")
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
            .getOrCreate()
        )

        self.spark.sparkContext.setLogLevel("WARN")
        print(f"ğŸš€ æ•¸æ“šå¹³å°å•Ÿå‹•å®Œæˆ - Spark {self.spark.version}")

    def create_sample_datasets(self) -> Dict[str, Any]:
        """å‰µå»ºç¤ºä¾‹æ•¸æ“šé›†"""
        print("ğŸ“Š å‰µå»ºç¤ºä¾‹æ•¸æ“šé›†...")

        # 1. ç”¨æˆ¶æ•¸æ“š
        users_data = [
            (1, "Alice", 25, "Premium", "alice@example.com", "2023-01-15", "USA"),
            (2, "Bob", 30, "Standard", "bob@example.com", "2023-02-10", "UK"),
            (
                3,
                "Charlie",
                35,
                "Premium",
                "charlie@example.com",
                "2023-01-20",
                "Canada",
            ),
            (4, "Diana", 28, "Standard", "diana@example.com", "2023-03-05", "USA"),
            (5, "Eve", 32, "VIP", "eve@example.com", "2023-01-25", "Germany"),
            (6, "Frank", 27, "Standard", "frank@example.com", "2023-02-15", "France"),
            (7, "Grace", 29, "Premium", "grace@example.com", "2023-01-30", "Japan"),
            (8, "Henry", 31, "VIP", "henry@example.com", "2023-02-20", "Australia"),
        ]

        users_df = self.spark.createDataFrame(
            users_data,
            [
                "user_id",
                "name",
                "age",
                "subscription_tier",
                "email",
                "registration_date",
                "country",
            ],
        )

        # 2. ç”¢å“æ•¸æ“š
        products_data = [
            (101, "Laptop Pro", "Electronics", 1299.99, "2023-01-01", True),
            (102, "Wireless Mouse", "Electronics", 29.99, "2023-01-01", True),
            (103, "Programming Book", "Books", 49.99, "2023-01-15", True),
            (104, "Coffee Mug", "Home", 15.99, "2023-02-01", True),
            (105, "Mechanical Keyboard", "Electronics", 149.99, "2023-01-10", True),
            (106, "Desk Lamp", "Home", 79.99, "2023-01-20", True),
            (107, "Python Course", "Education", 199.99, "2023-02-15", True),
            (108, "Notebook", "Office", 12.99, "2023-01-05", True),
        ]

        products_df = self.spark.createDataFrame(
            products_data,
            [
                "product_id",
                "product_name",
                "category",
                "price",
                "launch_date",
                "is_active",
            ],
        )

        # 3. äº¤æ˜“æ•¸æ“š
        transactions_data = []
        for i in range(1, 1001):  # 1000ç­†äº¤æ˜“
            user_id = (i % 8) + 1
            product_id = (i % 8) + 101
            quantity = 1 if i % 3 == 0 else 2
            base_price = [1299.99, 29.99, 49.99, 15.99, 149.99, 79.99, 199.99, 12.99][
                product_id - 101
            ]
            amount = base_price * quantity

            # å‰µå»ºæ™‚é–“åºåˆ—
            days_ago = i % 30
            transaction_time = datetime.now() - timedelta(
                days=days_ago, hours=i % 24, minutes=i % 60
            )

            transactions_data.append(
                (
                    i,
                    user_id,
                    product_id,
                    quantity,
                    amount,
                    transaction_time.strftime("%Y-%m-%d %H:%M:%S"),
                    "completed" if i % 10 != 0 else "failed",
                )
            )

        transactions_df = self.spark.createDataFrame(
            transactions_data,
            [
                "transaction_id",
                "user_id",
                "product_id",
                "quantity",
                "amount",
                "transaction_time",
                "status",
            ],
        )

        # 4. ç”¨æˆ¶è¡Œç‚ºæ•¸æ“šï¼ˆæµå¼æ•¸æ“šæ¨¡æ“¬ï¼‰
        behaviors_data = []
        for i in range(1, 5001):  # 5000æ¢è¡Œç‚ºè¨˜éŒ„
            user_id = (i % 8) + 1
            actions = ["view", "click", "add_to_cart", "purchase", "search", "logout"]
            action = actions[i % len(actions)]

            # æ¨¡æ“¬æœƒè©±
            session_id = f"session_{(i // 10) + 1}"
            page_url = f"/page/{(i % 20) + 1}"

            timestamp = datetime.now() - timedelta(hours=i % 168)  # 7å¤©å…§çš„æ•¸æ“š

            behaviors_data.append(
                (
                    i,
                    user_id,
                    session_id,
                    action,
                    page_url,
                    timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                    f"device_{(i % 3) + 1}",  # device_1, device_2, device_3
                )
            )

        behaviors_df = self.spark.createDataFrame(
            behaviors_data,
            [
                "event_id",
                "user_id",
                "session_id",
                "action",
                "page_url",
                "timestamp",
                "device_type",
            ],
        )

        return {
            "users": users_df,
            "products": products_df,
            "transactions": transactions_df,
            "behaviors": behaviors_df,
        }

    def batch_processing_pipeline(self, datasets: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰¹è™•ç†æ•¸æ“šç®¡é“"""
        print("ğŸ”„ åŸ·è¡Œæ‰¹è™•ç†æ•¸æ“šç®¡é“...")

        users_df = datasets["users"]
        products_df = datasets["products"]
        transactions_df = datasets["transactions"]
        behaviors_df = datasets["behaviors"]

        # 1. æ•¸æ“šæ¸…ç†å’Œé©—è­‰
        print("  ğŸ§¹ æ•¸æ“šæ¸…ç†...")

        # æ¸…ç†äº¤æ˜“æ•¸æ“š
        clean_transactions = transactions_df.filter(
            (col("amount") > 0) & (col("quantity") > 0) & (col("status").isNotNull())
        ).withColumn(
            "transaction_time",
            to_timestamp(col("transaction_time"), "yyyy-MM-dd HH:mm:ss"),
        )

        # æ¸…ç†è¡Œç‚ºæ•¸æ“š
        clean_behaviors = behaviors_df.filter(col("action").isNotNull()).withColumn(
            "timestamp", to_timestamp(col("timestamp"), "yyyy-MM-dd HH:mm:ss")
        )

        # 2. æ•¸æ“šèšåˆåˆ†æ
        print("  ğŸ“Š æ•¸æ“šèšåˆåˆ†æ...")

        # ç”¨æˆ¶äº¤æ˜“æ‘˜è¦
        user_transaction_summary = clean_transactions.groupBy("user_id").agg(
            count("transaction_id").alias("total_transactions"),
            spark_sum("amount").alias("total_spent"),
            avg("amount").alias("avg_transaction_amount"),
            spark_max("transaction_time").alias("last_transaction_date"),
            countDistinct("product_id").alias("unique_products_purchased"),
        )

        # ç”¢å“éŠ·å”®åˆ†æ
        product_sales_summary = (
            clean_transactions.join(products_df, "product_id")
            .groupBy("product_id", "product_name", "category")
            .agg(
                count("transaction_id").alias("total_sales"),
                spark_sum("amount").alias("total_revenue"),
                spark_sum("quantity").alias("total_quantity_sold"),
            )
        )

        # ç”¨æˆ¶è¡Œç‚ºåˆ†æ
        user_behavior_summary = clean_behaviors.groupBy("user_id").agg(
            count("event_id").alias("total_events"),
            countDistinct("session_id").alias("total_sessions"),
            countDistinct("action").alias("unique_actions"),
            spark_sum(when(col("action") == "purchase", 1).otherwise(0)).alias(
                "purchase_events"
            ),
        )

        # 3. é«˜ç´šåˆ†æ
        print("  ğŸ¯ é«˜ç´šåˆ†æ...")

        # å®¢æˆ¶ç”Ÿå‘½é€±æœŸåƒ¹å€¼åˆ†æ
        clv_analysis = (
            user_transaction_summary.join(users_df, "user_id")
            .withColumn(
                "clv_segment",
                when(col("total_spent") > 1000, "High Value")
                .when(col("total_spent") > 500, "Medium Value")
                .otherwise("Low Value"),
            )
            .withColumn(
                "engagement_score",
                (
                    col("total_transactions") * 0.3
                    + col("unique_products_purchased") * 0.7
                ),
            )
        )

        # ç”¢å“æ¨è–¦çŸ©é™£
        product_recommendation_matrix = (
            clean_transactions.join(clean_transactions.alias("t2"), "user_id")
            .filter(col("product_id") != col("t2.product_id"))
            .groupBy("product_id", "t2.product_id")
            .agg(count("*").alias("co_purchase_count"))
            .filter(col("co_purchase_count") > 2)
        )

        return {
            "user_transaction_summary": user_transaction_summary,
            "product_sales_summary": product_sales_summary,
            "user_behavior_summary": user_behavior_summary,
            "clv_analysis": clv_analysis,
            "product_recommendation_matrix": product_recommendation_matrix,
            "clean_transactions": clean_transactions,
            "clean_behaviors": clean_behaviors,
        }

    def machine_learning_pipeline(
        self, processed_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ©Ÿå™¨å­¸ç¿’ç®¡é“"""
        print("ğŸ¤– åŸ·è¡Œæ©Ÿå™¨å­¸ç¿’ç®¡é“...")

        # 1. ç‰¹å¾µå·¥ç¨‹
        print("  ğŸ”§ ç‰¹å¾µå·¥ç¨‹...")

        clv_data = processed_data["clv_analysis"]

        # æº–å‚™æ©Ÿå™¨å­¸ç¿’ç‰¹å¾µ
        ml_features = clv_data.select(
            "user_id",
            "age",
            "total_transactions",
            "total_spent",
            "avg_transaction_amount",
            "unique_products_purchased",
            "engagement_score",
            when(col("subscription_tier") == "VIP", 1)
            .when(col("subscription_tier") == "Premium", 0.7)
            .when(col("subscription_tier") == "Standard", 0.3)
            .otherwise(0)
            .alias("tier_score"),
            when(col("clv_segment") == "High Value", 1)
            .otherwise(0)
            .alias("is_high_value"),
        ).na.drop()

        # ç‰¹å¾µå‘é‡åŒ–
        feature_cols = [
            "age",
            "total_transactions",
            "total_spent",
            "avg_transaction_amount",
            "unique_products_purchased",
            "engagement_score",
            "tier_score",
        ]

        assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

        # æ¨™æº–åŒ–
        scaler = StandardScaler(inputCol="features", outputCol="scaled_features")

        # 2. åˆ†é¡æ¨¡å‹ï¼šé æ¸¬é«˜åƒ¹å€¼å®¢æˆ¶
        print("  ğŸ“ˆ è¨“ç·´åˆ†é¡æ¨¡å‹...")

        # æº–å‚™åˆ†é¡æ•¸æ“š
        classification_data = assembler.transform(ml_features)
        scaled_data = scaler.fit(classification_data).transform(classification_data)

        # åˆ†å‰²æ•¸æ“š
        train_data, test_data = scaled_data.randomSplit([0.8, 0.2], seed=42)

        # è¨“ç·´éš¨æ©Ÿæ£®æ—åˆ†é¡å™¨
        rf_classifier = RandomForestClassifier(
            featuresCol="scaled_features",
            labelCol="is_high_value",
            numTrees=10,
            maxDepth=5,
            seed=42,
        )

        rf_model = rf_classifier.fit(train_data)

        # é æ¸¬
        predictions = rf_model.transform(test_data)

        # è©•ä¼°
        evaluator = BinaryClassificationEvaluator(
            labelCol="is_high_value", rawPredictionCol="rawPrediction"
        )
        auc = evaluator.evaluate(predictions)

        # 3. èšé¡åˆ†æï¼šå®¢æˆ¶åˆ†ç¾¤
        print("  ğŸ¯ å®¢æˆ¶èšé¡åˆ†æ...")

        kmeans = KMeans(featuresCol="scaled_features", k=4, seed=42)

        kmeans_model = kmeans.fit(scaled_data)
        clustered_data = kmeans_model.transform(scaled_data)

        # åˆ†æèšé¡çµæœ
        cluster_analysis = (
            clustered_data.groupBy("prediction")
            .agg(
                count("*").alias("cluster_size"),
                avg("total_spent").alias("avg_spent"),
                avg("engagement_score").alias("avg_engagement"),
                avg("age").alias("avg_age"),
            )
            .orderBy("prediction")
        )

        return {
            "classification_model": rf_model,
            "classification_predictions": predictions,
            "classification_auc": auc,
            "clustering_model": kmeans_model,
            "clustered_data": clustered_data,
            "cluster_analysis": cluster_analysis,
            "ml_features": scaled_data,
        }

    def streaming_pipeline(self, datasets: Dict[str, Any]) -> None:
        """æµè™•ç†ç®¡é“ï¼ˆæ¨¡æ“¬ï¼‰"""
        print("ğŸŒŠ æ¨¡æ“¬æµè™•ç†ç®¡é“...")

        # æ³¨æ„ï¼šé€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„æµè™•ç†ç¤ºä¾‹
        # åœ¨å¯¦éš›ç’°å¢ƒä¸­ï¼Œä½ æœƒå¾ Kafkaã€Kinesis ç­‰æµæ•¸æ“šæºè®€å–

        behaviors_df = datasets["behaviors"]

        # æ¨¡æ“¬æµè™•ç†ï¼šå¯¦æ™‚ç”¨æˆ¶è¡Œç‚ºåˆ†æ
        print("  ğŸ“¡ å¯¦æ™‚ç”¨æˆ¶è¡Œç‚ºåˆ†æ...")

        # æŒ‰æ™‚é–“çª—å£èšåˆ
        windowed_behaviors = (
            behaviors_df.withColumn(
                "hour", date_format(col("timestamp"), "yyyy-MM-dd HH")
            )
            .groupBy("hour", "action")
            .agg(count("*").alias("action_count"))
            .orderBy("hour", "action")
        )

        # å¯¦æ™‚ç•°å¸¸æª¢æ¸¬
        anomaly_threshold = 100  # æ¯å°æ™‚è¶…é100æ¬¡åŒä¸€è¡Œç‚ºè¦–ç‚ºç•°å¸¸

        anomalies = windowed_behaviors.filter(col("action_count") > anomaly_threshold)

        print(f"  ğŸš¨ æª¢æ¸¬åˆ° {anomalies.count()} å€‹ç•°å¸¸è¡Œç‚ºæ¨¡å¼")

        if anomalies.count() > 0:
            print("  ç•°å¸¸è¡Œç‚ºè©³æƒ…ï¼š")
            anomalies.show(10, False)

        # å¯¦æ™‚æ¨è–¦
        print("  ğŸ’¡ å¯¦æ™‚æ¨è–¦è¨ˆç®—...")

        # æ¨¡æ“¬å¯¦æ™‚æ¨è–¦é‚è¼¯
        recent_behaviors = behaviors_df.filter(
            col("timestamp") > (current_timestamp() - expr("INTERVAL 1 HOUR"))
        )

        popular_pages = (
            recent_behaviors.groupBy("page_url")
            .agg(count("*").alias("visit_count"))
            .orderBy(desc("visit_count"))
            .limit(5)
        )

        print("  ğŸ”¥ å¯¦æ™‚ç†±é–€é é¢ï¼š")
        popular_pages.show(5, False)

    def performance_optimization(self, datasets: Dict[str, Any]) -> Dict[str, Any]:
        """æ€§èƒ½å„ªåŒ–ç¤ºä¾‹"""
        print("âš¡ æ‡‰ç”¨æ€§èƒ½å„ªåŒ–...")

        transactions_df = datasets["transactions"]
        users_df = datasets["users"]

        # 1. åˆ†å€å„ªåŒ–
        print("  ğŸ“Š åˆ†å€å„ªåŒ–...")

        # æŒ‰æ—¥æœŸåˆ†å€äº¤æ˜“æ•¸æ“š
        partitioned_transactions = transactions_df.withColumn(
            "transaction_date",
            date_format(to_timestamp(col("transaction_time")), "yyyy-MM-dd"),
        ).repartition(4, col("transaction_date"))

        # 2. ç·©å­˜ç­–ç•¥
        print("  ğŸ’¾ æ‡‰ç”¨ç·©å­˜ç­–ç•¥...")

        # ç·©å­˜ç¶“å¸¸ä½¿ç”¨çš„ç”¨æˆ¶æ•¸æ“š
        cached_users = users_df.cache()

        # 3. å»£æ’­é€£æ¥å„ªåŒ–
        print("  ğŸ“¡ å»£æ’­é€£æ¥å„ªåŒ–...")

        # å°‡å°è¡¨å»£æ’­ç”¨æ–¼é€£æ¥
        user_transactions = partitioned_transactions.join(
            broadcast(cached_users), "user_id"
        )

        # è§¸ç™¼ç·©å­˜
        cached_users.count()

        # 4. åˆ—å¼å­˜å„²å„ªåŒ–
        print("  ğŸ“‹ æŸ¥è©¢å„ªåŒ–...")

        # åªé¸æ“‡éœ€è¦çš„åˆ—
        optimized_query = user_transactions.select(
            "transaction_id", "user_id", "name", "amount", "subscription_tier"
        ).filter(col("amount") > 100)

        query_count = optimized_query.count()
        print(f"  å„ªåŒ–æŸ¥è©¢çµæœæ•¸é‡: {query_count}")

        return {
            "partitioned_transactions": partitioned_transactions,
            "cached_users": cached_users,
            "optimized_query": optimized_query,
        }

    def monitoring_and_alerting(
        self, processed_data: Dict[str, Any], ml_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç›£æ§å’Œå‘Šè­¦ç³»çµ±"""
        print("ğŸ“Š ç›£æ§å’Œå‘Šè­¦ç³»çµ±...")

        # 1. æ•¸æ“šè³ªé‡ç›£æ§
        print("  ğŸ” æ•¸æ“šè³ªé‡æª¢æŸ¥...")

        transactions_summary = processed_data["user_transaction_summary"]

        quality_metrics = {
            "total_users": transactions_summary.count(),
            "users_with_transactions": transactions_summary.filter(
                col("total_transactions") > 0
            ).count(),
            "high_value_users": transactions_summary.filter(
                col("total_spent") > 1000
            ).count(),
            "avg_transaction_value": transactions_summary.agg(
                avg("avg_transaction_amount")
            ).collect()[0][0],
        }

        # 2. æ¨¡å‹æ€§èƒ½ç›£æ§
        print("  ğŸ¤– æ¨¡å‹æ€§èƒ½ç›£æ§...")

        model_metrics = {
            "classification_auc": ml_results["classification_auc"],
            "total_predictions": ml_results["classification_predictions"].count(),
            "high_value_predictions": ml_results["classification_predictions"]
            .filter(col("prediction") == 1)
            .count(),
        }

        # 3. æ¥­å‹™æŒ‡æ¨™ç›£æ§
        print("  ğŸ’¼ æ¥­å‹™æŒ‡æ¨™ç›£æ§...")

        business_metrics = (
            processed_data["product_sales_summary"]
            .agg(
                spark_sum("total_revenue").alias("total_platform_revenue"),
                avg("total_revenue").alias("avg_product_revenue"),
                count("product_id").alias("active_products"),
            )
            .collect()[0]
        )

        # 4. å‘Šè­¦è¦å‰‡
        print("  ğŸš¨ å‘Šè­¦æª¢æŸ¥...")

        alerts = []

        # æ•¸æ“šè³ªé‡å‘Šè­¦
        if (
            quality_metrics["users_with_transactions"] / quality_metrics["total_users"]
            < 0.8
        ):
            alerts.append("è­¦å‘Šï¼šç”¨æˆ¶äº¤æ˜“åƒèˆ‡ç‡ä½æ–¼80%")

        # æ¨¡å‹æ€§èƒ½å‘Šè­¦
        if model_metrics["classification_auc"] < 0.7:
            alerts.append("è­¦å‘Šï¼šåˆ†é¡æ¨¡å‹AUCä½æ–¼0.7")

        # æ¥­å‹™æŒ‡æ¨™å‘Šè­¦
        if business_metrics["total_platform_revenue"] < 50000:
            alerts.append("è­¦å‘Šï¼šå¹³å°ç¸½æ”¶å…¥ä½æ–¼é æœŸ")

        monitoring_report = {
            "quality_metrics": quality_metrics,
            "model_metrics": model_metrics,
            "business_metrics": dict(business_metrics.asDict()),
            "alerts": alerts,
            "timestamp": datetime.now().isoformat(),
        }

        return monitoring_report

    def generate_comprehensive_report(
        self,
        datasets: Dict[str, Any],
        processed_data: Dict[str, Any],
        ml_results: Dict[str, Any],
        monitoring_report: Dict[str, Any],
    ) -> None:
        """ç”Ÿæˆç¶œåˆå ±å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆç¶œåˆæ•¸æ“šå¹³å°å ±å‘Š...")

        print("\n" + "=" * 80)
        print("ğŸ¢ ç¶œåˆæ•¸æ“šå¹³å°åˆ†æå ±å‘Š")
        print("=" * 80)

        # 1. æ•¸æ“šæ¦‚è¦½
        print("\nğŸ“Š æ•¸æ“šæ¦‚è¦½ï¼š")
        print(f"  â€¢ ç”¨æˆ¶ç¸½æ•¸: {datasets['users'].count()}")
        print(f"  â€¢ ç”¢å“ç¸½æ•¸: {datasets['products'].count()}")
        print(f"  â€¢ äº¤æ˜“ç¸½æ•¸: {datasets['transactions'].count()}")
        print(f"  â€¢ è¡Œç‚ºäº‹ä»¶ç¸½æ•¸: {datasets['behaviors'].count()}")

        # 2. æ¥­å‹™æ´å¯Ÿ
        print("\nğŸ’¼ æ¥­å‹™æ´å¯Ÿï¼š")

        # ç†±é–€ç”¢å“
        top_products = (
            processed_data["product_sales_summary"]
            .orderBy(desc("total_revenue"))
            .limit(3)
        )
        print("  ğŸ”¥ ç†±é–€ç”¢å“TOP3ï¼š")
        for row in top_products.collect():
            print(f"    - {row.product_name}: ${row.total_revenue:,.2f}")

        # å®¢æˆ¶åƒ¹å€¼åˆ†ä½ˆ
        clv_distribution = processed_data["clv_analysis"].groupBy("clv_segment").count()
        print("  ğŸ’ å®¢æˆ¶åƒ¹å€¼åˆ†ä½ˆï¼š")
        for row in clv_distribution.collect():
            print(f"    - {row.clv_segment}: {row.count} ä½å®¢æˆ¶")

        # 3. æ©Ÿå™¨å­¸ç¿’çµæœ
        print("\nğŸ¤– æ©Ÿå™¨å­¸ç¿’çµæœï¼š")
        print(f"  â€¢ åˆ†é¡æ¨¡å‹AUC: {ml_results['classification_auc']:.3f}")

        cluster_summary = ml_results["cluster_analysis"]
        print("  ğŸ¯ å®¢æˆ¶èšé¡åˆ†æï¼š")
        for row in cluster_summary.collect():
            print(
                f"    - ç¾¤çµ„ {row.prediction}: {row.cluster_size} äºº, å¹³å‡æ¶ˆè²» ${row.avg_spent:,.2f}"
            )

        # 4. æ€§èƒ½æŒ‡æ¨™
        print("\nâš¡ æ€§èƒ½å„ªåŒ–æ•ˆæœï¼š")
        print("  âœ… æ‡‰ç”¨äº†æ•¸æ“šåˆ†å€å„ªåŒ–")
        print("  âœ… ä½¿ç”¨äº†å»£æ’­é€£æ¥")
        print("  âœ… æ‡‰ç”¨äº†ç·©å­˜ç­–ç•¥")
        print("  âœ… å¯¦æ–½äº†æŸ¥è©¢å„ªåŒ–")

        # 5. ç›£æ§å‘Šè­¦
        print("\nğŸ“Š ç›£æ§ç‹€æ…‹ï¼š")
        print(
            f"  â€¢ æ•¸æ“šè³ªé‡å¾—åˆ†: {monitoring_report['quality_metrics']['users_with_transactions'] / monitoring_report['quality_metrics']['total_users'] * 100:.1f}%"
        )
        print(
            f"  â€¢ æ¨¡å‹æ€§èƒ½: AUC {monitoring_report['model_metrics']['classification_auc']:.3f}"
        )
        print(
            f"  â€¢ å¹³å°ç¸½æ”¶å…¥: ${monitoring_report['business_metrics']['total_platform_revenue']:,.2f}"
        )

        if monitoring_report["alerts"]:
            print("\nğŸš¨ ç³»çµ±å‘Šè­¦ï¼š")
            for alert in monitoring_report["alerts"]:
                print(f"  âš ï¸  {alert}")
        else:
            print("\nâœ… ç³»çµ±é‹è¡Œæ­£å¸¸ï¼Œç„¡å‘Šè­¦")

        # 6. å»ºè­°å’Œä¸‹ä¸€æ­¥
        print("\nğŸ’¡ å»ºè­°å’Œä¸‹ä¸€æ­¥ï¼š")
        print("  1. è€ƒæ…®å¢åŠ æ›´å¤šç‰¹å¾µä¾†æå‡æ¨¡å‹æ€§èƒ½")
        print("  2. å¯¦æ–½A/Bæ¸¬è©¦ä¾†å„ªåŒ–æ¨è–¦ç®—æ³•")
        print("  3. å¢åŠ å¯¦æ™‚æµè™•ç†èƒ½åŠ›")
        print("  4. å»ºç«‹æ›´å®Œå–„çš„æ•¸æ“šæ²»ç†æµç¨‹")
        print("  5. æ“´å±•åˆ°å¤šé›²éƒ¨ç½²æ¶æ§‹")

        print("\n" + "=" * 80)
        print("ğŸ“ˆ æ•¸æ“šå¹³å°åˆ†æå®Œæˆï¼")
        print("=" * 80)

    def run_comprehensive_pipeline(self) -> None:
        """é‹è¡Œå®Œæ•´çš„æ•¸æ“šå¹³å°ç®¡é“"""
        print("ğŸš€ å•Ÿå‹•ç¶œåˆæ•¸æ“šå¹³å°...")

        try:
            # 1. å‰µå»ºç¤ºä¾‹æ•¸æ“š
            datasets = self.create_sample_datasets()

            # 2. æ‰¹è™•ç†ç®¡é“
            processed_data = self.batch_processing_pipeline(datasets)

            # 3. æ©Ÿå™¨å­¸ç¿’ç®¡é“
            ml_results = self.machine_learning_pipeline(processed_data)

            # 4. æµè™•ç†ç®¡é“ï¼ˆæ¨¡æ“¬ï¼‰
            self.streaming_pipeline(datasets)

            # 5. æ€§èƒ½å„ªåŒ–
            optimization_results = self.performance_optimization(datasets)

            # 6. ç›£æ§å’Œå‘Šè­¦
            monitoring_report = self.monitoring_and_alerting(processed_data, ml_results)

            # 7. ç”Ÿæˆç¶œåˆå ±å‘Š
            self.generate_comprehensive_report(
                datasets, processed_data, ml_results, monitoring_report
            )

        except Exception as e:
            print(f"âŒ æ•¸æ“šå¹³å°åŸ·è¡Œå‡ºéŒ¯: {e}")
            raise

        finally:
            self.cleanup()

    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        print("ğŸ§¹ æ¸…ç† Spark è³‡æº...")
        self.spark.stop()


def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 80)
    print("ğŸ¯ ç¬¬8ç«  ç·´ç¿’4ï¼šç¶œåˆæ•¸æ“šå¹³å°æ§‹å»º")
    print("=" * 80)

    # å‰µå»ºä¸¦é‹è¡Œæ•¸æ“šå¹³å°
    platform = ComprehensiveDataPlatform()
    platform.run_comprehensive_pipeline()


if __name__ == "__main__":
    main()


"""
ç·´ç¿’ä»»å‹™
========

åŸºç¤ä»»å‹™ï¼š
1. é‹è¡Œå®Œæ•´çš„æ•¸æ“šå¹³å°ç®¡é“
2. ç†è§£æ¯å€‹çµ„ä»¶çš„åŠŸèƒ½å’Œä½œç”¨
3. åˆ†æç”Ÿæˆçš„æ¥­å‹™æ´å¯Ÿå ±å‘Š

é€²éšä»»å‹™ï¼š
1. ä¿®æ”¹æ©Ÿå™¨å­¸ç¿’æ¨¡å‹åƒæ•¸ï¼Œè§€å¯Ÿæ€§èƒ½è®ŠåŒ–
2. æ·»åŠ æ–°çš„ç‰¹å¾µå·¥ç¨‹æ­¥é©Ÿ
3. å¯¦ç¾æ›´è¤‡é›œçš„ç•°å¸¸æª¢æ¸¬ç®—æ³•
4. å„ªåŒ–æµè™•ç†ç®¡é“çš„æ€§èƒ½

é«˜ç´šä»»å‹™ï¼š
1. é›†æˆå¤–éƒ¨æ•¸æ“šæºï¼ˆå¦‚APIæ•¸æ“šï¼‰
2. å¯¦ç¾æ¨¡å‹çš„è‡ªå‹•é‡è¨“ç·´æ©Ÿåˆ¶
3. å»ºç«‹æ›´å®Œå–„çš„æ•¸æ“šè¡€ç·£è¿½è¹¤
4. è¨­è¨ˆç½é›£æ¢å¾©å’Œæ•…éšœè½‰ç§»æ–¹æ¡ˆ

æ“´å±•ç·´ç¿’ï¼š
1. å°‡æ‰¹è™•ç†å’Œæµè™•ç†çµæœå­˜å„²åˆ°ä¸åŒçš„å­˜å„²ç³»çµ±
2. å¯¦ç¾æ•¸æ“šç‰ˆæœ¬æ§åˆ¶å’Œå›æ»¾æ©Ÿåˆ¶
3. æ·»åŠ æ•¸æ“šéš±ç§ä¿è­·æªæ–½
4. å»ºç«‹æˆæœ¬ç›£æ§å’Œå„ªåŒ–ç³»çµ±

å­¸ç¿’é‡é»ï¼š
- ç«¯åˆ°ç«¯æ•¸æ“šè™•ç†ç®¡é“è¨­è¨ˆ
- æ‰¹æµä¸€é«”åŒ–æ•¸æ“šè™•ç†
- æ©Ÿå™¨å­¸ç¿’èˆ‡æ•¸æ“šå·¥ç¨‹çš„é›†æˆ
- æ€§èƒ½èª¿å„ªæœ€ä½³å¯¦è¸
- ç”Ÿç”¢ç’°å¢ƒç›£æ§å’Œé‹ç¶­

æ€è€ƒå•é¡Œï¼š
1. å¦‚ä½•è¨­è¨ˆå¯æ“´å±•çš„æ•¸æ“šå¹³å°æ¶æ§‹ï¼Ÿ
2. å¦‚ä½•å¹³è¡¡å¯¦æ™‚æ€§å’Œæº–ç¢ºæ€§çš„éœ€æ±‚ï¼Ÿ
3. å¦‚ä½•ç¢ºä¿æ•¸æ“šè³ªé‡å’Œä¸€è‡´æ€§ï¼Ÿ
4. å¦‚ä½•å¯¦ç¾æœ‰æ•ˆçš„æˆæœ¬æ§åˆ¶ï¼Ÿ
5. å¦‚ä½•è™•ç†æ•¸æ“šæ²»ç†å’Œåˆè¦è¦æ±‚ï¼Ÿ
"""
