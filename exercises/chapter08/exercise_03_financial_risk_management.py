#!/usr/bin/env python3
"""
ç¬¬8ç« ç·´ç¿’3ï¼šé‡‘èé¢¨éšªç®¡ç†ç³»çµ±
å»ºç«‹é‡‘èé¢¨éšªå¯¦æ™‚ç›£æ§ç³»çµ±ï¼Œæª¢æ¸¬ç•°å¸¸äº¤æ˜“ã€æ¬ºè©è¡Œç‚ºã€å¸‚å ´é¢¨éšªç­‰
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, count, avg, sum as spark_sum, max as spark_max, \
    min as spark_min, stddev, window, from_json, to_json, struct, \
    explode, collect_list, array_contains, split, regexp_extract, \
    current_timestamp, unix_timestamp, date_format, hour, dayofweek, \
    row_number, dense_rank, lag, lead, first, last, \
    abs as spark_abs, sqrt, pow as spark_pow, expr, broadcast, \
    rand, round as spark_round, desc, asc, lit, array, concat_ws, \
    percentile_approx, var_pop, skewness, kurtosis
from pyspark.sql.streaming import GroupState, GroupStateTimeout
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, \
    TimestampType, BooleanType, ArrayType, MapType
from pyspark.sql.window import Window

from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, Bucketizer, \
    PCA, PolynomialExpansion, QuantileDiscretizer
from pyspark.ml.clustering import KMeans, GaussianMixture
from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, \
    GBTClassifier, DecisionTreeClassifier
from pyspark.ml.regression import LinearRegression, RandomForestRegressor
from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator, \
    MulticlassClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml import Pipeline

import time
import random
import json
import math
from datetime import datetime, timedelta
import numpy as np

def main():
    # å‰µå»º SparkSession
    spark = SparkSession.builder \
        .appName("é‡‘èé¢¨éšªç®¡ç†ç³»çµ±") \
        .master("local[*]") \
        .config("spark.sql.streaming.checkpointLocation", "/tmp/risk_management_checkpoint") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print("=== ç¬¬8ç« ç·´ç¿’3ï¼šé‡‘èé¢¨éšªç®¡ç†ç³»çµ± ===")
    
    # 1. æ•¸æ“šæ¨¡å‹å®šç¾©
    print("\n1. å®šç¾©é‡‘èæ•¸æ“šæ¨¡å‹:")
    
    # 1.1 äº¤æ˜“æ•¸æ“šæ¨¡å‹
    transaction_schema = StructType([
        StructField("transaction_id", StringType(), True),
        StructField("account_id", StringType(), True),
        StructField("customer_id", StringType(), True),
        StructField("transaction_type", StringType(), True),  # transfer, withdrawal, deposit, payment
        StructField("amount", DoubleType(), True),
        StructField("currency", StringType(), True),
        StructField("counterparty_account", StringType(), True),
        StructField("counterparty_bank", StringType(), True),
        StructField("country_code", StringType(), True),
        StructField("channel", StringType(), True),  # atm, online, mobile, branch
        StructField("timestamp", TimestampType(), True),
        StructField("description", StringType(), True),
        StructField("merchant_category", StringType(), True),
        StructField("is_international", BooleanType(), True),
        StructField("risk_score", DoubleType(), True)
    ])
    
    # 1.2 å¸‚å ´æ•¸æ“šæ¨¡å‹
    market_data_schema = StructType([
        StructField("symbol", StringType(), True),
        StructField("price", DoubleType(), True),
        StructField("volume", IntegerType(), True),
        StructField("bid_price", DoubleType(), True),
        StructField("ask_price", DoubleType(), True),
        StructField("timestamp", TimestampType(), True),
        StructField("exchange", StringType(), True),
        StructField("sector", StringType(), True),
        StructField("market_cap", DoubleType(), True),
        StructField("volatility", DoubleType(), True)
    ])
    
    # 1.3 å®¢æˆ¶æª”æ¡ˆæ¨¡å‹
    customer_profile_schema = StructType([
        StructField("customer_id", StringType(), True),
        StructField("age", IntegerType(), True),
        StructField("income_level", StringType(), True),
        StructField("occupation", StringType(), True),
        StructField("country", StringType(), True),
        StructField("account_type", StringType(), True),
        StructField("credit_score", IntegerType(), True),
        StructField("risk_tolerance", StringType(), True),
        StructField("kyc_status", StringType(), True),
        StructField("account_balance", DoubleType(), True),
        StructField("last_updated", TimestampType(), True)
    ])
    
    print("é‡‘èæ•¸æ“šæ¨¡å‹å®šç¾©å®Œæˆ")
    
    # 2. é‡‘èæ•¸æ“šç”Ÿæˆå™¨
    print("\n2. é‡‘èæ•¸æ“šç”Ÿæˆå™¨:")
    
    class FinancialDataGenerator:
        """é‡‘èæ•¸æ“šç”Ÿæˆå™¨"""
        
        def __init__(self, spark_session):
            self.spark = spark_session
            self.customers = [f"customer_{i}" for i in range(1, 10001)]
            self.accounts = [f"account_{i}" for i in range(1, 15001)]
            self.symbols = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'META', 'NVDA', 'NFLX', 'JPM', 'BAC']
            
        def generate_transaction_stream(self):
            """ç”Ÿæˆäº¤æ˜“æ•¸æ“šæµ"""
            
            base_stream = self.spark.readStream \
                .format("rate") \
                .option("rowsPerSecond", 100) \
                .option("numPartitions", 4) \
                .load()
            
            transaction_stream = base_stream.select(
                col("timestamp"),
                col("value")
            ).withColumn(
                "transaction_id", concat_ws("", lit("txn_"), col("value"))
            ).withColumn(
                "account_id", concat_ws("", lit("account_"), (col("value") % 15000) + 1)
            ).withColumn(
                "customer_id", concat_ws("", lit("customer_"), (col("value") % 10000) + 1)
            ).withColumn(
                "transaction_type",
                when(col("value") % 10 == 0, "transfer")
                .when(col("value") % 10 == 1, "withdrawal")
                .when(col("value") % 10 == 2, "deposit")
                .when(col("value") % 10 == 3, "payment")
                .otherwise("transfer")
            ).withColumn(
                "amount",
                when(col("transaction_type") == "transfer", 
                     when(col("value") % 1000 == 0, (col("value") % 100000) + 50000)  # å¤§é¡è½‰å¸³
                     .otherwise((col("value") % 10000) + 100))
                .when(col("transaction_type") == "withdrawal",
                     (col("value") % 5000) + 100)
                .when(col("transaction_type") == "deposit", 
                     (col("value") % 20000) + 500)
                .otherwise((col("value") % 1000) + 50)
            ).withColumn(
                "currency",
                when(col("value") % 20 == 0, "EUR")
                .when(col("value") % 30 == 0, "GBP")
                .when(col("value") % 40 == 0, "JPY")
                .when(col("value") % 50 == 0, "CNY")
                .otherwise("USD")
            ).withColumn(
                "counterparty_account", 
                concat_ws("", lit("cp_account_"), (col("value") % 5000) + 1)
            ).withColumn(
                "counterparty_bank",
                when(col("value") % 5 == 0, "BANK_A")
                .when(col("value") % 5 == 1, "BANK_B")
                .when(col("value") % 5 == 2, "BANK_C")
                .when(col("value") % 5 == 3, "BANK_D")
                .otherwise("BANK_E")
            ).withColumn(
                "country_code",
                when(col("value") % 10 == 0, "US")
                .when(col("value") % 10 == 1, "UK")
                .when(col("value") % 10 == 2, "CN")
                .when(col("value") % 10 == 3, "JP")
                .when(col("value") % 10 == 4, "DE")
                .otherwise("FR")
            ).withColumn(
                "channel",
                when(col("value") % 4 == 0, "online")
                .when(col("value") % 4 == 1, "mobile")
                .when(col("value") % 4 == 2, "atm")
                .otherwise("branch")
            ).withColumn(
                "description",
                concat_ws(" ", col("transaction_type"), lit("transaction"))
            ).withColumn(
                "merchant_category",
                when(col("transaction_type") == "payment",
                     when(col("value") % 6 == 0, "grocery")
                     .when(col("value") % 6 == 1, "gas")
                     .when(col("value") % 6 == 2, "restaurant")
                     .when(col("value") % 6 == 3, "retail")
                     .when(col("value") % 6 == 4, "online")
                     .otherwise("other"))
                .otherwise("N/A")
            ).withColumn(
                "is_international",
                col("country_code") != "US"
            ).withColumn(
                "risk_score",
                when(col("amount") > 50000, rand() * 0.3 + 0.7)  # å¤§é¡äº¤æ˜“é«˜é¢¨éšª
                .when(col("is_international"), rand() * 0.2 + 0.5)  # åœ‹éš›äº¤æ˜“ä¸­ç­‰é¢¨éšª
                .when(col("channel") == "atm", rand() * 0.1 + 0.2)  # ATMäº¤æ˜“ä½é¢¨éšª
                .otherwise(rand() * 0.4 + 0.1)  # å…¶ä»–äº¤æ˜“
            ).select([field.name for field in transaction_schema.fields])
            
            return transaction_stream
        
        def generate_market_data_stream(self):
            """ç”Ÿæˆå¸‚å ´æ•¸æ“šæµ"""
            
            base_stream = self.spark.readStream \
                .format("rate") \
                .option("rowsPerSecond", 30) \
                .option("numPartitions", 2) \
                .load()
            
            market_stream = base_stream.select(
                col("timestamp"),
                col("value")
            ).withColumn(
                "symbol_idx", col("value") % 10
            ).withColumn(
                "symbol",
                when(col("symbol_idx") == 0, "AAPL")
                .when(col("symbol_idx") == 1, "GOOGL")
                .when(col("symbol_idx") == 2, "MSFT")
                .when(col("symbol_idx") == 3, "AMZN")
                .when(col("symbol_idx") == 4, "TSLA")
                .when(col("symbol_idx") == 5, "META")
                .when(col("symbol_idx") == 6, "NVDA")
                .when(col("symbol_idx") == 7, "NFLX")
                .when(col("symbol_idx") == 8, "JPM")
                .otherwise("BAC")
            ).withColumn(
                "base_price",
                when(col("symbol") == "AAPL", 150.0)
                .when(col("symbol") == "GOOGL", 2500.0)
                .when(col("symbol") == "MSFT", 300.0)
                .when(col("symbol") == "AMZN", 3000.0)
                .when(col("symbol") == "TSLA", 800.0)
                .when(col("symbol") == "META", 200.0)
                .when(col("symbol") == "NVDA", 400.0)
                .when(col("symbol") == "NFLX", 350.0)
                .when(col("symbol") == "JPM", 120.0)
                .otherwise(50.0)
            ).withColumn(
                "price_change", (rand() - 0.5) * 0.1  # Â±5% éš¨æ©Ÿè®Šå‹•
            ).withColumn(
                "price", col("base_price") * (1 + col("price_change"))
            ).withColumn(
                "volume", (col("value") % 1000000) + 100000
            ).withColumn(
                "spread", col("base_price") * 0.001  # 0.1% é»å·®
            ).withColumn(
                "bid_price", col("price") - col("spread") / 2
            ).withColumn(
                "ask_price", col("price") + col("spread") / 2
            ).withColumn(
                "exchange",
                when(col("value") % 3 == 0, "NYSE")
                .when(col("value") % 3 == 1, "NASDAQ")
                .otherwise("AMEX")
            ).withColumn(
                "sector",
                when(col("symbol").isin("AAPL", "GOOGL", "MSFT", "AMZN", "META", "NVDA", "NFLX"), "Technology")
                .when(col("symbol") == "TSLA", "Automotive")
                .otherwise("Financial")
            ).withColumn(
                "market_cap", col("price") * 1000000000  # ç°¡åŒ–å¸‚å€¼è¨ˆç®—
            ).withColumn(
                "volatility", spark_abs(col("price_change")) * 10  # æ³¢å‹•ç‡
            ).select([field.name for field in market_data_schema.fields])
            
            return market_stream
    
    # 3. æ¬ºè©æª¢æ¸¬ç³»çµ±
    print("\n3. æ™ºèƒ½æ¬ºè©æª¢æ¸¬ç³»çµ±:")
    
    class FraudDetectionSystem:
        """æ¬ºè©æª¢æ¸¬ç³»çµ±"""
        
        def __init__(self, spark_session):
            self.spark = spark_session
            self.fraud_models = {}
        
        def rule_based_fraud_detection(self, transaction_stream):
            """åŸºæ–¼è¦å‰‡çš„æ¬ºè©æª¢æ¸¬"""
            
            # è¨ˆç®—å®¢æˆ¶è¡Œç‚ºçµ±è¨ˆ
            customer_behavior = transaction_stream \
                .withWatermark("timestamp", "1 hour") \
                .groupBy(
                    window(col("timestamp"), "10 minutes", "1 minute"),
                    col("customer_id")
                ).agg(
                    count("*").alias("transaction_count"),
                    spark_sum("amount").alias("total_amount"),
                    avg("amount").alias("avg_amount"),
                    max("amount").alias("max_amount"),
                    expr("approx_count_distinct(country_code)").alias("unique_countries"),
                    expr("approx_count_distinct(channel)").alias("unique_channels"),
                    expr("approx_count_distinct(merchant_category)").alias("unique_merchants"),
                    collect_list("transaction_type").alias("transaction_types")
                )
            
            # æ¬ºè©è¦å‰‡è©•åˆ†
            fraud_rules = customer_behavior.withColumn(
                "velocity_score",
                when(col("transaction_count") > 50, 4)
                .when(col("transaction_count") > 20, 3)
                .when(col("transaction_count") > 10, 2)
                .when(col("transaction_count") > 5, 1)
                .otherwise(0)
            ).withColumn(
                "amount_score",
                when(col("max_amount") > 100000, 4)
                .when(col("total_amount") > 500000, 4)
                .when(col("max_amount") > 50000, 3)
                .when(col("avg_amount") > 10000, 2)
                .otherwise(0)
            ).withColumn(
                "geographic_score",
                when(col("unique_countries") > 5, 4)
                .when(col("unique_countries") > 3, 3)
                .when(col("unique_countries") > 2, 2)
                .otherwise(0)
            ).withColumn(
                "behavior_score",
                when(col("unique_channels") > 3, 2)
                .when(col("unique_merchants") > 10, 2)
                .otherwise(0)
            ).withColumn(
                "total_fraud_score",
                col("velocity_score") + col("amount_score") + 
                col("geographic_score") + col("behavior_score")
            ).withColumn(
                "fraud_risk_level",
                when(col("total_fraud_score") >= 10, "CRITICAL")
                .when(col("total_fraud_score") >= 7, "HIGH")
                .when(col("total_fraud_score") >= 4, "MEDIUM")
                .otherwise("LOW")
            ).filter(col("fraud_risk_level") != "LOW")
            
            return fraud_rules
        
        def anomaly_based_fraud_detection(self, transaction_stream):
            """åŸºæ–¼ç•°å¸¸æª¢æ¸¬çš„æ¬ºè©è­˜åˆ¥"""
            
            # æ™‚é–“åºåˆ—ç•°å¸¸æª¢æ¸¬
            time_series_anomalies = transaction_stream \
                .withWatermark("timestamp", "30 minutes") \
                .groupBy(
                    window(col("timestamp"), "5 minutes"),
                    col("customer_id")
                ).agg(
                    count("*").alias("txn_count"),
                    avg("amount").alias("avg_amount"),
                    stddev("amount").alias("amount_stddev")
                )
            
            # è¨ˆç®—Z-Scoreç•°å¸¸
            window_spec = Window.partitionBy("customer_id").orderBy("window")
            
            anomaly_scores = time_series_anomalies \
                .withColumn("txn_count_ma",
                           avg("txn_count").over(window_spec.rowsBetween(-4, 0))) \
                .withColumn("txn_count_std",
                           stddev("txn_count").over(window_spec.rowsBetween(-4, 0))) \
                .withColumn("amount_ma",
                           avg("avg_amount").over(window_spec.rowsBetween(-4, 0))) \
                .withColumn("amount_std_ma",
                           avg("amount_stddev").over(window_spec.rowsBetween(-4, 0))) \
                .withColumn("txn_zscore",
                           when(col("txn_count_std") > 0,
                                spark_abs(col("txn_count") - col("txn_count_ma")) / col("txn_count_std"))
                           .otherwise(0)) \
                .withColumn("amount_zscore",
                           when(col("amount_std_ma") > 0,
                                spark_abs(col("avg_amount") - col("amount_ma")) / col("amount_std_ma"))
                           .otherwise(0)) \
                .withColumn("combined_anomaly_score",
                           col("txn_zscore") + col("amount_zscore")) \
                .withColumn("is_anomaly",
                           col("combined_anomaly_score") > 3) \
                .filter(col("is_anomaly"))
            
            return anomaly_scores
        
        def network_analysis_fraud_detection(self, transaction_stream):
            """åŸºæ–¼ç¶²çµ¡åˆ†æçš„æ¬ºè©æª¢æ¸¬"""
            
            # æ§‹å»ºäº¤æ˜“ç¶²çµ¡
            transaction_network = transaction_stream \
                .filter(col("transaction_type") == "transfer") \
                .withWatermark("timestamp", "2 hours") \
                .groupBy(
                    window(col("timestamp"), "30 minutes"),
                    col("account_id"),
                    col("counterparty_account")
                ).agg(
                    count("*").alias("edge_weight"),
                    spark_sum("amount").alias("total_amount"),
                    avg("amount").alias("avg_amount")
                )
            
            # è¨ˆç®—ç¯€é»ä¸­å¿ƒæ€§æŒ‡æ¨™
            account_centrality = transaction_network \
                .groupBy("window", "account_id") \
                .agg(
                    count("*").alias("degree_centrality"),
                    spark_sum("edge_weight").alias("weighted_degree"),
                    spark_sum("total_amount").alias("total_flow"),
                    expr("approx_count_distinct(counterparty_account)").alias("unique_counterparties")
                ).withColumn(
                    "centrality_score",
                    col("degree_centrality") * 0.3 + 
                    col("weighted_degree") * 0.3 +
                    col("unique_counterparties") * 0.4
                ).withColumn(
                    "network_risk",
                    when(col("centrality_score") > 100, "HIGH")
                    .when(col("centrality_score") > 50, "MEDIUM")
                    .otherwise("LOW")
                ).filter(col("network_risk") != "LOW")
            
            return account_centrality
    
    # 4. å¸‚å ´é¢¨éšªç®¡ç†
    print("\n4. å¸‚å ´é¢¨éšªç®¡ç†ç³»çµ±:")
    
    class MarketRiskManager:
        """å¸‚å ´é¢¨éšªç®¡ç†ç³»çµ±"""
        
        def __init__(self, spark_session):
            self.spark = spark_session
        
        def calculate_var(self, market_stream, confidence_level=0.95):
            """è¨ˆç®—é¢¨éšªåƒ¹å€¼ (Value at Risk)"""
            
            # è¨ˆç®—æ”¶ç›Šç‡
            price_returns = market_stream \
                .withWatermark("timestamp", "1 hour") \
                .withColumn("prev_price",
                           lag("price").over(Window.partitionBy("symbol").orderBy("timestamp"))) \
                .withColumn("return",
                           when(col("prev_price") > 0,
                                (col("price") - col("prev_price")) / col("prev_price"))
                           .otherwise(0)) \
                .filter(col("prev_price").isNotNull())
            
            # è¨ˆç®—VaR
            var_calculation = price_returns \
                .withWatermark("timestamp", "2 hours") \
                .groupBy(
                    window(col("timestamp"), "1 hour", "15 minutes"),
                    col("symbol"),
                    col("sector")
                ).agg(
                    collect_list("return").alias("returns"),
                    count("return").alias("return_count"),
                    avg("return").alias("mean_return"),
                    stddev("return").alias("return_volatility")
                ).withColumn(
                    "var_95",
                    percentile_approx(col("returns"), 1 - confidence_level)
                ).withColumn(
                    "var_99", 
                    percentile_approx(col("returns"), 0.01)
                ).withColumn(
                    "expected_shortfall",
                    # ç°¡åŒ–çš„ESè¨ˆç®—
                    col("var_95") * 1.2
                ).withColumn(
                    "risk_level",
                    when(col("var_95") < -0.05, "EXTREME")
                    .when(col("var_95") < -0.03, "HIGH")
                    .when(col("var_95") < -0.02, "MEDIUM")
                    .otherwise("LOW")
                )
            
            return var_calculation
        
        def volatility_monitoring(self, market_stream):
            """æ³¢å‹•ç‡ç›£æ§"""
            
            volatility_metrics = market_stream \
                .withWatermark("timestamp", "30 minutes") \
                .groupBy(
                    window(col("timestamp"), "15 minutes", "5 minutes"),
                    col("symbol"),
                    col("sector")
                ).agg(
                    avg("price").alias("avg_price"),
                    stddev("price").alias("price_std"),
                    max("price").alias("max_price"),
                    min("price").alias("min_price"),
                    avg("volume").alias("avg_volume"),
                    avg("volatility").alias("avg_volatility")
                ).withColumn(
                    "price_range",
                    (col("max_price") - col("min_price")) / col("avg_price")
                ).withColumn(
                    "realized_volatility",
                    col("price_std") / col("avg_price")
                ).withColumn(
                    "volatility_regime",
                    when(col("realized_volatility") > 0.1, "HIGH_VOLATILITY")
                    .when(col("realized_volatility") > 0.05, "MEDIUM_VOLATILITY") 
                    .otherwise("LOW_VOLATILITY")
                ).withColumn(
                    "volume_volatility_ratio",
                    col("avg_volume") / (col("realized_volatility") * 1000000)
                )
            
            return volatility_metrics
        
        def correlation_analysis(self, market_stream):
            """ç›¸é—œæ€§åˆ†æ"""
            
            # æº–å‚™é…å°æ•¸æ“šé€²è¡Œç›¸é—œæ€§è¨ˆç®—
            price_pairs = market_stream \
                .withWatermark("timestamp", "1 hour") \
                .select("timestamp", "symbol", "price") \
                .withColumn("window_time", 
                           (unix_timestamp("timestamp") / 300).cast("long") * 300) \
                .groupBy("window_time", "symbol") \
                .agg(avg("price").alias("avg_price"))
            
            # è‡ªé€£æ¥è¨ˆç®—ç›¸é—œæ€§
            correlation_pairs = price_pairs.alias("p1") \
                .join(price_pairs.alias("p2"), 
                      col("p1.window_time") == col("p2.window_time")) \
                .filter(col("p1.symbol") < col("p2.symbol")) \
                .select(
                    col("p1.window_time").alias("timestamp"),
                    col("p1.symbol").alias("symbol1"),
                    col("p2.symbol").alias("symbol2"),
                    col("p1.avg_price").alias("price1"),
                    col("p2.avg_price").alias("price2")
                )
            
            # æ»¾å‹•ç›¸é—œæ€§è¨ˆç®—ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
            rolling_correlation = correlation_pairs \
                .withColumn("price1_normalized", 
                           col("price1") / avg("price1").over(
                               Window.partitionBy("symbol1", "symbol2")
                                     .orderBy("timestamp")
                                     .rowsBetween(-9, 0))) \
                .withColumn("price2_normalized",
                           col("price2") / avg("price2").over(
                               Window.partitionBy("symbol1", "symbol2")
                                     .orderBy("timestamp")
                                     .rowsBetween(-9, 0))) \
                .withColumn("correlation_proxy",
                           col("price1_normalized") * col("price2_normalized"))
            
            return rolling_correlation
    
    # 5. ä¿¡ç”¨é¢¨éšªè©•ä¼°
    print("\n5. ä¿¡ç”¨é¢¨éšªè©•ä¼°ç³»çµ±:")
    
    class CreditRiskAssessment:
        """ä¿¡ç”¨é¢¨éšªè©•ä¼°ç³»çµ±"""
        
        def __init__(self, spark_session):
            self.spark = spark_session
        
        def credit_scoring(self, transaction_stream):
            """å‹•æ…‹ä¿¡ç”¨è©•åˆ†"""
            
            credit_metrics = transaction_stream \
                .withWatermark("timestamp", "24 hours") \
                .groupBy(
                    window(col("timestamp"), "6 hours", "1 hour"),
                    col("customer_id")
                ).agg(
                    count("*").alias("transaction_frequency"),
                    spark_sum("amount").alias("total_transaction_amount"),
                    avg("amount").alias("avg_transaction_amount"),
                    stddev("amount").alias("amount_volatility"),
                    spark_sum(when(col("transaction_type") == "deposit", col("amount")).otherwise(0)).alias("total_deposits"),
                    spark_sum(when(col("transaction_type") == "withdrawal", col("amount")).otherwise(0)).alias("total_withdrawals"),
                    expr("approx_count_distinct(counterparty_account)").alias("counterparty_diversity"),
                    avg("risk_score").alias("avg_risk_score")
                ).withColumn(
                    "balance_trend",
                    col("total_deposits") - col("total_withdrawals")
                ).withColumn(
                    "transaction_stability",
                    when(col("amount_volatility") > 0,
                         col("avg_transaction_amount") / col("amount_volatility"))
                    .otherwise(col("avg_transaction_amount"))
                ).withColumn(
                    "credit_score_base",
                    when(col("balance_trend") > 0, 20).otherwise(0) +
                    when(col("transaction_frequency") > 10, 15).otherwise(col("transaction_frequency")) +
                    when(col("transaction_stability") > 1000, 25)
                    .when(col("transaction_stability") > 500, 20)
                    .when(col("transaction_stability") > 100, 15)
                    .otherwise(10) +
                    when(col("counterparty_diversity") > 10, 20)
                    .when(col("counterparty_diversity") > 5, 15)
                    .otherwise(col("counterparty_diversity") * 2)
                ).withColumn(
                    "risk_adjustment",
                    col("avg_risk_score") * (-20)
                ).withColumn(
                    "credit_score",
                    spark_max(lit(300), 
                              spark_min(lit(850), 
                                       col("credit_score_base") + col("risk_adjustment") + 500))
                ).withColumn(
                    "credit_grade",
                    when(col("credit_score") >= 750, "EXCELLENT")
                    .when(col("credit_score") >= 700, "GOOD")
                    .when(col("credit_score") >= 650, "FAIR")
                    .when(col("credit_score") >= 600, "POOR")
                    .otherwise("VERY_POOR")
                )
            
            return credit_metrics
        
        def default_probability_estimation(self, credit_metrics):
            """é•ç´„æ¦‚ç‡ä¼°è¨ˆ"""
            
            default_prob = credit_metrics \
                .withColumn("pd_base",
                           when(col("credit_grade") == "EXCELLENT", 0.01)
                           .when(col("credit_grade") == "GOOD", 0.03)
                           .when(col("credit_grade") == "FAIR", 0.07)
                           .when(col("credit_grade") == "POOR", 0.15)
                           .otherwise(0.25)) \
                .withColumn("macroeconomic_factor", lit(1.2)) \
                .withColumn("behavior_adjustment",
                           when(col("balance_trend") < -10000, 1.5)
                           .when(col("balance_trend") < -5000, 1.2)
                           .when(col("balance_trend") > 10000, 0.8)
                           .otherwise(1.0)) \
                .withColumn("probability_of_default",
                           col("pd_base") * col("macroeconomic_factor") * col("behavior_adjustment")) \
                .withColumn("risk_category",
                           when(col("probability_of_default") > 0.2, "HIGH_RISK")
                           .when(col("probability_of_default") > 0.1, "MEDIUM_RISK")
                           .when(col("probability_of_default") > 0.05, "LOW_RISK")
                           .otherwise("MINIMAL_RISK"))
            
            return default_prob
        
        def exposure_at_default(self, transaction_stream, credit_metrics):
            """é•ç´„æ™‚é¢¨éšªæš´éœ²è¨ˆç®—"""
            
            # è¨ˆç®—å®¢æˆ¶çš„ä¿¡è²¸é¡åº¦ä½¿ç”¨æƒ…æ³
            exposure_metrics = transaction_stream \
                .withWatermark("timestamp", "12 hours") \
                .groupBy(
                    window(col("timestamp"), "4 hours"),
                    col("customer_id")
                ).agg(
                    max("amount").alias("max_single_transaction"),
                    spark_sum("amount").alias("total_exposure"),
                    count("*").alias("transaction_count")
                ).join(
                    credit_metrics.select("window", "customer_id", "credit_score", "probability_of_default"),
                    ["window", "customer_id"]
                ).withColumn(
                    "credit_limit_estimate",
                    when(col("credit_score") >= 750, 100000)
                    .when(col("credit_score") >= 700, 50000)
                    .when(col("credit_score") >= 650, 25000)
                    .when(col("credit_score") >= 600, 10000)
                    .otherwise(5000)
                ).withColumn(
                    "utilization_rate",
                    col("total_exposure") / col("credit_limit_estimate")
                ).withColumn(
                    "exposure_at_default",
                    col("total_exposure") * (1 + col("utilization_rate") * 0.5)
                ).withColumn(
                    "expected_loss",
                    col("probability_of_default") * col("exposure_at_default") * 0.6  # å‡è¨­å›æ”¶ç‡40%
                )
            
            return exposure_metrics
    
    # 6. æ³•è¦åˆè¦ç›£æ§
    print("\n6. æ³•è¦åˆè¦ç›£æ§ç³»çµ±:")
    
    class ComplianceMonitor:
        """æ³•è¦åˆè¦ç›£æ§"""
        
        def __init__(self, spark_session):
            self.spark = spark_session
        
        def aml_monitoring(self, transaction_stream):
            """åæ´—éŒ¢ç›£æ§"""
            
            # å¯ç–‘äº¤æ˜“æ¨¡å¼æª¢æ¸¬
            suspicious_patterns = transaction_stream \
                .withWatermark("timestamp", "24 hours") \
                .groupBy(
                    window(col("timestamp"), "12 hours", "1 hour"),
                    col("customer_id")
                ).agg(
                    count("*").alias("transaction_count"),
                    spark_sum("amount").alias("total_amount"),
                    expr("approx_count_distinct(country_code)").alias("countries"),
                    expr("approx_count_distinct(currency)").alias("currencies"),
                    spark_sum(when(col("amount") > 10000, 1).otherwise(0)).alias("large_transactions"),
                    spark_sum(when(col("is_international"), 1).otherwise(0)).alias("international_transactions")
                ).withColumn(
                    "structuring_score",
                    when((col("transaction_count") > 20) & 
                         (col("total_amount") > 100000) &
                         (col("large_transactions") == 0), 3)  # æ‹†åˆ†äº¤æ˜“é¿å…å ±å‘Š
                    .otherwise(0)
                ).withColumn(
                    "layering_score", 
                    when((col("countries") > 3) & (col("currencies") > 2), 2)  # è¤‡é›œè½‰ç§»
                    .otherwise(0)
                ).withColumn(
                    "unusual_activity_score",
                    when(col("international_transactions") / col("transaction_count") > 0.8, 2)
                    .when(col("total_amount") > 1000000, 3)
                    .otherwise(0)
                ).withColumn(
                    "aml_risk_score",
                    col("structuring_score") + col("layering_score") + col("unusual_activity_score")
                ).withColumn(
                    "aml_alert_level",
                    when(col("aml_risk_score") >= 5, "CRITICAL")
                    .when(col("aml_risk_score") >= 3, "HIGH")
                    .when(col("aml_risk_score") >= 1, "MEDIUM")
                    .otherwise("LOW")
                ).filter(col("aml_alert_level") != "LOW")
            
            return suspicious_patterns
        
        def large_transaction_reporting(self, transaction_stream):
            """å¤§é¡äº¤æ˜“å ±å‘Š"""
            
            large_transactions = transaction_stream \
                .filter(col("amount") >= 10000) \
                .withColumn("reporting_requirement",
                           when(col("amount") >= 10000, "CTR")  # Currency Transaction Report
                           .when((col("amount") >= 3000) & col("is_international"), "IFT")  # International Fund Transfer
                           .otherwise("NONE")) \
                .withColumn("compliance_status",
                           when(col("reporting_requirement") != "NONE", "REQUIRES_REPORTING")
                           .otherwise("NO_ACTION")) \
                .filter(col("compliance_status") == "REQUIRES_REPORTING")
            
            return large_transactions
        
        def sanctions_screening(self, transaction_stream):
            """åˆ¶è£åå–®ç¯©æŸ¥"""
            
            # æ¨¡æ“¬åˆ¶è£åå–®
            sanctions_list = ["BLOCKED_BANK_1", "BLOCKED_BANK_2", "SANCTIONED_COUNTRY"]
            
            sanctions_alerts = transaction_stream \
                .withColumn("sanctions_hit",
                           when(col("counterparty_bank").isin(sanctions_list), "BANK_MATCH")
                           .when(col("country_code").isin("IR", "KP", "SY"), "COUNTRY_SANCTIONS")
                           .otherwise("CLEAR")) \
                .withColumn("alert_severity",
                           when(col("sanctions_hit") == "BANK_MATCH", "CRITICAL")
                           .when(col("sanctions_hit") == "COUNTRY_SANCTIONS", "HIGH")
                           .otherwise("NONE")) \
                .filter(col("alert_severity") != "NONE")
            
            return sanctions_alerts
    
    # 7. å£“åŠ›æ¸¬è©¦ç³»çµ±
    print("\n7. å£“åŠ›æ¸¬è©¦ç³»çµ±:")
    
    class StressTesting:
        """å£“åŠ›æ¸¬è©¦ç³»çµ±"""
        
        def __init__(self, spark_session):
            self.spark = spark_session
        
        def market_stress_scenarios(self, market_stream):
            """å¸‚å ´å£“åŠ›æƒ…å¢ƒæ¸¬è©¦"""
            
            # å®šç¾©å£“åŠ›æƒ…å¢ƒ
            stress_scenarios = [
                ("market_crash", -0.3),      # å¸‚å ´å´©ç›¤30%
                ("moderate_decline", -0.15),  # ä¸­åº¦ä¸‹è·Œ15%
                ("volatility_spike", 0.5),   # æ³¢å‹•ç‡é£†å‡50%
                ("liquidity_crisis", -0.2)   # æµå‹•æ€§å±æ©Ÿ20%
            ]
            
            base_portfolio = market_stream \
                .withWatermark("timestamp", "1 hour") \
                .groupBy(
                    window(col("timestamp"), "30 minutes"),
                    col("symbol"),
                    col("sector")
                ).agg(
                    avg("price").alias("current_price"),
                    avg("volume").alias("avg_volume"),
                    avg("volatility").alias("current_volatility"),
                    count("*").alias("data_points")
                )
            
            # æ‡‰ç”¨å£“åŠ›æƒ…å¢ƒ
            stress_results = base_portfolio
            
            for scenario_name, shock_factor in stress_scenarios:
                stress_results = stress_results \
                    .withColumn(f"{scenario_name}_price",
                               when(scenario_name == "volatility_spike",
                                    col("current_price"))  # æ³¢å‹•ç‡æƒ…å¢ƒä¸æ”¹è®Šåƒ¹æ ¼
                               .otherwise(col("current_price") * (1 + shock_factor))) \
                    .withColumn(f"{scenario_name}_volatility",
                               when(scenario_name == "volatility_spike",
                                    col("current_volatility") * (1 + shock_factor))
                               .otherwise(col("current_volatility"))) \
                    .withColumn(f"{scenario_name}_impact",
                               spark_abs(shock_factor))
            
            return stress_results
        
        def portfolio_var_stress(self, stress_results):
            """æŠ•è³‡çµ„åˆVaRå£“åŠ›æ¸¬è©¦"""
            
            portfolio_stress = stress_results \
                .groupBy("window") \
                .agg(
                    avg("market_crash_impact").alias("avg_crash_impact"),
                    avg("moderate_decline_impact").alias("avg_decline_impact"),
                    avg("volatility_spike_impact").alias("avg_vol_impact"),
                    avg("liquidity_crisis_impact").alias("avg_liquidity_impact"),
                    count("*").alias("portfolio_size")
                ).withColumn(
                    "worst_case_scenario",
                    spark_max(spark_max("avg_crash_impact", "avg_decline_impact"),
                             spark_max("avg_vol_impact", "avg_liquidity_impact"))
                ).withColumn(
                    "stress_test_rating",
                    when(col("worst_case_scenario") > 0.25, "FAIL")
                    .when(col("worst_case_scenario") > 0.15, "WARNING")
                    .otherwise("PASS")
                )
            
            return portfolio_stress
    
    # 8. é¢¨éšªç®¡ç†å”èª¿å™¨
    print("\n8. é¢¨éšªç®¡ç†ç³»çµ±å”èª¿:")
    
    class RiskManagementOrchestrator:
        """é¢¨éšªç®¡ç†ç³»çµ±å”èª¿å™¨"""
        
        def __init__(self, spark_session):
            self.spark = spark_session
            self.data_generator = FinancialDataGenerator(spark_session)
            self.fraud_detector = FraudDetectionSystem(spark_session)
            self.market_risk_manager = MarketRiskManager(spark_session)
            self.credit_risk_assessor = CreditRiskAssessment(spark_session)
            self.compliance_monitor = ComplianceMonitor(spark_session)
            self.stress_tester = StressTesting(spark_session)
        
        def start_risk_management_pipeline(self):
            """å•Ÿå‹•é¢¨éšªç®¡ç†ç®¡é“"""
            
            print("\nå•Ÿå‹•é‡‘èé¢¨éšªç®¡ç†ç³»çµ±...")
            
            # å‰µå»ºæ•¸æ“šæµ
            transaction_stream = self.data_generator.generate_transaction_stream()
            market_stream = self.data_generator.generate_market_data_stream()
            
            print("âœ“ é‡‘èæ•¸æ“šæµå‰µå»ºå®Œæˆ")
            
            # æ¬ºè©æª¢æ¸¬
            rule_fraud = self.fraud_detector.rule_based_fraud_detection(transaction_stream)
            anomaly_fraud = self.fraud_detector.anomaly_based_fraud_detection(transaction_stream)
            network_fraud = self.fraud_detector.network_analysis_fraud_detection(transaction_stream)
            
            # å¸‚å ´é¢¨éšªç®¡ç†
            var_analysis = self.market_risk_manager.calculate_var(market_stream)
            volatility_monitoring = self.market_risk_manager.volatility_monitoring(market_stream)
            correlation_analysis = self.market_risk_manager.correlation_analysis(market_stream)
            
            # ä¿¡ç”¨é¢¨éšªè©•ä¼°
            credit_scoring = self.credit_risk_assessor.credit_scoring(transaction_stream)
            default_probability = self.credit_risk_assessor.default_probability_estimation(credit_scoring)
            exposure_analysis = self.credit_risk_assessor.exposure_at_default(transaction_stream, credit_scoring)
            
            # åˆè¦ç›£æ§
            aml_monitoring = self.compliance_monitor.aml_monitoring(transaction_stream)
            large_transaction_reports = self.compliance_monitor.large_transaction_reporting(transaction_stream)
            sanctions_screening = self.compliance_monitor.sanctions_screening(transaction_stream)
            
            # å£“åŠ›æ¸¬è©¦
            stress_scenarios = self.stress_tester.market_stress_scenarios(market_stream)
            portfolio_stress = self.stress_tester.portfolio_var_stress(stress_scenarios)
            
            # å•Ÿå‹•æ‰€æœ‰æŸ¥è©¢
            queries = []
            
            # æ¬ºè©æª¢æ¸¬ç›£æ§
            fraud_rule_query = rule_fraud.writeStream \
                .outputMode("append") \
                .format("console") \
                .option("truncate", False) \
                .option("numRows", 5) \
                .trigger(processingTime="60 seconds") \
                .queryName("fraud_rules") \
                .start()
            queries.append(fraud_rule_query)
            
            # å¸‚å ´é¢¨éšªç›£æ§
            var_query = var_analysis.writeStream \
                .outputMode("append") \
                .format("console") \
                .option("truncate", False) \
                .option("numRows", 5) \
                .trigger(processingTime="45 seconds") \
                .queryName("var_analysis") \
                .start()
            queries.append(var_query)
            
            # ä¿¡ç”¨é¢¨éšªç›£æ§
            credit_query = default_probability.writeStream \
                .outputMode("append") \
                .format("console") \
                .option("truncate", False) \
                .option("numRows", 5) \
                .trigger(processingTime="90 seconds") \
                .queryName("credit_risk") \
                .start()
            queries.append(credit_query)
            
            # åˆè¦ç›£æ§
            aml_query = aml_monitoring.writeStream \
                .outputMode("append") \
                .format("console") \
                .option("truncate", False) \
                .option("numRows", 3) \
                .trigger(processingTime="120 seconds") \
                .queryName("aml_monitoring") \
                .start()
            queries.append(aml_query)
            
            # å¤§é¡äº¤æ˜“å ±å‘Š
            large_txn_query = large_transaction_reports.writeStream \
                .outputMode("append") \
                .format("console") \
                .option("truncate", False) \
                .option("numRows", 5) \
                .trigger(processingTime="30 seconds") \
                .queryName("large_transactions") \
                .start()
            queries.append(large_txn_query)
            
            # åˆ¶è£ç¯©æŸ¥
            sanctions_query = sanctions_screening.writeStream \
                .outputMode("append") \
                .format("console") \
                .option("truncate", False) \
                .trigger(processingTime="60 seconds") \
                .queryName("sanctions_screening") \
                .start()
            queries.append(sanctions_query)
            
            print(f"âœ“ å·²å•Ÿå‹• {len(queries)} å€‹é¢¨éšªç®¡ç†æŸ¥è©¢")
            
            return queries
        
        def generate_risk_dashboard(self, queries):
            """ç”Ÿæˆé¢¨éšªç®¡ç†å„€è¡¨æ¿"""
            
            print("\né¢¨éšªç®¡ç†å„€è¡¨æ¿:")
            print("=" * 60)
            
            risk_metrics = {
                "æ¬ºè©æª¢æ¸¬": "å¯¦æ™‚ç›£æ§ç•°å¸¸äº¤æ˜“æ¨¡å¼",
                "å¸‚å ´é¢¨éšª": "VaRè¨ˆç®—å’Œæ³¢å‹•ç‡ç›£æ§",
                "ä¿¡ç”¨é¢¨éšª": "å‹•æ…‹ä¿¡ç”¨è©•åˆ†å’Œé•ç´„æ¦‚ç‡",
                "åˆè¦ç›£æ§": "AMLå’Œåˆ¶è£åå–®ç¯©æŸ¥",
                "å£“åŠ›æ¸¬è©¦": "å¸‚å ´æƒ…å¢ƒåˆ†æ",
                "å¯¦æ™‚å‘Šè­¦": f"{len(queries)} å€‹ç›£æ§æµç¨‹é‹è¡Œä¸­"
            }
            
            for category, description in risk_metrics.items():
                print(f"ğŸ“Š {category:<12}: {description}")
            
            print("=" * 60)
            
            return risk_metrics
    
    # 9. åŸ·è¡Œé¢¨éšªç®¡ç†ç³»çµ±
    print("\n9. åŸ·è¡Œé¢¨éšªç®¡ç†ç³»çµ±:")
    
    orchestrator = RiskManagementOrchestrator(spark)
    
    # å•Ÿå‹•é¢¨éšªç®¡ç†ç®¡é“
    running_queries = orchestrator.start_risk_management_pipeline()
    
    # ç”Ÿæˆå„€è¡¨æ¿
    dashboard_metrics = orchestrator.generate_risk_dashboard(running_queries)
    
    # é‹è¡Œä¸€æ®µæ™‚é–“
    print("\nç­‰å¾…é¢¨éšªæ•¸æ“šè™•ç†...")
    time.sleep(90)  # é‹è¡Œ1.5åˆ†é˜
    
    # ç›£æ§æŸ¥è©¢ç‹€æ…‹
    print("\næŸ¥è©¢ç‹€æ…‹ç›£æ§:")
    for query in running_queries:
        try:
            progress = query.lastProgress
            if progress:
                print(f"  {query.name}: æ‰¹æ¬¡ID {progress.get('batchId', 'N/A')}, "
                      f"è™•ç†æ™‚é–“ {progress.get('durationMs', {}).get('totalMs', 'N/A')} ms")
        except Exception as e:
            print(f"  {query.name}: ç›£æ§å¤±æ•— - {e}")
    
    # å†é‹è¡Œä¸€æ®µæ™‚é–“
    time.sleep(90)  # å†é‹è¡Œ1.5åˆ†é˜
    
    # 10. ç³»çµ±ç¸½çµå’Œå»ºè­°
    print("\n10. é¢¨éšªç®¡ç†ç³»çµ±ç¸½çµ:")
    
    def generate_risk_management_summary():
        """ç”Ÿæˆé¢¨éšªç®¡ç†ç³»çµ±ç¸½çµ"""
        
        system_capabilities = {
            "æ¬ºè©æª¢æ¸¬": "è¦å‰‡åŸºç¤ + ç•°å¸¸æª¢æ¸¬ + ç¶²çµ¡åˆ†æ",
            "å¸‚å ´é¢¨éšª": "VaRè¨ˆç®— + æ³¢å‹•ç‡ç›£æ§ + ç›¸é—œæ€§åˆ†æ", 
            "ä¿¡ç”¨é¢¨éšª": "å‹•æ…‹è©•åˆ† + é•ç´„æ¦‚ç‡ + é¢¨éšªæš´éœ²",
            "åˆè¦ç›£æ§": "åæ´—éŒ¢ + å¤§é¡äº¤æ˜“ + åˆ¶è£ç¯©æŸ¥",
            "å£“åŠ›æ¸¬è©¦": "å¤šæƒ…å¢ƒåˆ†æ + æŠ•è³‡çµ„åˆè©•ä¼°",
            "å¯¦æ™‚è™•ç†": "ç§’ç´šéŸ¿æ‡‰ + è‡ªå‹•å‘Šè­¦"
        }
        
        implementation_benefits = [
            "å¯¦æ™‚é¢¨éšªç›£æ§å’Œé è­¦",
            "å¤šç¶­åº¦é¢¨éšªè©•ä¼°é«”ç³»",
            "è‡ªå‹•åŒ–åˆè¦å ±å‘Šç”Ÿæˆ",
            "æ™ºèƒ½åŒ–ç•°å¸¸æª¢æ¸¬",
            "å£“åŠ›æ¸¬è©¦å’Œæƒ…å¢ƒåˆ†æ",
            "å¯æ“´å±•çš„é¢¨éšªç®¡ç†æ¶æ§‹"
        ]
        
        future_enhancements = [
            "æ•´åˆæ©Ÿå™¨å­¸ç¿’æ¨¡å‹æé«˜æª¢æ¸¬æº–ç¢ºæ€§",
            "å»ºç«‹é¢¨éšªé™é¡å‹•æ…‹èª¿æ•´æ©Ÿåˆ¶",
            "å¯¦æ–½æ¨¡å‹é¢¨éšªç®¡ç†æ¡†æ¶",
            "å¢åŠ ESGé¢¨éšªè©•ä¼°ç¶­åº¦",
            "å»ºç«‹è·¨æ©Ÿæ§‹é¢¨éšªæ•¸æ“šå…±äº«",
            "å¯¦ç¾ç›£ç®¡å ±å‘Šè‡ªå‹•åŒ–ç”Ÿæˆ"
        ]
        
        print("ç³»çµ±åŠŸèƒ½è¦†è“‹:")
        for capability, description in system_capabilities.items():
            print(f"- {capability}: {description}")
        
        print("\nå¯¦æ–½æ•ˆç›Š:")
        for i, benefit in enumerate(implementation_benefits, 1):
            print(f"{i}. {benefit}")
        
        print("\næœªä¾†å¢å¼·æ–¹å‘:")
        for i, enhancement in enumerate(future_enhancements, 1):
            print(f"{i}. {enhancement}")
        
        return system_capabilities, implementation_benefits, future_enhancements
    
    capabilities, benefits, enhancements = generate_risk_management_summary()
    
    # åœæ­¢æ‰€æœ‰æŸ¥è©¢
    print("\nåœæ­¢é¢¨éšªç®¡ç†ç³»çµ±...")
    for query in running_queries:
        try:
            query.stop()
            print(f"âœ“ å·²åœæ­¢æŸ¥è©¢: {query.name}")
        except Exception as e:
            print(f"âœ— åœæ­¢æŸ¥è©¢ {query.name} å¤±æ•—: {e}")
    
    # æ¸…ç†è³‡æº
    spark.stop()
    print("\né‡‘èé¢¨éšªç®¡ç†ç³»çµ±ç·´ç¿’å®Œæˆï¼")

if __name__ == "__main__":
    main()