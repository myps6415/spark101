{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬2ç« ï¼šRDD åŸºæœ¬æ“ä½œ\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- æ·±å…¥ç†è§£ RDD (Resilient Distributed Dataset) æ¦‚å¿µ\n",
    "- æŒæ¡ Transformation å’Œ Action æ“ä½œ\n",
    "- å­¸æœƒ RDD çš„å‰µå»ºå’ŒåŸºæœ¬æ“ä½œ\n",
    "- ç†è§£ Spark çš„åˆ†å€æ©Ÿåˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦çš„æ¨¡çµ„\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‰µå»º SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»º SparkContext\n",
    "conf = SparkConf().setAppName(\"RDDåŸºæœ¬æ“ä½œ\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "print(\"ğŸ¯ RDD åŸºç¤æ“ä½œç¤ºç¯„\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Spark ç‰ˆæœ¬: {sc.version}\")\n",
    "print(f\"æ‡‰ç”¨ç¨‹å¼åç¨±: {sc.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‰µå»º RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. å¾é›†åˆå‰µå»º RDD\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "\n",
    "print(f\"åŸå§‹æ•¸æ“š: {numbers}\")\n",
    "print(f\"RDD åˆ†å€æ•¸: {numbers_rdd.getNumPartitions()}\")\n",
    "print(f\"RDD å…§å®¹: {numbers_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. æŒ‡å®šåˆ†å€æ•¸å‰µå»º RDD\n",
    "numbers_rdd_4_partitions = sc.parallelize(numbers, 4)\n",
    "print(f\"æŒ‡å®š4å€‹åˆ†å€çš„RDDåˆ†å€æ•¸: {numbers_rdd_4_partitions.getNumPartitions()}\")\n",
    "\n",
    "# æŸ¥çœ‹æ¯å€‹åˆ†å€çš„å…§å®¹\n",
    "def show_partition_content(index, iterator):\n",
    "    yield f\"åˆ†å€ {index}: {list(iterator)}\"\n",
    "\n",
    "partition_content = numbers_rdd_4_partitions.mapPartitionsWithIndex(show_partition_content).collect()\n",
    "for content in partition_content:\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation æ“ä½œï¼ˆå»¶é²åŸ·è¡Œï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ Transformation æ“ä½œ:\")\n",
    "\n",
    "# éæ¿¾å¶æ•¸\n",
    "even_rdd = numbers_rdd.filter(lambda x: x % 2 == 0)\n",
    "print(f\"å¶æ•¸: {even_rdd.collect()}\")\n",
    "\n",
    "# æ˜ å°„æ“ä½œ - å¹³æ–¹\n",
    "squared_rdd = numbers_rdd.map(lambda x: x ** 2)\n",
    "print(f\"å¹³æ–¹: {squared_rdd.collect()}\")\n",
    "\n",
    "# æ˜ å°„æ“ä½œ - è½‰æ›ç‚ºå­—ä¸²\n",
    "string_rdd = numbers_rdd.map(lambda x: f\"æ•¸å­—_{x}\")\n",
    "print(f\"å­—ä¸²è½‰æ›: {string_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰å¹³åŒ–æ˜ å°„\n",
    "words = [\"hello world\", \"spark is awesome\", \"big data processing\"]\n",
    "words_rdd = sc.parallelize(words)\n",
    "flat_words_rdd = words_rdd.flatMap(lambda x: x.split())\n",
    "\n",
    "print(f\"åŸå§‹å¥å­: {words}\")\n",
    "print(f\"åˆ†è©çµæœ: {flat_words_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»é‡æ“ä½œ\n",
    "duplicate_numbers = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\n",
    "duplicate_rdd = sc.parallelize(duplicate_numbers)\n",
    "unique_rdd = duplicate_rdd.distinct()\n",
    "\n",
    "print(f\"åŸå§‹æ•¸æ“š: {duplicate_numbers}\")\n",
    "print(f\"å»é‡çµæœ: {unique_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action æ“ä½œï¼ˆè§¸ç™¼è¨ˆç®—ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âš¡ Action æ“ä½œ:\")\n",
    "\n",
    "# è¨ˆæ•¸\n",
    "count = numbers_rdd.count()\n",
    "print(f\"å…ƒç´ ç¸½æ•¸: {count}\")\n",
    "\n",
    "# æ±‚å’Œ\n",
    "total = numbers_rdd.reduce(lambda x, y: x + y)\n",
    "print(f\"ç¸½å’Œ: {total}\")\n",
    "\n",
    "# å–å‰ N å€‹å…ƒç´ \n",
    "first_three = numbers_rdd.take(3)\n",
    "print(f\"å‰ä¸‰å€‹å…ƒç´ : {first_three}\")\n",
    "\n",
    "# å–å‰ N å€‹å…ƒç´ ï¼ˆæ’åºå¾Œï¼‰\n",
    "top_three = numbers_rdd.top(3)\n",
    "print(f\"æœ€å¤§çš„ä¸‰å€‹å…ƒç´ : {top_three}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµ±è¨ˆä¿¡æ¯\n",
    "stats = numbers_rdd.stats()\n",
    "print(f\"çµ±è¨ˆä¿¡æ¯:\")\n",
    "print(f\"  å¹³å‡å€¼: {stats.mean():.2f}\")\n",
    "print(f\"  æ¨™æº–å·®: {stats.stdev():.2f}\")\n",
    "print(f\"  æœ€å¤§å€¼: {stats.max()}\")\n",
    "print(f\"  æœ€å°å€¼: {stats.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## éµå€¼å° RDD æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”‘ éµå€¼å° RDD æ“ä½œ:\")\n",
    "\n",
    "# å‰µå»ºéµå€¼å°\n",
    "pairs_rdd = numbers_rdd.map(lambda x: (x % 3, x))\n",
    "print(f\"éµå€¼å°: {pairs_rdd.collect()}\")\n",
    "\n",
    "# æŒ‰éµåˆ†çµ„\n",
    "grouped_rdd = pairs_rdd.groupByKey()\n",
    "grouped_result = grouped_rdd.mapValues(list).collect()\n",
    "print(f\"æŒ‰éµåˆ†çµ„: {grouped_result}\")\n",
    "\n",
    "# æŒ‰éµæ±‚å’Œ\n",
    "sum_by_key = pairs_rdd.reduceByKey(lambda x, y: x + y)\n",
    "print(f\"æŒ‰éµæ±‚å’Œ: {sum_by_key.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ›´å¯¦éš›çš„ä¾‹å­ï¼šå–®è©è¨ˆæ•¸\n",
    "text = [\"spark is great\", \"spark is fast\", \"python is great\"]\n",
    "text_rdd = sc.parallelize(text)\n",
    "\n",
    "word_count = text_rdd.flatMap(lambda line: line.split()) \\\n",
    "                    .map(lambda word: (word, 1)) \\\n",
    "                    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"å–®è©è¨ˆæ•¸çµæœ:\")\n",
    "for word, count in word_count.collect():\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD çš„æŒä¹…åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºè¨ˆç®—è¤‡é›œçš„ RDD\n",
    "complex_rdd = numbers_rdd.map(lambda x: x ** 2).filter(lambda x: x > 25)\n",
    "\n",
    "print(\"ç¬¬ä¸€æ¬¡è¨ˆç®—ï¼ˆæ²’æœ‰ç·©å­˜ï¼‰:\")\n",
    "print(f\"çµæœ: {complex_rdd.collect()}\")\n",
    "\n",
    "# ç·©å­˜ RDD\n",
    "complex_rdd.cache()\n",
    "\n",
    "print(\"ç¬¬äºŒæ¬¡è¨ˆç®—ï¼ˆä½¿ç”¨ç·©å­˜ï¼‰:\")\n",
    "print(f\"çµæœ: {complex_rdd.collect()}\")\n",
    "print(f\"è¨ˆæ•¸: {complex_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD çš„ Lineageï¼ˆè¡€çµ±ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹ RDD çš„è¡€çµ±ä¿¡æ¯\n",
    "print(\"RDD è¡€çµ±ä¿¡æ¯:\")\n",
    "print(complex_rdd.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç·´ç¿’é¡Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’1ï¼šåŸºæœ¬ RDD æ“ä½œ\n",
    "çµ¦å®šä¸€å€‹æ•¸å­—åˆ—è¡¨ï¼Œå®Œæˆä»¥ä¸‹ä»»å‹™ï¼š\n",
    "1. å‰µå»º RDD\n",
    "2. æ‰¾å‡ºæ‰€æœ‰å¥‡æ•¸\n",
    "3. å°‡æ¯å€‹å¥‡æ•¸ä¹˜ä»¥ 3\n",
    "4. è¨ˆç®—çµæœçš„ç¸½å’Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµ¦å®šæ•¸æ“š\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "# åœ¨é€™è£¡å¯«ä½ çš„ä»£ç¢¼\n",
    "# æç¤ºï¼šä½¿ç”¨ filter(), map(), reduce() ç­‰æ“ä½œ\n",
    "\n",
    "# ä½ çš„ä»£ç¢¼åœ¨æ­¤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’2ï¼šæ–‡æœ¬è™•ç†\n",
    "çµ¦å®šä¸€äº›æ–‡æœ¬æ•¸æ“šï¼Œå®Œæˆä»¥ä¸‹ä»»å‹™ï¼š\n",
    "1. å°‡æ–‡æœ¬åˆ†è©\n",
    "2. éæ¿¾æ‰é•·åº¦å°æ–¼ 3 çš„å–®è©\n",
    "3. çµ±è¨ˆæ¯å€‹å–®è©çš„å‡ºç¾æ¬¡æ•¸\n",
    "4. æ‰¾å‡ºå‡ºç¾æ¬¡æ•¸æœ€å¤šçš„å‰ 3 å€‹å–®è©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµ¦å®šæ–‡æœ¬æ•¸æ“š\n",
    "texts = [\n",
    "    \"Apache Spark is a unified analytics engine\",\n",
    "    \"Spark provides high-level APIs in Java, Scala, Python and R\",\n",
    "    \"Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone\",\n",
    "    \"Spark is built on the concept of resilient distributed datasets\"\n",
    "]\n",
    "\n",
    "# åœ¨é€™è£¡å¯«ä½ çš„ä»£ç¢¼\n",
    "# æç¤ºï¼šä½¿ç”¨ flatMap(), filter(), map(), reduceByKey(), takeOrdered() ç­‰æ“ä½œ\n",
    "\n",
    "# ä½ çš„ä»£ç¢¼åœ¨æ­¤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’3ï¼šæ•¸æ“šåˆ†æ\n",
    "çµ¦å®šéŠ·å”®æ•¸æ“šï¼Œå®Œæˆä»¥ä¸‹ä»»å‹™ï¼š\n",
    "1. è¨ˆç®—æ¯å€‹ç”¢å“çš„ç¸½éŠ·å”®é¡\n",
    "2. æ‰¾å‡ºéŠ·å”®é¡æœ€é«˜çš„ç”¢å“\n",
    "3. è¨ˆç®—å¹³å‡éŠ·å”®é¡\n",
    "4. æ‰¾å‡ºéŠ·å”®é¡é«˜æ–¼å¹³å‡å€¼çš„ç”¢å“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµ¦å®šéŠ·å”®æ•¸æ“š (ç”¢å“, éŠ·å”®é¡)\n",
    "sales_data = [\n",
    "    (\"Laptop\", 25000),\n",
    "    (\"Phone\", 18000),\n",
    "    (\"Tablet\", 15000),\n",
    "    (\"Laptop\", 27000),\n",
    "    (\"Phone\", 19000),\n",
    "    (\"Watch\", 8000),\n",
    "    (\"Tablet\", 16000),\n",
    "    (\"Watch\", 9000)\n",
    "]\n",
    "\n",
    "# åœ¨é€™è£¡å¯«ä½ çš„ä»£ç¢¼\n",
    "# æç¤ºï¼šä½¿ç”¨ reduceByKey(), max(), mean() ç­‰æ“ä½œ\n",
    "\n",
    "# ä½ çš„ä»£ç¢¼åœ¨æ­¤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¸…ç†è³‡æº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å–æ¶ˆç·©å­˜\n",
    "complex_rdd.unpersist()\n",
    "\n",
    "# åœæ­¢ SparkContext\n",
    "sc.stop()\n",
    "print(\"âœ… RDD æ“ä½œç¤ºç¯„å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¸½çµ\n",
    "\n",
    "åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘å€‘å­¸ç¿’äº†ï¼š\n",
    "\n",
    "1. **RDD çš„åŸºæœ¬æ¦‚å¿µ** - åˆ†æ•£å¼ã€å®¹éŒ¯çš„æ•¸æ“šé›†\n",
    "2. **Transformation vs Action** - å»¶é²åŸ·è¡Œ vs ç«‹å³åŸ·è¡Œ\n",
    "3. **åŸºæœ¬ RDD æ“ä½œ** - map, filter, reduce, collect ç­‰\n",
    "4. **éµå€¼å° RDD** - groupByKey, reduceByKey ç­‰\n",
    "5. **RDD æŒä¹…åŒ–** - cache å’Œ persist\n",
    "6. **RDD è¡€çµ±** - å®¹éŒ¯å’Œé‡æ–°è¨ˆç®—æ©Ÿåˆ¶\n",
    "\n",
    "### é—œéµè¦é»\n",
    "- RDD æ˜¯ Spark çš„æ ¸å¿ƒæ•¸æ“šæŠ½è±¡\n",
    "- Transformation æ“ä½œæ˜¯å»¶é²åŸ·è¡Œçš„\n",
    "- Action æ“ä½œæœƒè§¸ç™¼å¯¦éš›çš„è¨ˆç®—\n",
    "- åˆç†ä½¿ç”¨ç·©å­˜å¯ä»¥æé«˜æ€§èƒ½\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "- ç¹¼çºŒå­¸ç¿’ [ç¬¬3ç« ï¼šDataFrame å’Œ Dataset API](03_dataframe_operations.ipynb)\n",
    "- äº†è§£æ›´é«˜ç´šçš„æ•¸æ“šè™•ç†æŠ½è±¡\n",
    "- æ¢ç´¢çµæ§‹åŒ–æ•¸æ“šè™•ç†"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}