{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2章：RDD 基本操作\n",
    "\n",
    "## 學習目標\n",
    "- 深入理解 RDD (Resilient Distributed Dataset) 概念\n",
    "- 掌握 Transformation 和 Action 操作\n",
    "- 學會 RDD 的創建和基本操作\n",
    "- 理解 Spark 的分區機制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入必要的模組\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 創建 SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建 SparkContext\n",
    "conf = SparkConf().setAppName(\"RDD基本操作\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "print(\"🎯 RDD 基礎操作示範\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Spark 版本: {sc.version}\")\n",
    "print(f\"應用程式名稱: {sc.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 創建 RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 從集合創建 RDD\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "\n",
    "print(f\"原始數據: {numbers}\")\n",
    "print(f\"RDD 分區數: {numbers_rdd.getNumPartitions()}\")\n",
    "print(f\"RDD 內容: {numbers_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 指定分區數創建 RDD\n",
    "numbers_rdd_4_partitions = sc.parallelize(numbers, 4)\n",
    "print(f\"指定4個分區的RDD分區數: {numbers_rdd_4_partitions.getNumPartitions()}\")\n",
    "\n",
    "# 查看每個分區的內容\n",
    "def show_partition_content(index, iterator):\n",
    "    yield f\"分區 {index}: {list(iterator)}\"\n",
    "\n",
    "partition_content = numbers_rdd_4_partitions.mapPartitionsWithIndex(show_partition_content).collect()\n",
    "for content in partition_content:\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation 操作（延遲執行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔄 Transformation 操作:\")\n",
    "\n",
    "# 過濾偶數\n",
    "even_rdd = numbers_rdd.filter(lambda x: x % 2 == 0)\n",
    "print(f\"偶數: {even_rdd.collect()}\")\n",
    "\n",
    "# 映射操作 - 平方\n",
    "squared_rdd = numbers_rdd.map(lambda x: x ** 2)\n",
    "print(f\"平方: {squared_rdd.collect()}\")\n",
    "\n",
    "# 映射操作 - 轉換為字串\n",
    "string_rdd = numbers_rdd.map(lambda x: f\"數字_{x}\")\n",
    "print(f\"字串轉換: {string_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 扁平化映射\n",
    "words = [\"hello world\", \"spark is awesome\", \"big data processing\"]\n",
    "words_rdd = sc.parallelize(words)\n",
    "flat_words_rdd = words_rdd.flatMap(lambda x: x.split())\n",
    "\n",
    "print(f\"原始句子: {words}\")\n",
    "print(f\"分詞結果: {flat_words_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去重操作\n",
    "duplicate_numbers = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\n",
    "duplicate_rdd = sc.parallelize(duplicate_numbers)\n",
    "unique_rdd = duplicate_rdd.distinct()\n",
    "\n",
    "print(f\"原始數據: {duplicate_numbers}\")\n",
    "print(f\"去重結果: {unique_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action 操作（觸發計算）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"⚡ Action 操作:\")\n",
    "\n",
    "# 計數\n",
    "count = numbers_rdd.count()\n",
    "print(f\"元素總數: {count}\")\n",
    "\n",
    "# 求和\n",
    "total = numbers_rdd.reduce(lambda x, y: x + y)\n",
    "print(f\"總和: {total}\")\n",
    "\n",
    "# 取前 N 個元素\n",
    "first_three = numbers_rdd.take(3)\n",
    "print(f\"前三個元素: {first_three}\")\n",
    "\n",
    "# 取前 N 個元素（排序後）\n",
    "top_three = numbers_rdd.top(3)\n",
    "print(f\"最大的三個元素: {top_three}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 統計信息\n",
    "stats = numbers_rdd.stats()\n",
    "print(f\"統計信息:\")\n",
    "print(f\"  平均值: {stats.mean():.2f}\")\n",
    "print(f\"  標準差: {stats.stdev():.2f}\")\n",
    "print(f\"  最大值: {stats.max()}\")\n",
    "print(f\"  最小值: {stats.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 鍵值對 RDD 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔑 鍵值對 RDD 操作:\")\n",
    "\n",
    "# 創建鍵值對\n",
    "pairs_rdd = numbers_rdd.map(lambda x: (x % 3, x))\n",
    "print(f\"鍵值對: {pairs_rdd.collect()}\")\n",
    "\n",
    "# 按鍵分組\n",
    "grouped_rdd = pairs_rdd.groupByKey()\n",
    "grouped_result = grouped_rdd.mapValues(list).collect()\n",
    "print(f\"按鍵分組: {grouped_result}\")\n",
    "\n",
    "# 按鍵求和\n",
    "sum_by_key = pairs_rdd.reduceByKey(lambda x, y: x + y)\n",
    "print(f\"按鍵求和: {sum_by_key.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更實際的例子：單詞計數\n",
    "text = [\"spark is great\", \"spark is fast\", \"python is great\"]\n",
    "text_rdd = sc.parallelize(text)\n",
    "\n",
    "word_count = text_rdd.flatMap(lambda line: line.split()) \\\n",
    "                    .map(lambda word: (word, 1)) \\\n",
    "                    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"單詞計數結果:\")\n",
    "for word, count in word_count.collect():\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 的持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建計算複雜的 RDD\n",
    "complex_rdd = numbers_rdd.map(lambda x: x ** 2).filter(lambda x: x > 25)\n",
    "\n",
    "print(\"第一次計算（沒有緩存）:\")\n",
    "print(f\"結果: {complex_rdd.collect()}\")\n",
    "\n",
    "# 緩存 RDD\n",
    "complex_rdd.cache()\n",
    "\n",
    "print(\"第二次計算（使用緩存）:\")\n",
    "print(f\"結果: {complex_rdd.collect()}\")\n",
    "print(f\"計數: {complex_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 的 Lineage（血統）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 RDD 的血統信息\n",
    "print(\"RDD 血統信息:\")\n",
    "print(complex_rdd.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習1：基本 RDD 操作\n",
    "給定一個數字列表，完成以下任務：\n",
    "1. 創建 RDD\n",
    "2. 找出所有奇數\n",
    "3. 將每個奇數乘以 3\n",
    "4. 計算結果的總和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 給定數據\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "# 在這裡寫你的代碼\n",
    "# 提示：使用 filter(), map(), reduce() 等操作\n",
    "\n",
    "# 你的代碼在此\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習2：文本處理\n",
    "給定一些文本數據，完成以下任務：\n",
    "1. 將文本分詞\n",
    "2. 過濾掉長度小於 3 的單詞\n",
    "3. 統計每個單詞的出現次數\n",
    "4. 找出出現次數最多的前 3 個單詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 給定文本數據\n",
    "texts = [\n",
    "    \"Apache Spark is a unified analytics engine\",\n",
    "    \"Spark provides high-level APIs in Java, Scala, Python and R\",\n",
    "    \"Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone\",\n",
    "    \"Spark is built on the concept of resilient distributed datasets\"\n",
    "]\n",
    "\n",
    "# 在這裡寫你的代碼\n",
    "# 提示：使用 flatMap(), filter(), map(), reduceByKey(), takeOrdered() 等操作\n",
    "\n",
    "# 你的代碼在此\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習3：數據分析\n",
    "給定銷售數據，完成以下任務：\n",
    "1. 計算每個產品的總銷售額\n",
    "2. 找出銷售額最高的產品\n",
    "3. 計算平均銷售額\n",
    "4. 找出銷售額高於平均值的產品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 給定銷售數據 (產品, 銷售額)\n",
    "sales_data = [\n",
    "    (\"Laptop\", 25000),\n",
    "    (\"Phone\", 18000),\n",
    "    (\"Tablet\", 15000),\n",
    "    (\"Laptop\", 27000),\n",
    "    (\"Phone\", 19000),\n",
    "    (\"Watch\", 8000),\n",
    "    (\"Tablet\", 16000),\n",
    "    (\"Watch\", 9000)\n",
    "]\n",
    "\n",
    "# 在這裡寫你的代碼\n",
    "# 提示：使用 reduceByKey(), max(), mean() 等操作\n",
    "\n",
    "# 你的代碼在此\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清理資源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取消緩存\n",
    "complex_rdd.unpersist()\n",
    "\n",
    "# 停止 SparkContext\n",
    "sc.stop()\n",
    "print(\"✅ RDD 操作示範完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "在本章中，我們學習了：\n",
    "\n",
    "1. **RDD 的基本概念** - 分散式、容錯的數據集\n",
    "2. **Transformation vs Action** - 延遲執行 vs 立即執行\n",
    "3. **基本 RDD 操作** - map, filter, reduce, collect 等\n",
    "4. **鍵值對 RDD** - groupByKey, reduceByKey 等\n",
    "5. **RDD 持久化** - cache 和 persist\n",
    "6. **RDD 血統** - 容錯和重新計算機制\n",
    "\n",
    "### 關鍵要點\n",
    "- RDD 是 Spark 的核心數據抽象\n",
    "- Transformation 操作是延遲執行的\n",
    "- Action 操作會觸發實際的計算\n",
    "- 合理使用緩存可以提高性能\n",
    "\n",
    "### 下一步\n",
    "- 繼續學習 [第3章：DataFrame 和 Dataset API](03_dataframe_operations.ipynb)\n",
    "- 了解更高級的數據處理抽象\n",
    "- 探索結構化數據處理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}