{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第六章：機器學習 (MLlib)\n",
    "\n",
    "Apache Spark MLlib 是 Spark 的機器學習函式庫，提供了分散式機器學習演算法和工具。\n",
    "\n",
    "## 學習目標\n",
    "- 了解 Spark MLlib 的基本概念\n",
    "- 學習資料預處理和特徵工程\n",
    "- 掌握分類、迴歸和聚類演算法\n",
    "- 理解模型訓練和評估流程\n",
    "- 學習模型管線 (Pipeline) 的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 機器學習相關模組\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.clustering import KMeans, GaussianMixture\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# 建立 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark MLlib Tutorial\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark 版本: {spark.version}\")\n",
    "print(f\"可用核心數: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 資料準備\n",
    "\n",
    "我們將使用經典的鳶尾花資料集和房價資料集來示範不同的機器學習任務。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立鳶尾花資料集\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# 轉換為 Spark DataFrame\n",
    "iris_pandas = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_pandas['target'] = iris.target\n",
    "iris_pandas['species'] = iris_pandas['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "iris_df = spark.createDataFrame(iris_pandas)\n",
    "iris_df.show(10)\n",
    "iris_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立房價資料集（回歸問題）\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "boston_pandas = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "boston_pandas['price'] = boston.target\n",
    "\n",
    "boston_df = spark.createDataFrame(boston_pandas)\n",
    "boston_df.show(10)\n",
    "boston_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 資料探索和視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 鳶尾花資料集統計\n",
    "print(\"鳶尾花資料集統計:\")\n",
    "iris_df.describe().show()\n",
    "\n",
    "# 類別分佈\n",
    "print(\"\\n類別分佈:\")\n",
    "iris_df.groupBy('species').count().show()\n",
    "\n",
    "# 特徵相關性\n",
    "print(\"\\n特徵相關性:\")\n",
    "iris_stats = iris_df.select([col for col in iris_df.columns if col not in ['target', 'species']]).toPandas()\n",
    "correlation_matrix = iris_stats.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('鳶尾花特徵相關性')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特徵工程\n",
    "\n",
    "在機器學習中，特徵工程是將原始資料轉換為適合模型訓練的特徵的過程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為分類問題準備特徵\n",
    "def prepare_classification_features(df, feature_cols, label_col):\n",
    "    \"\"\"\n",
    "    準備分類問題的特徵\n",
    "    \"\"\"\n",
    "    # 1. 將標籤轉換為數值\n",
    "    indexer = StringIndexer(inputCol=label_col, outputCol=\"label\")\n",
    "    \n",
    "    # 2. 組合特徵向量\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    \n",
    "    # 3. 標準化特徵\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    \n",
    "    # 建立預處理管線\n",
    "    pipeline = Pipeline(stages=[indexer, assembler, scaler])\n",
    "    \n",
    "    # 訓練並轉換資料\n",
    "    model = pipeline.fit(df)\n",
    "    transformed_df = model.transform(df)\n",
    "    \n",
    "    return transformed_df, model\n",
    "\n",
    "# 準備鳶尾花資料\n",
    "feature_cols = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "iris_prepared, iris_preprocessor = prepare_classification_features(iris_df, feature_cols, 'species')\n",
    "\n",
    "iris_prepared.select('species', 'label', 'features', 'scaledFeatures').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 分類問題\n",
    "\n",
    "我們將使用多種分類演算法來預測鳶尾花的品種。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割訓練和測試資料\n",
    "train_df, test_df = iris_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"訓練資料數量: {train_df.count()}\")\n",
    "print(f\"測試資料數量: {test_df.count()}\")\n",
    "\n",
    "# 檢查類別分佈\n",
    "print(\"\\n訓練集類別分佈:\")\n",
    "train_df.groupBy('species').count().show()\n",
    "\n",
    "print(\"\\n測試集類別分佈:\")\n",
    "test_df.groupBy('species').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 邏輯迴歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 邏輯迴歸模型\n",
    "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"label\", maxIter=10)\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# 預測\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select('species', 'label', 'prediction', 'probability').show(10, truncate=False)\n",
    "\n",
    "# 評估\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "print(f\"邏輯迴歸準確率: {lr_accuracy:.4f}\")\n",
    "\n",
    "# 其他評估指標\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "lr_f1 = evaluator_f1.evaluate(lr_predictions)\n",
    "print(f\"邏輯迴歸 F1 分數: {lr_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 決策樹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決策樹模型\n",
    "dt = DecisionTreeClassifier(featuresCol=\"scaledFeatures\", labelCol=\"label\", maxDepth=5)\n",
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "# 預測\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "\n",
    "# 評估\n",
    "dt_accuracy = evaluator.evaluate(dt_predictions)\n",
    "dt_f1 = evaluator_f1.evaluate(dt_predictions)\n",
    "print(f\"決策樹準確率: {dt_accuracy:.4f}\")\n",
    "print(f\"決策樹 F1 分數: {dt_f1:.4f}\")\n",
    "\n",
    "# 查看特徵重要性\n",
    "print(\"\\n特徵重要性:\")\n",
    "feature_importance = dt_model.featureImportances\n",
    "for i, importance in enumerate(feature_importance):\n",
    "    print(f\"{feature_cols[i]}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 隨機森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機森林模型\n",
    "rf = RandomForestClassifier(featuresCol=\"scaledFeatures\", labelCol=\"label\", numTrees=10, maxDepth=5)\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# 預測\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "# 評估\n",
    "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
    "rf_f1 = evaluator_f1.evaluate(rf_predictions)\n",
    "print(f\"隨機森林準確率: {rf_accuracy:.4f}\")\n",
    "print(f\"隨機森林 F1 分數: {rf_f1:.4f}\")\n",
    "\n",
    "# 查看特徵重要性\n",
    "print(\"\\n特徵重要性:\")\n",
    "feature_importance = rf_model.featureImportances\n",
    "for i, importance in enumerate(feature_importance):\n",
    "    print(f\"{feature_cols[i]}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 迴歸問題\n",
    "\n",
    "使用房價資料集來示範迴歸問題的解決方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備回歸資料\n",
    "regression_feature_cols = [col for col in boston_df.columns if col != 'price']\n",
    "\n",
    "# 組合特徵向量\n",
    "assembler = VectorAssembler(inputCols=regression_feature_cols, outputCol=\"features\")\n",
    "boston_assembled = assembler.transform(boston_df)\n",
    "\n",
    "# 標準化特徵\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(boston_assembled)\n",
    "boston_prepared = scaler_model.transform(boston_assembled)\n",
    "\n",
    "# 分割資料\n",
    "train_reg, test_reg = boston_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"回歸訓練資料數量: {train_reg.count()}\")\n",
    "print(f\"回歸測試資料數量: {test_reg.count()}\")\n",
    "\n",
    "# 查看目標變數統計\n",
    "boston_prepared.describe('price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 線性迴歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線性迴歸模型\n",
    "lr_reg = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"price\", maxIter=10)\n",
    "lr_reg_model = lr_reg.fit(train_reg)\n",
    "\n",
    "# 預測\n",
    "lr_reg_predictions = lr_reg_model.transform(test_reg)\n",
    "lr_reg_predictions.select('price', 'prediction').show(10)\n",
    "\n",
    "# 評估\n",
    "reg_evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "\n",
    "lr_rmse = reg_evaluator.setMetricName(\"rmse\").evaluate(lr_reg_predictions)\n",
    "lr_mae = reg_evaluator.setMetricName(\"mae\").evaluate(lr_reg_predictions)\n",
    "lr_r2 = reg_evaluator.setMetricName(\"r2\").evaluate(lr_reg_predictions)\n",
    "\n",
    "print(f\"線性迴歸 RMSE: {lr_rmse:.4f}\")\n",
    "print(f\"線性迴歸 MAE: {lr_mae:.4f}\")\n",
    "print(f\"線性迴歸 R²: {lr_r2:.4f}\")\n",
    "\n",
    "# 模型係數\n",
    "print(f\"\\n截距: {lr_reg_model.intercept:.4f}\")\n",
    "print(\"係數:\")\n",
    "coefficients = lr_reg_model.coefficients\n",
    "for i, coef in enumerate(coefficients):\n",
    "    print(f\"{regression_feature_cols[i]}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 隨機森林迴歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機森林迴歸模型\n",
    "rf_reg = RandomForestRegressor(featuresCol=\"scaledFeatures\", labelCol=\"price\", numTrees=10, maxDepth=5)\n",
    "rf_reg_model = rf_reg.fit(train_reg)\n",
    "\n",
    "# 預測\n",
    "rf_reg_predictions = rf_reg_model.transform(test_reg)\n",
    "\n",
    "# 評估\n",
    "rf_rmse = reg_evaluator.setMetricName(\"rmse\").evaluate(rf_reg_predictions)\n",
    "rf_mae = reg_evaluator.setMetricName(\"mae\").evaluate(rf_reg_predictions)\n",
    "rf_r2 = reg_evaluator.setMetricName(\"r2\").evaluate(rf_reg_predictions)\n",
    "\n",
    "print(f\"隨機森林迴歸 RMSE: {rf_rmse:.4f}\")\n",
    "print(f\"隨機森林迴歸 MAE: {rf_mae:.4f}\")\n",
    "print(f\"隨機森林迴歸 R²: {rf_r2:.4f}\")\n",
    "\n",
    "# 特徵重要性\n",
    "print(\"\\n特徵重要性:\")\n",
    "feature_importance = rf_reg_model.featureImportances\n",
    "importance_list = [(regression_feature_cols[i], float(importance)) for i, importance in enumerate(feature_importance)]\n",
    "importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, importance in importance_list[:10]:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 聚類分析\n",
    "\n",
    "使用無監督學習進行聚類分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用鳶尾花資料進行聚類（不使用標籤）\n",
    "clustering_data = iris_prepared.select('scaledFeatures')\n",
    "\n",
    "# K-means 聚類\n",
    "kmeans = KMeans(featuresCol=\"scaledFeatures\", k=3, seed=42)\n",
    "kmeans_model = kmeans.fit(clustering_data)\n",
    "\n",
    "# 預測聚類\n",
    "kmeans_predictions = kmeans_model.transform(iris_prepared)\n",
    "kmeans_predictions.select('species', 'label', 'prediction').show(20)\n",
    "\n",
    "# 計算聚類中心\n",
    "centers = kmeans_model.clusterCenters()\n",
    "print(\"聚類中心:\")\n",
    "for i, center in enumerate(centers):\n",
    "    print(f\"聚類 {i}: {center}\")\n",
    "\n",
    "# 計算 WSSSE (Within Set Sum of Squared Errors)\n",
    "wssse = kmeans_model.summary.trainingCost\n",
    "print(f\"\\nWSSE: {wssse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較聚類結果與真實標籤\n",
    "clustering_comparison = kmeans_predictions.groupBy('species', 'prediction').count().orderBy('species', 'prediction')\n",
    "clustering_comparison.show()\n",
    "\n",
    "# 使用混淆矩陣視覺化\n",
    "confusion_data = clustering_comparison.toPandas()\n",
    "confusion_matrix = confusion_data.pivot(index='species', columns='prediction', values='count').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('K-means 聚類結果 vs 真實標籤')\n",
    "plt.xlabel('聚類標籤')\n",
    "plt.ylabel('真實標籤')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 模型管線 (Pipeline)\n",
    "\n",
    "使用 Pipeline 將預處理和模型訓練步驟整合在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立完整的機器學習管線\n",
    "def create_classification_pipeline():\n",
    "    \"\"\"\n",
    "    建立完整的分類管線\n",
    "    \"\"\"\n",
    "    # 1. 標籤編碼\n",
    "    indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
    "    \n",
    "    # 2. 特徵組合\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    \n",
    "    # 3. 特徵標準化\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    \n",
    "    # 4. 分類模型\n",
    "    rf = RandomForestClassifier(featuresCol=\"scaledFeatures\", labelCol=\"label\", numTrees=10)\n",
    "    \n",
    "    # 建立管線\n",
    "    pipeline = Pipeline(stages=[indexer, assembler, scaler, rf])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# 使用原始資料（未預處理）\n",
    "train_raw, test_raw = iris_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 建立和訓練管線\n",
    "pipeline = create_classification_pipeline()\n",
    "pipeline_model = pipeline.fit(train_raw)\n",
    "\n",
    "# 預測\n",
    "pipeline_predictions = pipeline_model.transform(test_raw)\n",
    "pipeline_predictions.select('species', 'label', 'prediction', 'probability').show(10, truncate=False)\n",
    "\n",
    "# 評估\n",
    "pipeline_accuracy = evaluator.evaluate(pipeline_predictions)\n",
    "print(f\"管線模型準確率: {pipeline_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 模型調優\n",
    "\n",
    "使用交叉驗證和參數網格搜索進行模型調優。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參數網格搜索\n",
    "def hyperparameter_tuning():\n",
    "    \"\"\"\n",
    "    使用交叉驗證進行超參數調優\n",
    "    \"\"\"\n",
    "    # 建立基本管線\n",
    "    indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    rf = RandomForestClassifier(featuresCol=\"scaledFeatures\", labelCol=\"label\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=[indexer, assembler, scaler, rf])\n",
    "    \n",
    "    # 定義參數網格\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [5, 10, 20]) \\\n",
    "        .addGrid(rf.maxDepth, [3, 5, 7]) \\\n",
    "        .build()\n",
    "    \n",
    "    # 交叉驗證\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                             estimatorParamMaps=paramGrid,\n",
    "                             evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"),\n",
    "                             numFolds=3)\n",
    "    \n",
    "    # 訓練模型\n",
    "    cv_model = crossval.fit(train_raw)\n",
    "    \n",
    "    return cv_model\n",
    "\n",
    "# 執行超參數調優\n",
    "print(\"開始超參數調優...\")\n",
    "cv_model = hyperparameter_tuning()\n",
    "\n",
    "# 使用最佳模型進行預測\n",
    "best_predictions = cv_model.transform(test_raw)\n",
    "best_accuracy = evaluator.evaluate(best_predictions)\n",
    "print(f\"最佳模型準確率: {best_accuracy:.4f}\")\n",
    "\n",
    "# 取得最佳參數\n",
    "best_model = cv_model.bestModel\n",
    "best_rf = best_model.stages[-1]  # 最後一個階段是隨機森林\n",
    "print(f\"最佳參數:\")\n",
    "print(f\"  numTrees: {best_rf.getNumTrees()}\")\n",
    "print(f\"  maxDepth: {best_rf.getMaxDepth()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 模型持久化\n",
    "\n",
    "保存和載入訓練好的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model_path = \"/tmp/spark_ml_model\"\n",
    "\n",
    "try:\n",
    "    # 保存最佳模型\n",
    "    cv_model.bestModel.write().overwrite().save(model_path)\n",
    "    print(f\"模型已保存至: {model_path}\")\n",
    "    \n",
    "    # 載入模型\n",
    "    from pyspark.ml import PipelineModel\n",
    "    loaded_model = PipelineModel.load(model_path)\n",
    "    \n",
    "    # 使用載入的模型進行預測\n",
    "    loaded_predictions = loaded_model.transform(test_raw)\n",
    "    loaded_accuracy = evaluator.evaluate(loaded_predictions)\n",
    "    print(f\"載入模型準確率: {loaded_accuracy:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"模型保存/載入錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 模型解釋性\n",
    "\n",
    "理解模型的決策過程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徵重要性分析\n",
    "def analyze_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    分析特徵重要性\n",
    "    \"\"\"\n",
    "    # 取得隨機森林模型\n",
    "    rf_model = model.stages[-1]\n",
    "    \n",
    "    # 特徵重要性\n",
    "    importances = rf_model.featureImportances\n",
    "    \n",
    "    # 建立特徵重要性 DataFrame\n",
    "    importance_data = [(feature_names[i], float(importances[i])) for i in range(len(feature_names))]\n",
    "    importance_df = spark.createDataFrame(importance_data, ['feature', 'importance'])\n",
    "    \n",
    "    # 排序並顯示\n",
    "    importance_df.orderBy(col('importance').desc()).show()\n",
    "    \n",
    "    # 視覺化\n",
    "    importance_pandas = importance_df.toPandas().sort_values('importance', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_pandas['feature'], importance_pandas['importance'])\n",
    "    plt.xlabel('特徵重要性')\n",
    "    plt.title('隨機森林特徵重要性')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# 分析最佳模型的特徵重要性\n",
    "importance_df = analyze_feature_importance(cv_model.bestModel, feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 模型性能比較\n",
    "\n",
    "比較不同模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較所有模型的性能\n",
    "models_performance = {\n",
    "    '邏輯迴歸': lr_accuracy,\n",
    "    '決策樹': dt_accuracy,\n",
    "    '隨機森林': rf_accuracy,\n",
    "    '調優後隨機森林': best_accuracy\n",
    "}\n",
    "\n",
    "print(\"模型性能比較:\")\n",
    "for model_name, accuracy in models_performance.items():\n",
    "    print(f\"{model_name}: {accuracy:.4f}\")\n",
    "\n",
    "# 視覺化比較\n",
    "model_names = list(models_performance.keys())\n",
    "accuracies = list(models_performance.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, accuracies, color=['blue', 'green', 'red', 'purple'])\n",
    "plt.ylabel('準確率')\n",
    "plt.title('不同模型性能比較')\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "# 在柱狀圖上顯示數值\n",
    "for bar, accuracy in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{accuracy:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 總結\n",
    "\n",
    "在這個章節中，我們學習了：\n",
    "\n",
    "1. **MLlib 基礎概念**：了解 Spark 機器學習庫的核心組件\n",
    "2. **資料預處理**：特徵工程、標準化、編碼等技術\n",
    "3. **分類問題**：邏輯迴歸、決策樹、隨機森林等演算法\n",
    "4. **迴歸問題**：線性迴歸、隨機森林迴歸等方法\n",
    "5. **聚類分析**：K-means 等無監督學習方法\n",
    "6. **模型管線**：Pipeline 的使用和好處\n",
    "7. **模型調優**：交叉驗證和參數網格搜索\n",
    "8. **模型持久化**：保存和載入訓練好的模型\n",
    "9. **模型解釋性**：特徵重要性分析\n",
    "10. **性能比較**：不同模型的評估和比較\n",
    "\n",
    "### 關鍵要點：\n",
    "- 使用 Pipeline 可以將預處理和模型訓練步驟整合\n",
    "- 交叉驗證是避免過度擬合的重要技術\n",
    "- 特徵工程對模型性能有重大影響\n",
    "- 模型解釋性對於理解業務問題很重要\n",
    "- 不同演算法適合不同類型的問題\n",
    "\n",
    "### 下一步：\n",
    "- 學習更高級的特徵工程技術\n",
    "- 探索深度學習和神經網絡\n",
    "- 了解大規模機器學習的最佳實踐\n",
    "- 學習模型部署和監控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理資源\n",
    "spark.stop()\n",
    "print(\"Spark 會話已結束\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}