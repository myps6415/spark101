{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬å…«ç« ï¼šå¯¦æˆ°å°ˆæ¡ˆ\n",
    "\n",
    "æœ¬ç« å°‡é€šéä¸‰å€‹å®Œæ•´çš„å¯¦æˆ°å°ˆæ¡ˆä¾†ç¶œåˆæ‡‰ç”¨å‰é¢å­¸åˆ°çš„ Spark æŠ€è¡“ã€‚æ¯å€‹å°ˆæ¡ˆéƒ½æ¨¡æ“¬çœŸå¯¦çš„æ¥­å‹™å ´æ™¯ï¼ŒåŒ…å«å®Œæ•´çš„è³‡æ–™è™•ç†æµç¨‹ã€‚\n",
    "\n",
    "## å°ˆæ¡ˆæ¦‚è¦½\n",
    "1. **æ—¥èªŒåˆ†æç³»çµ±** - åˆ†æç¶²ç«™è¨ªå•æ—¥èªŒï¼Œè­˜åˆ¥ç•°å¸¸è¡Œç‚º\n",
    "2. **å³æ™‚ç›£æ§ç³»çµ±** - ç›£æ§ç³»çµ±æŒ‡æ¨™ï¼Œå³æ™‚å‘Šè­¦\n",
    "3. **æ¨è–¦ç³»çµ±** - åŸºæ–¼å”åŒéæ¿¾çš„å•†å“æ¨è–¦\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- æŒæ¡ç«¯åˆ°ç«¯çš„ Spark å°ˆæ¡ˆé–‹ç™¼æµç¨‹\n",
    "- å­¸ç¿’å¦‚ä½•è™•ç†çœŸå¯¦çš„æ¥­å‹™å•é¡Œ\n",
    "- äº†è§£ Spark åœ¨ä¸åŒé ˜åŸŸçš„æ‡‰ç”¨\n",
    "- åŸ¹é¤Šç³»çµ±æ€§æ€ç¶­å’Œè§£æ±ºå•é¡Œçš„èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦çš„åº«\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# å»ºç«‹ SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Real-world Projects\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark ç‰ˆæœ¬: {spark.version}\")\n",
    "print(f\"å¯ç”¨æ ¸å¿ƒæ•¸: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å°ˆæ¡ˆä¸€ï¼šæ—¥èªŒåˆ†æç³»çµ±\n",
    "\n",
    "### å°ˆæ¡ˆèƒŒæ™¯\n",
    "ç¶²ç«™æ¯å¤©ç”¢ç”Ÿå¤§é‡è¨ªå•æ—¥èªŒï¼Œéœ€è¦åˆ†æé€™äº›æ—¥èªŒä¾†ï¼š\n",
    "- è­˜åˆ¥ç•°å¸¸è¨ªå•æ¨¡å¼\n",
    "- åˆ†æç”¨æˆ¶è¡Œç‚º\n",
    "- ç›£æ§ç³»çµ±æ€§èƒ½\n",
    "- æª¢æ¸¬æ½›åœ¨çš„å®‰å…¨å¨è„…\n",
    "\n",
    "### æŠ€è¡“è¦é»\n",
    "- æ—¥èªŒè§£æå’Œè³‡æ–™æ¸…ç†\n",
    "- æ™‚é–“åºåˆ—åˆ†æ\n",
    "- ç•°å¸¸æª¢æ¸¬\n",
    "- è³‡æ–™è¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°ˆæ¡ˆä¸€ï¼šæ—¥èªŒåˆ†æç³»çµ±\n",
    "\n",
    "class LogAnalyzer:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        self.log_df = None\n",
    "        \n",
    "    def generate_sample_logs(self, num_logs=100000):\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆæ¨¡æ“¬çš„ç¶²ç«™è¨ªå•æ—¥èªŒ\n",
    "        \"\"\"\n",
    "        print(f\"ç”Ÿæˆ {num_logs} æ¢æ¨¡æ“¬æ—¥èªŒ...\")\n",
    "        \n",
    "        # å®šç¾©å¸¸è¦‹çš„ IP åœ°å€ã€ç”¨æˆ¶ä»£ç†ã€è«‹æ±‚è·¯å¾‘ç­‰\n",
    "        ips = [f\"192.168.1.{i}\" for i in range(1, 100)] + \\\n",
    "              [f\"10.0.0.{i}\" for i in range(1, 50)] + \\\n",
    "              [\"suspicious.ip.1\", \"suspicious.ip.2\", \"suspicious.ip.3\"]  # æ¨¡æ“¬ç•°å¸¸ IP\n",
    "        \n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
    "            \"bot/crawler\",  # æ¨¡æ“¬çˆ¬èŸ²\n",
    "            \"malicious-bot\"  # æ¨¡æ“¬æƒ¡æ„çˆ¬èŸ²\n",
    "        ]\n",
    "        \n",
    "        paths = [\n",
    "            \"/\", \"/home\", \"/about\", \"/contact\", \"/products\", \"/login\", \"/register\",\n",
    "            \"/api/users\", \"/api/products\", \"/api/orders\",\n",
    "            \"/admin\", \"/admin/users\", \"/admin/config\",  # æ•æ„Ÿè·¯å¾‘\n",
    "            \"/../../etc/passwd\", \"/admin/../../etc/passwd\"  # æ¨¡æ“¬æ”»æ“Š\n",
    "        ]\n",
    "        \n",
    "        methods = [\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\"]\n",
    "        status_codes = [200, 201, 301, 302, 400, 401, 403, 404, 500, 502, 503]\n",
    "        \n",
    "        logs = []\n",
    "        base_time = datetime.now() - timedelta(days=7)\n",
    "        \n",
    "        for i in range(num_logs):\n",
    "            # ç”Ÿæˆæ™‚é–“æˆ³ï¼ˆæ¨¡æ“¬ä¸€é€±çš„æ—¥èªŒï¼‰\n",
    "            timestamp = base_time + timedelta(\n",
    "                days=random.randint(0, 6),\n",
    "                hours=random.randint(0, 23),\n",
    "                minutes=random.randint(0, 59),\n",
    "                seconds=random.randint(0, 59)\n",
    "            )\n",
    "            \n",
    "            # æ¨¡æ“¬ç•°å¸¸è¡Œç‚º\n",
    "            if random.random() < 0.05:  # 5% çš„è«‹æ±‚æ˜¯ç•°å¸¸çš„\n",
    "                ip = random.choice([\"suspicious.ip.1\", \"suspicious.ip.2\", \"suspicious.ip.3\"])\n",
    "                path = random.choice([\"/admin\", \"/../../etc/passwd\", \"/admin/../../etc/passwd\"])\n",
    "                status_code = random.choice([401, 403, 404])\n",
    "                user_agent = random.choice([\"bot/crawler\", \"malicious-bot\"])\n",
    "                response_size = random.randint(100, 500)\n",
    "                response_time = random.uniform(0.1, 2.0)\n",
    "            else:\n",
    "                ip = random.choice(ips)\n",
    "                path = random.choice(paths)\n",
    "                status_code = random.choice(status_codes)\n",
    "                user_agent = random.choice(user_agents)\n",
    "                response_size = random.randint(500, 10000)\n",
    "                response_time = random.uniform(0.05, 1.0)\n",
    "            \n",
    "            method = random.choice(methods)\n",
    "            \n",
    "            # ç”Ÿæˆæ—¥èªŒè¡Œï¼ˆApache Common Log Formatï¼‰\n",
    "            log_line = f'{ip} - - [{timestamp.strftime(\"%d/%b/%Y:%H:%M:%S +0000\")}] \"{method} {path} HTTP/1.1\" {status_code} {response_size} \"{user_agent}\" {response_time:.3f}'\n",
    "            \n",
    "            logs.append((\n",
    "                timestamp,\n",
    "                ip,\n",
    "                method,\n",
    "                path,\n",
    "                status_code,\n",
    "                response_size,\n",
    "                user_agent,\n",
    "                response_time,\n",
    "                log_line\n",
    "            ))\n",
    "        \n",
    "        # å»ºç«‹ DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"timestamp\", TimestampType(), True),\n",
    "            StructField(\"ip\", StringType(), True),\n",
    "            StructField(\"method\", StringType(), True),\n",
    "            StructField(\"path\", StringType(), True),\n",
    "            StructField(\"status_code\", IntegerType(), True),\n",
    "            StructField(\"response_size\", IntegerType(), True),\n",
    "            StructField(\"user_agent\", StringType(), True),\n",
    "            StructField(\"response_time\", DoubleType(), True),\n",
    "            StructField(\"raw_log\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        self.log_df = self.spark.createDataFrame(logs, schema)\n",
    "        print(f\"æˆåŠŸç”Ÿæˆ {self.log_df.count()} æ¢æ—¥èªŒè¨˜éŒ„\")\n",
    "        \n",
    "        return self.log_df\n",
    "    \n",
    "    def basic_statistics(self):\n",
    "        \"\"\"\n",
    "        åŸºæœ¬çµ±è¨ˆåˆ†æ\n",
    "        \"\"\"\n",
    "        print(\"\\n=== åŸºæœ¬çµ±è¨ˆåˆ†æ ===\")\n",
    "        \n",
    "        # ç¸½é«”çµ±è¨ˆ\n",
    "        total_requests = self.log_df.count()\n",
    "        unique_ips = self.log_df.select(\"ip\").distinct().count()\n",
    "        unique_paths = self.log_df.select(\"path\").distinct().count()\n",
    "        \n",
    "        print(f\"ç¸½è«‹æ±‚æ•¸: {total_requests:,}\")\n",
    "        print(f\"å”¯ä¸€ IP æ•¸: {unique_ips:,}\")\n",
    "        print(f\"å”¯ä¸€è·¯å¾‘æ•¸: {unique_paths:,}\")\n",
    "        \n",
    "        # ç‹€æ…‹ç¢¼åˆ†ä½ˆ\n",
    "        print(\"\\nç‹€æ…‹ç¢¼åˆ†ä½ˆ:\")\n",
    "        status_dist = self.log_df.groupBy(\"status_code\") \\\n",
    "                                .count() \\\n",
    "                                .orderBy(\"status_code\") \\\n",
    "                                .collect()\n",
    "        \n",
    "        for row in status_dist:\n",
    "            percentage = (row['count'] / total_requests) * 100\n",
    "            print(f\"  {row['status_code']}: {row['count']:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # è«‹æ±‚æ–¹æ³•åˆ†ä½ˆ\n",
    "        print(\"\\nè«‹æ±‚æ–¹æ³•åˆ†ä½ˆ:\")\n",
    "        method_dist = self.log_df.groupBy(\"method\") \\\n",
    "                                .count() \\\n",
    "                                .orderBy(col(\"count\").desc()) \\\n",
    "                                .collect()\n",
    "        \n",
    "        for row in method_dist:\n",
    "            percentage = (row['count'] / total_requests) * 100\n",
    "            print(f\"  {row['method']}: {row['count']:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # éŸ¿æ‡‰æ™‚é–“çµ±è¨ˆ\n",
    "        print(\"\\néŸ¿æ‡‰æ™‚é–“çµ±è¨ˆ:\")\n",
    "        response_time_stats = self.log_df.select(\"response_time\").describe().collect()\n",
    "        for row in response_time_stats:\n",
    "            print(f\"  {row['summary']}: {float(row['response_time']):.3f}s\")\n",
    "    \n",
    "    def detect_anomalies(self):\n",
    "        \"\"\"\n",
    "        ç•°å¸¸æª¢æ¸¬\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ç•°å¸¸æª¢æ¸¬ ===\")\n",
    "        \n",
    "        # 1. æª¢æ¸¬é«˜é »ç‡è«‹æ±‚çš„ IPï¼ˆå¯èƒ½æ˜¯æ”»æ“Šæˆ–çˆ¬èŸ²ï¼‰\n",
    "        print(\"\\n1. é«˜é »ç‡è«‹æ±‚ IP æª¢æ¸¬:\")\n",
    "        high_freq_ips = self.log_df.groupBy(\"ip\") \\\n",
    "                                  .count() \\\n",
    "                                  .filter(col(\"count\") > 1000) \\\n",
    "                                  .orderBy(col(\"count\").desc())\n",
    "        \n",
    "        high_freq_ips.show()\n",
    "        \n",
    "        # 2. æª¢æ¸¬ç•°å¸¸ç‹€æ…‹ç¢¼ï¼ˆ4xx, 5xx éŒ¯èª¤ï¼‰\n",
    "        print(\"\\n2. ç•°å¸¸ç‹€æ…‹ç¢¼åˆ†æ:\")\n",
    "        error_analysis = self.log_df.filter((col(\"status_code\") >= 400) | (col(\"status_code\") >= 500)) \\\n",
    "                                   .groupBy(\"ip\", \"status_code\") \\\n",
    "                                   .count() \\\n",
    "                                   .filter(col(\"count\") > 50) \\\n",
    "                                   .orderBy(col(\"count\").desc())\n",
    "        \n",
    "        error_analysis.show()\n",
    "        \n",
    "        # 3. æª¢æ¸¬æ•æ„Ÿè·¯å¾‘è¨ªå•\n",
    "        print(\"\\n3. æ•æ„Ÿè·¯å¾‘è¨ªå•æª¢æ¸¬:\")\n",
    "        sensitive_paths = [\"/admin\", \"/../../etc/passwd\", \"/admin/../../etc/passwd\"]\n",
    "        \n",
    "        sensitive_access = self.log_df.filter(col(\"path\").rlike(\"admin|etc/passwd|config\")) \\\n",
    "                                     .groupBy(\"ip\", \"path\") \\\n",
    "                                     .count() \\\n",
    "                                     .orderBy(col(\"count\").desc())\n",
    "        \n",
    "        sensitive_access.show(truncate=False)\n",
    "        \n",
    "        # 4. æª¢æ¸¬ç•°å¸¸éŸ¿æ‡‰æ™‚é–“\n",
    "        print(\"\\n4. ç•°å¸¸éŸ¿æ‡‰æ™‚é–“æª¢æ¸¬:\")\n",
    "        \n",
    "        # è¨ˆç®—éŸ¿æ‡‰æ™‚é–“çš„çµ±è¨ˆé‡\n",
    "        response_time_stats = self.log_df.select(\n",
    "            mean(\"response_time\").alias(\"mean_response_time\"),\n",
    "            stddev(\"response_time\").alias(\"std_response_time\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        mean_time = response_time_stats['mean_response_time']\n",
    "        std_time = response_time_stats['std_response_time']\n",
    "        threshold = mean_time + 2 * std_time  # 2 æ¨™æº–å·®ä½œç‚ºé–¾å€¼\n",
    "        \n",
    "        slow_requests = self.log_df.filter(col(\"response_time\") > threshold) \\\n",
    "                                  .select(\"timestamp\", \"ip\", \"path\", \"response_time\") \\\n",
    "                                  .orderBy(col(\"response_time\").desc())\n",
    "        \n",
    "        print(f\"éŸ¿æ‡‰æ™‚é–“ç•°å¸¸é–¾å€¼: {threshold:.3f}s\")\n",
    "        print(f\"ç•°å¸¸æ…¢è«‹æ±‚æ•¸é‡: {slow_requests.count()}\")\n",
    "        slow_requests.show(10)\n",
    "        \n",
    "        # 5. æª¢æ¸¬çˆ¬èŸ²è¡Œç‚º\n",
    "        print(\"\\n5. çˆ¬èŸ²è¡Œç‚ºæª¢æ¸¬:\")\n",
    "        bot_behavior = self.log_df.filter(col(\"user_agent\").rlike(\"bot|crawler|spider\")) \\\n",
    "                                 .groupBy(\"ip\", \"user_agent\") \\\n",
    "                                 .count() \\\n",
    "                                 .orderBy(col(\"count\").desc())\n",
    "        \n",
    "        bot_behavior.show(truncate=False)\n",
    "        \n",
    "        return {\n",
    "            'high_freq_ips': high_freq_ips,\n",
    "            'error_analysis': error_analysis,\n",
    "            'sensitive_access': sensitive_access,\n",
    "            'slow_requests': slow_requests,\n",
    "            'bot_behavior': bot_behavior\n",
    "        }\n",
    "    \n",
    "    def time_series_analysis(self):\n",
    "        \"\"\"\n",
    "        æ™‚é–“åºåˆ—åˆ†æ\n",
    "        \"\"\"\n",
    "        print(\"\\n=== æ™‚é–“åºåˆ—åˆ†æ ===\")\n",
    "        \n",
    "        # æŒ‰å°æ™‚çµ±è¨ˆè«‹æ±‚é‡\n",
    "        hourly_stats = self.log_df.withColumn(\"hour\", hour(\"timestamp\")) \\\n",
    "                                 .groupBy(\"hour\") \\\n",
    "                                 .agg(\n",
    "                                     count(\"*\").alias(\"request_count\"),\n",
    "                                     avg(\"response_time\").alias(\"avg_response_time\"),\n",
    "                                     countDistinct(\"ip\").alias(\"unique_ips\")\n",
    "                                 ) \\\n",
    "                                 .orderBy(\"hour\")\n",
    "        \n",
    "        print(\"\\næ¯å°æ™‚è«‹æ±‚çµ±è¨ˆ:\")\n",
    "        hourly_stats.show(24)\n",
    "        \n",
    "        # æŒ‰å¤©çµ±è¨ˆ\n",
    "        daily_stats = self.log_df.withColumn(\"date\", date_format(\"timestamp\", \"yyyy-MM-dd\")) \\\n",
    "                                .groupBy(\"date\") \\\n",
    "                                .agg(\n",
    "                                    count(\"*\").alias(\"request_count\"),\n",
    "                                    avg(\"response_time\").alias(\"avg_response_time\"),\n",
    "                                    countDistinct(\"ip\").alias(\"unique_ips\")\n",
    "                                ) \\\n",
    "                                .orderBy(\"date\")\n",
    "        \n",
    "        print(\"\\næ¯æ—¥è«‹æ±‚çµ±è¨ˆ:\")\n",
    "        daily_stats.show()\n",
    "        \n",
    "        # è¦–è¦ºåŒ–æ™‚é–“åºåˆ—\n",
    "        hourly_data = hourly_stats.toPandas()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # æ¯å°æ™‚è«‹æ±‚é‡\n",
    "        axes[0, 0].plot(hourly_data['hour'], hourly_data['request_count'], marker='o')\n",
    "        axes[0, 0].set_title('æ¯å°æ™‚è«‹æ±‚é‡')\n",
    "        axes[0, 0].set_xlabel('å°æ™‚')\n",
    "        axes[0, 0].set_ylabel('è«‹æ±‚æ•¸')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # æ¯å°æ™‚å¹³å‡éŸ¿æ‡‰æ™‚é–“\n",
    "        axes[0, 1].plot(hourly_data['hour'], hourly_data['avg_response_time'], marker='o', color='red')\n",
    "        axes[0, 1].set_title('æ¯å°æ™‚å¹³å‡éŸ¿æ‡‰æ™‚é–“')\n",
    "        axes[0, 1].set_xlabel('å°æ™‚')\n",
    "        axes[0, 1].set_ylabel('éŸ¿æ‡‰æ™‚é–“ (ç§’)')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # æ¯å°æ™‚å”¯ä¸€ IP æ•¸\n",
    "        axes[1, 0].plot(hourly_data['hour'], hourly_data['unique_ips'], marker='o', color='green')\n",
    "        axes[1, 0].set_title('æ¯å°æ™‚å”¯ä¸€ IP æ•¸')\n",
    "        axes[1, 0].set_xlabel('å°æ™‚')\n",
    "        axes[1, 0].set_ylabel('å”¯ä¸€ IP æ•¸')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # ç‹€æ…‹ç¢¼åˆ†ä½ˆ\n",
    "        status_data = self.log_df.groupBy(\"status_code\").count().toPandas()\n",
    "        axes[1, 1].pie(status_data['count'], labels=status_data['status_code'], autopct='%1.1f%%')\n",
    "        axes[1, 1].set_title('ç‹€æ…‹ç¢¼åˆ†ä½ˆ')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return hourly_stats, daily_stats\n",
    "    \n",
    "    def generate_security_report(self):\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆå®‰å…¨å ±å‘Š\n",
    "        \"\"\"\n",
    "        print(\"\\n=== å®‰å…¨åˆ†æå ±å‘Š ===\")\n",
    "        \n",
    "        # å¨è„…ç­‰ç´šè©•ä¼°\n",
    "        threats = []\n",
    "        \n",
    "        # æª¢æ¸¬ SQL æ³¨å…¥å˜—è©¦\n",
    "        sql_injection = self.log_df.filter(\n",
    "            col(\"path\").rlike(\"(union|select|drop|insert|update|delete|script|alert)\") |\n",
    "            col(\"path\").contains(\"'\") |\n",
    "            col(\"path\").contains(\"--\")\n",
    "        ).count()\n",
    "        \n",
    "        if sql_injection > 0:\n",
    "            threats.append(f\"æª¢æ¸¬åˆ° {sql_injection} æ¬¡å¯èƒ½çš„ SQL æ³¨å…¥å˜—è©¦\")\n",
    "        \n",
    "        # æª¢æ¸¬ç›®éŒ„éæ­·æ”»æ“Š\n",
    "        directory_traversal = self.log_df.filter(\n",
    "            col(\"path\").contains(\"../\") |\n",
    "            col(\"path\").contains(\"..\\\\\")\n",
    "        ).count()\n",
    "        \n",
    "        if directory_traversal > 0:\n",
    "            threats.append(f\"æª¢æ¸¬åˆ° {directory_traversal} æ¬¡å¯èƒ½çš„ç›®éŒ„éæ­·æ”»æ“Š\")\n",
    "        \n",
    "        # æª¢æ¸¬æš´åŠ›ç ´è§£\n",
    "        brute_force = self.log_df.filter(\n",
    "            (col(\"path\").contains(\"/login\") | col(\"path\").contains(\"/admin\")) &\n",
    "            (col(\"status_code\") == 401)\n",
    "        ).groupBy(\"ip\").count().filter(col(\"count\") > 10).count()\n",
    "        \n",
    "        if brute_force > 0:\n",
    "            threats.append(f\"æª¢æ¸¬åˆ° {brute_force} å€‹ IP å¯èƒ½é€²è¡Œæš´åŠ›ç ´è§£æ”»æ“Š\")\n",
    "        \n",
    "        # æª¢æ¸¬ DDoS æ”»æ“Š\n",
    "        ddos_threshold = 5000  # æ¯å€‹ IP è¶…é 5000 æ¬¡è«‹æ±‚è¦–ç‚ºå¯ç–‘\n",
    "        ddos_ips = self.log_df.groupBy(\"ip\").count().filter(col(\"count\") > ddos_threshold).count()\n",
    "        \n",
    "        if ddos_ips > 0:\n",
    "            threats.append(f\"æª¢æ¸¬åˆ° {ddos_ips} å€‹ IP å¯èƒ½é€²è¡Œ DDoS æ”»æ“Š\")\n",
    "        \n",
    "        # è¼¸å‡ºå¨è„…å ±å‘Š\n",
    "        if threats:\n",
    "            print(\"\\nğŸš¨ æª¢æ¸¬åˆ°ä»¥ä¸‹å®‰å…¨å¨è„…:\")\n",
    "            for i, threat in enumerate(threats, 1):\n",
    "                print(f\"  {i}. {threat}\")\n",
    "        else:\n",
    "            print(\"\\nâœ… æœªæª¢æ¸¬åˆ°æ˜é¡¯çš„å®‰å…¨å¨è„…\")\n",
    "        \n",
    "        # ç”Ÿæˆå»ºè­°\n",
    "        print(\"\\nğŸ’¡ å®‰å…¨å»ºè­°:\")\n",
    "        suggestions = [\n",
    "            \"å®šæœŸç›£æ§ç•°å¸¸ IP åœ°å€çš„æ´»å‹•\",\n",
    "            \"å°æ•æ„Ÿç«¯é»å¯¦æ–½è¨ªå•æ§åˆ¶\",\n",
    "            \"è¨­ç½®é€Ÿç‡é™åˆ¶ä»¥é˜²æ­¢æš´åŠ›ç ´è§£\",\n",
    "            \"å•Ÿç”¨ Web æ‡‰ç”¨é˜²ç«ç‰† (WAF)\",\n",
    "            \"å®šæœŸæ›´æ–°å®‰å…¨è¦å‰‡å’Œç°½å\",\n",
    "            \"å»ºç«‹å³æ™‚è­¦å ±ç³»çµ±\"\n",
    "        ]\n",
    "        \n",
    "        for i, suggestion in enumerate(suggestions, 1):\n",
    "            print(f\"  {i}. {suggestion}\")\n",
    "        \n",
    "        return threats\n",
    "\n",
    "# åŸ·è¡Œæ—¥èªŒåˆ†æç³»çµ±\n",
    "print(\"=== å°ˆæ¡ˆä¸€ï¼šæ—¥èªŒåˆ†æç³»çµ± ===\")\n",
    "\n",
    "log_analyzer = LogAnalyzer(spark)\n",
    "log_data = log_analyzer.generate_sample_logs(50000)\n",
    "\n",
    "# é¡¯ç¤ºæ¨£æœ¬è³‡æ–™\n",
    "print(\"\\næ—¥èªŒæ¨£æœ¬:\")\n",
    "log_data.show(10, truncate=False)\n",
    "\n",
    "# åŸ·è¡Œåˆ†æ\n",
    "log_analyzer.basic_statistics()\n",
    "anomalies = log_analyzer.detect_anomalies()\n",
    "hourly_stats, daily_stats = log_analyzer.time_series_analysis()\n",
    "threats = log_analyzer.generate_security_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å°ˆæ¡ˆäºŒï¼šå³æ™‚ç›£æ§ç³»çµ±\n",
    "\n",
    "### å°ˆæ¡ˆèƒŒæ™¯\n",
    "å»ºç«‹ä¸€å€‹å³æ™‚ç›£æ§ç³»çµ±ä¾†ç›£æ§æœå‹™å™¨å’Œæ‡‰ç”¨ç¨‹å¼çš„å¥åº·ç‹€æ…‹ï¼š\n",
    "- ç›£æ§ç³»çµ±æŒ‡æ¨™ï¼ˆCPUã€è¨˜æ†¶é«”ã€ç£ç¢Ÿã€ç¶²è·¯ï¼‰\n",
    "- æª¢æ¸¬ç•°å¸¸æƒ…æ³ä¸¦ç™¼é€è­¦å ±\n",
    "- æä¾›æ­·å²è¶¨å‹¢åˆ†æ\n",
    "- é æ¸¬æ½›åœ¨çš„ç³»çµ±å•é¡Œ\n",
    "\n",
    "### æŠ€è¡“è¦é»\n",
    "- å³æ™‚è³‡æ–™è™•ç†\n",
    "- é–¾å€¼ç›£æ§\n",
    "- è¶¨å‹¢åˆ†æ\n",
    "- ç•°å¸¸é æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°ˆæ¡ˆäºŒï¼šå³æ™‚ç›£æ§ç³»çµ±\n",
    "\n",
    "class MonitoringSystem:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        self.metrics_df = None\n",
    "        self.alert_thresholds = {\n",
    "            'cpu_usage': 80.0,\n",
    "            'memory_usage': 85.0,\n",
    "            'disk_usage': 90.0,\n",
    "            'network_latency': 100.0,\n",
    "            'error_rate': 5.0\n",
    "        }\n",
    "    \n",
    "    def generate_metrics_data(self, num_records=50000):\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆæ¨¡æ“¬çš„ç³»çµ±æŒ‡æ¨™è³‡æ–™\n",
    "        \"\"\"\n",
    "        print(f\"ç”Ÿæˆ {num_records} æ¢ç³»çµ±æŒ‡æ¨™è³‡æ–™...\")\n",
    "        \n",
    "        servers = [f\"server-{i:02d}\" for i in range(1, 21)]  # 20 å°æœå‹™å™¨\n",
    "        services = [\"web\", \"api\", \"database\", \"cache\", \"queue\"]\n",
    "        \n",
    "        metrics = []\n",
    "        base_time = datetime.now() - timedelta(hours=24)\n",
    "        \n",
    "        for i in range(num_records):\n",
    "            timestamp = base_time + timedelta(seconds=i * 30)  # æ¯ 30 ç§’ä¸€å€‹è³‡æ–™é»\n",
    "            server = random.choice(servers)\n",
    "            service = random.choice(services)\n",
    "            \n",
    "            # ç”ŸæˆåŸºæº–å€¼\n",
    "            if service == \"database\":\n",
    "                # è³‡æ–™åº«é€šå¸¸ä½¿ç”¨æ›´å¤š CPU å’Œè¨˜æ†¶é«”\n",
    "                cpu_base = 60 + random.gauss(0, 10)\n",
    "                memory_base = 70 + random.gauss(0, 10)\n",
    "            elif service == \"web\":\n",
    "                cpu_base = 30 + random.gauss(0, 10)\n",
    "                memory_base = 40 + random.gauss(0, 10)\n",
    "            else:\n",
    "                cpu_base = 45 + random.gauss(0, 10)\n",
    "                memory_base = 50 + random.gauss(0, 10)\n",
    "            \n",
    "            # æ¨¡æ“¬ç•°å¸¸æƒ…æ³\n",
    "            if random.random() < 0.1:  # 10% çš„æ™‚é–“æœ‰ç•°å¸¸\n",
    "                cpu_usage = min(100, max(0, cpu_base + random.gauss(30, 10)))\n",
    "                memory_usage = min(100, max(0, memory_base + random.gauss(25, 10)))\n",
    "                disk_usage = min(100, max(0, random.gauss(85, 10)))\n",
    "                network_latency = max(0, random.gauss(120, 30))\n",
    "                error_rate = max(0, random.gauss(8, 3))\n",
    "            else:\n",
    "                cpu_usage = min(100, max(0, cpu_base))\n",
    "                memory_usage = min(100, max(0, memory_base))\n",
    "                disk_usage = min(100, max(0, random.gauss(65, 15)))\n",
    "                network_latency = max(0, random.gauss(25, 10))\n",
    "                error_rate = max(0, random.gauss(1, 0.5))\n",
    "            \n",
    "            request_count = max(0, int(random.gauss(100, 30)))\n",
    "            response_time = max(0, random.gauss(200, 50))\n",
    "            \n",
    "            metrics.append((\n",
    "                timestamp,\n",
    "                server,\n",
    "                service,\n",
    "                cpu_usage,\n",
    "                memory_usage,\n",
    "                disk_usage,\n",
    "                network_latency,\n",
    "                error_rate,\n",
    "                request_count,\n",
    "                response_time\n",
    "            ))\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"timestamp\", TimestampType(), True),\n",
    "            StructField(\"server\", StringType(), True),\n",
    "            StructField(\"service\", StringType(), True),\n",
    "            StructField(\"cpu_usage\", DoubleType(), True),\n",
    "            StructField(\"memory_usage\", DoubleType(), True),\n",
    "            StructField(\"disk_usage\", DoubleType(), True),\n",
    "            StructField(\"network_latency\", DoubleType(), True),\n",
    "            StructField(\"error_rate\", DoubleType(), True),\n",
    "            StructField(\"request_count\", IntegerType(), True),\n",
    "            StructField(\"response_time\", DoubleType(), True)\n",
    "        ])\n",
    "        \n",
    "        self.metrics_df = self.spark.createDataFrame(metrics, schema)\n",
    "        print(f\"æˆåŠŸç”Ÿæˆ {self.metrics_df.count()} æ¢ç³»çµ±æŒ‡æ¨™\")\n",
    "        \n",
    "        return self.metrics_df\n",
    "    \n",
    "    def monitor_alerts(self):\n",
    "        \"\"\"\n",
    "        ç›£æ§è­¦å ±\n",
    "        \"\"\"\n",
    "        print(\"\\n=== è­¦å ±ç›£æ§ ===\")\n",
    "        \n",
    "        alerts = []\n",
    "        \n",
    "        # CPU ä½¿ç”¨ç‡è­¦å ±\n",
    "        cpu_alerts = self.metrics_df.filter(\n",
    "            col(\"cpu_usage\") > self.alert_thresholds['cpu_usage']\n",
    "        ).select(\"timestamp\", \"server\", \"service\", \"cpu_usage\")\n",
    "        \n",
    "        cpu_count = cpu_alerts.count()\n",
    "        if cpu_count > 0:\n",
    "            alerts.append(f\"CPU ä½¿ç”¨ç‡è­¦å ±: {cpu_count} æ¬¡\")\n",
    "            print(f\"\\nğŸš¨ CPU ä½¿ç”¨ç‡è¶…é {self.alert_thresholds['cpu_usage']}% çš„è¨˜éŒ„:\")\n",
    "            cpu_alerts.orderBy(col(\"cpu_usage\").desc()).show(10)\n",
    "        \n",
    "        # è¨˜æ†¶é«”ä½¿ç”¨ç‡è­¦å ±\n",
    "        memory_alerts = self.metrics_df.filter(\n",
    "            col(\"memory_usage\") > self.alert_thresholds['memory_usage']\n",
    "        ).select(\"timestamp\", \"server\", \"service\", \"memory_usage\")\n",
    "        \n",
    "        memory_count = memory_alerts.count()\n",
    "        if memory_count > 0:\n",
    "            alerts.append(f\"è¨˜æ†¶é«”ä½¿ç”¨ç‡è­¦å ±: {memory_count} æ¬¡\")\n",
    "            print(f\"\\nğŸš¨ è¨˜æ†¶é«”ä½¿ç”¨ç‡è¶…é {self.alert_thresholds['memory_usage']}% çš„è¨˜éŒ„:\")\n",
    "            memory_alerts.orderBy(col(\"memory_usage\").desc()).show(10)\n",
    "        \n",
    "        # ç£ç¢Ÿä½¿ç”¨ç‡è­¦å ±\n",
    "        disk_alerts = self.metrics_df.filter(\n",
    "            col(\"disk_usage\") > self.alert_thresholds['disk_usage']\n",
    "        ).select(\"timestamp\", \"server\", \"service\", \"disk_usage\")\n",
    "        \n",
    "        disk_count = disk_alerts.count()\n",
    "        if disk_count > 0:\n",
    "            alerts.append(f\"ç£ç¢Ÿä½¿ç”¨ç‡è­¦å ±: {disk_count} æ¬¡\")\n",
    "            print(f\"\\nğŸš¨ ç£ç¢Ÿä½¿ç”¨ç‡è¶…é {self.alert_thresholds['disk_usage']}% çš„è¨˜éŒ„:\")\n",
    "            disk_alerts.orderBy(col(\"disk_usage\").desc()).show(10)\n",
    "        \n",
    "        # ç¶²è·¯å»¶é²è­¦å ±\n",
    "        network_alerts = self.metrics_df.filter(\n",
    "            col(\"network_latency\") > self.alert_thresholds['network_latency']\n",
    "        ).select(\"timestamp\", \"server\", \"service\", \"network_latency\")\n",
    "        \n",
    "        network_count = network_alerts.count()\n",
    "        if network_count > 0:\n",
    "            alerts.append(f\"ç¶²è·¯å»¶é²è­¦å ±: {network_count} æ¬¡\")\n",
    "            print(f\"\\nğŸš¨ ç¶²è·¯å»¶é²è¶…é {self.alert_thresholds['network_latency']}ms çš„è¨˜éŒ„:\")\n",
    "            network_alerts.orderBy(col(\"network_latency\").desc()).show(10)\n",
    "        \n",
    "        # éŒ¯èª¤ç‡è­¦å ±\n",
    "        error_alerts = self.metrics_df.filter(\n",
    "            col(\"error_rate\") > self.alert_thresholds['error_rate']\n",
    "        ).select(\"timestamp\", \"server\", \"service\", \"error_rate\")\n",
    "        \n",
    "        error_count = error_alerts.count()\n",
    "        if error_count > 0:\n",
    "            alerts.append(f\"éŒ¯èª¤ç‡è­¦å ±: {error_count} æ¬¡\")\n",
    "            print(f\"\\nğŸš¨ éŒ¯èª¤ç‡è¶…é {self.alert_thresholds['error_rate']}% çš„è¨˜éŒ„:\")\n",
    "            error_alerts.orderBy(col(\"error_rate\").desc()).show(10)\n",
    "        \n",
    "        # ç”Ÿæˆè­¦å ±æ‘˜è¦\n",
    "        print(\"\\nğŸ“Š è­¦å ±æ‘˜è¦:\")\n",
    "        if alerts:\n",
    "            for alert in alerts:\n",
    "                print(f\"  â€¢ {alert}\")\n",
    "        else:\n",
    "            print(\"  âœ… æ²’æœ‰è§¸ç™¼è­¦å ±\")\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def system_health_dashboard(self):\n",
    "        \"\"\"\n",
    "        ç³»çµ±å¥åº·å„€è¡¨æ¿\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ç³»çµ±å¥åº·å„€è¡¨æ¿ ===\")\n",
    "        \n",
    "        # è¨ˆç®—å„æœå‹™çš„å¹³å‡æŒ‡æ¨™\n",
    "        service_health = self.metrics_df.groupBy(\"service\").agg(\n",
    "            avg(\"cpu_usage\").alias(\"avg_cpu\"),\n",
    "            avg(\"memory_usage\").alias(\"avg_memory\"),\n",
    "            avg(\"disk_usage\").alias(\"avg_disk\"),\n",
    "            avg(\"network_latency\").alias(\"avg_latency\"),\n",
    "            avg(\"error_rate\").alias(\"avg_error_rate\"),\n",
    "            avg(\"response_time\").alias(\"avg_response_time\")\n",
    "        ).orderBy(\"service\")\n",
    "        \n",
    "        print(\"\\nå„æœå‹™å¥åº·ç‹€æ…‹:\")\n",
    "        service_health.show()\n",
    "        \n",
    "        # è¨ˆç®—å„æœå‹™å™¨çš„å¹³å‡æŒ‡æ¨™\n",
    "        server_health = self.metrics_df.groupBy(\"server\").agg(\n",
    "            avg(\"cpu_usage\").alias(\"avg_cpu\"),\n",
    "            avg(\"memory_usage\").alias(\"avg_memory\"),\n",
    "            avg(\"disk_usage\").alias(\"avg_disk\"),\n",
    "            count(\"*\").alias(\"data_points\")\n",
    "        ).orderBy(\"server\")\n",
    "        \n",
    "        print(\"\\nå„æœå‹™å™¨å¥åº·ç‹€æ…‹:\")\n",
    "        server_health.show()\n",
    "        \n",
    "        # è­˜åˆ¥æœ€ç¹å¿™çš„æœå‹™å™¨\n",
    "        busy_servers = self.metrics_df.groupBy(\"server\").agg(\n",
    "            avg(\"cpu_usage\").alias(\"avg_cpu\"),\n",
    "            avg(\"memory_usage\").alias(\"avg_memory\"),\n",
    "            sum(\"request_count\").alias(\"total_requests\")\n",
    "        ).orderBy(col(\"avg_cpu\").desc())\n",
    "        \n",
    "        print(\"\\næœ€ç¹å¿™çš„æœå‹™å™¨ (æŒ‰ CPU ä½¿ç”¨ç‡æ’åº):\")\n",
    "        busy_servers.show(10)\n",
    "        \n",
    "        # è¦–è¦ºåŒ–ç³»çµ±å¥åº·ç‹€æ…‹\n",
    "        service_data = service_health.toPandas()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # CPU ä½¿ç”¨ç‡\n",
    "        axes[0, 0].bar(service_data['service'], service_data['avg_cpu'])\n",
    "        axes[0, 0].set_title('å„æœå‹™å¹³å‡ CPU ä½¿ç”¨ç‡')\n",
    "        axes[0, 0].set_ylabel('CPU ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # è¨˜æ†¶é«”ä½¿ç”¨ç‡\n",
    "        axes[0, 1].bar(service_data['service'], service_data['avg_memory'], color='orange')\n",
    "        axes[0, 1].set_title('å„æœå‹™å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨ç‡')\n",
    "        axes[0, 1].set_ylabel('è¨˜æ†¶é«”ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # ç£ç¢Ÿä½¿ç”¨ç‡\n",
    "        axes[0, 2].bar(service_data['service'], service_data['avg_disk'], color='green')\n",
    "        axes[0, 2].set_title('å„æœå‹™å¹³å‡ç£ç¢Ÿä½¿ç”¨ç‡')\n",
    "        axes[0, 2].set_ylabel('ç£ç¢Ÿä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # ç¶²è·¯å»¶é²\n",
    "        axes[1, 0].bar(service_data['service'], service_data['avg_latency'], color='red')\n",
    "        axes[1, 0].set_title('å„æœå‹™å¹³å‡ç¶²è·¯å»¶é²')\n",
    "        axes[1, 0].set_ylabel('å»¶é² (ms)')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # éŒ¯èª¤ç‡\n",
    "        axes[1, 1].bar(service_data['service'], service_data['avg_error_rate'], color='purple')\n",
    "        axes[1, 1].set_title('å„æœå‹™å¹³å‡éŒ¯èª¤ç‡')\n",
    "        axes[1, 1].set_ylabel('éŒ¯èª¤ç‡ (%)')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # éŸ¿æ‡‰æ™‚é–“\n",
    "        axes[1, 2].bar(service_data['service'], service_data['avg_response_time'], color='brown')\n",
    "        axes[1, 2].set_title('å„æœå‹™å¹³å‡éŸ¿æ‡‰æ™‚é–“')\n",
    "        axes[1, 2].set_ylabel('éŸ¿æ‡‰æ™‚é–“ (ms)')\n",
    "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return service_health, server_health\n",
    "    \n",
    "    def trend_analysis(self):\n",
    "        \"\"\"\n",
    "        è¶¨å‹¢åˆ†æ\n",
    "        \"\"\"\n",
    "        print(\"\\n=== è¶¨å‹¢åˆ†æ ===\")\n",
    "        \n",
    "        # æŒ‰å°æ™‚åˆ†æè¶¨å‹¢\n",
    "        hourly_trends = self.metrics_df.withColumn(\"hour\", hour(\"timestamp\")) \\\n",
    "                                      .groupBy(\"hour\") \\\n",
    "                                      .agg(\n",
    "                                          avg(\"cpu_usage\").alias(\"avg_cpu\"),\n",
    "                                          avg(\"memory_usage\").alias(\"avg_memory\"),\n",
    "                                          avg(\"network_latency\").alias(\"avg_latency\"),\n",
    "                                          sum(\"request_count\").alias(\"total_requests\")\n",
    "                                      ) \\\n",
    "                                      .orderBy(\"hour\")\n",
    "        \n",
    "        print(\"\\næ¯å°æ™‚è¶¨å‹¢:\")\n",
    "        hourly_trends.show(24)\n",
    "        \n",
    "        # æª¢æ¸¬ç•°å¸¸è¶¨å‹¢\n",
    "        print(\"\\nç•°å¸¸è¶¨å‹¢æª¢æ¸¬:\")\n",
    "        \n",
    "        # è¨ˆç®—ç§»å‹•å¹³å‡\n",
    "        window_spec = Window.orderBy(\"timestamp\").rowsBetween(-4, 0)  # 5 é»ç§»å‹•å¹³å‡\n",
    "        \n",
    "        with_moving_avg = self.metrics_df.withColumn(\n",
    "            \"cpu_moving_avg\", avg(\"cpu_usage\").over(window_spec)\n",
    "        ).withColumn(\n",
    "            \"cpu_deviation\", abs(col(\"cpu_usage\") - col(\"cpu_moving_avg\"))\n",
    "        )\n",
    "        \n",
    "        # æ‰¾å‡ºåé›¢ç§»å‹•å¹³å‡è¼ƒå¤§çš„é»\n",
    "        anomalies = with_moving_avg.filter(col(\"cpu_deviation\") > 20) \\\n",
    "                                  .select(\"timestamp\", \"server\", \"service\", \"cpu_usage\", \"cpu_moving_avg\", \"cpu_deviation\") \\\n",
    "                                  .orderBy(col(\"cpu_deviation\").desc())\n",
    "        \n",
    "        print(f\"æª¢æ¸¬åˆ° {anomalies.count()} å€‹ CPU ä½¿ç”¨ç‡ç•°å¸¸é»:\")\n",
    "        anomalies.show(10)\n",
    "        \n",
    "        # è¦–è¦ºåŒ–è¶¨å‹¢\n",
    "        trend_data = hourly_trends.toPandas()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # CPU è¶¨å‹¢\n",
    "        axes[0, 0].plot(trend_data['hour'], trend_data['avg_cpu'], marker='o')\n",
    "        axes[0, 0].set_title('æ¯å°æ™‚å¹³å‡ CPU ä½¿ç”¨ç‡è¶¨å‹¢')\n",
    "        axes[0, 0].set_xlabel('å°æ™‚')\n",
    "        axes[0, 0].set_ylabel('CPU ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # è¨˜æ†¶é«”è¶¨å‹¢\n",
    "        axes[0, 1].plot(trend_data['hour'], trend_data['avg_memory'], marker='o', color='orange')\n",
    "        axes[0, 1].set_title('æ¯å°æ™‚å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨ç‡è¶¨å‹¢')\n",
    "        axes[0, 1].set_xlabel('å°æ™‚')\n",
    "        axes[0, 1].set_ylabel('è¨˜æ†¶é«”ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # ç¶²è·¯å»¶é²è¶¨å‹¢\n",
    "        axes[1, 0].plot(trend_data['hour'], trend_data['avg_latency'], marker='o', color='red')\n",
    "        axes[1, 0].set_title('æ¯å°æ™‚å¹³å‡ç¶²è·¯å»¶é²è¶¨å‹¢')\n",
    "        axes[1, 0].set_xlabel('å°æ™‚')\n",
    "        axes[1, 0].set_ylabel('å»¶é² (ms)')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # è«‹æ±‚é‡è¶¨å‹¢\n",
    "        axes[1, 1].plot(trend_data['hour'], trend_data['total_requests'], marker='o', color='green')\n",
    "        axes[1, 1].set_title('æ¯å°æ™‚ç¸½è«‹æ±‚é‡è¶¨å‹¢')\n",
    "        axes[1, 1].set_xlabel('å°æ™‚')\n",
    "        axes[1, 1].set_ylabel('è«‹æ±‚æ•¸')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return hourly_trends, anomalies\n",
    "    \n",
    "    def generate_monitoring_report(self):\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆç›£æ§å ±å‘Š\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ç›£æ§å ±å‘Š ===\")\n",
    "        \n",
    "        # ç³»çµ±æ•´é«”å¥åº·åº¦è©•åˆ†\n",
    "        avg_metrics = self.metrics_df.agg(\n",
    "            avg(\"cpu_usage\").alias(\"avg_cpu\"),\n",
    "            avg(\"memory_usage\").alias(\"avg_memory\"),\n",
    "            avg(\"disk_usage\").alias(\"avg_disk\"),\n",
    "            avg(\"network_latency\").alias(\"avg_latency\"),\n",
    "            avg(\"error_rate\").alias(\"avg_error_rate\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        # è¨ˆç®—å¥åº·åº¦è©•åˆ†ï¼ˆ0-100ï¼‰\n",
    "        cpu_score = max(0, 100 - avg_metrics['avg_cpu'])\n",
    "        memory_score = max(0, 100 - avg_metrics['avg_memory'])\n",
    "        disk_score = max(0, 100 - avg_metrics['avg_disk'])\n",
    "        latency_score = max(0, 100 - avg_metrics['avg_latency'])\n",
    "        error_score = max(0, 100 - avg_metrics['avg_error_rate'] * 10)\n",
    "        \n",
    "        overall_score = (cpu_score + memory_score + disk_score + latency_score + error_score) / 5\n",
    "        \n",
    "        print(f\"\\nğŸ¥ ç³»çµ±æ•´é«”å¥åº·åº¦è©•åˆ†: {overall_score:.1f}/100\")\n",
    "        \n",
    "        # å¥åº·åº¦ç­‰ç´š\n",
    "        if overall_score >= 80:\n",
    "            health_status = \"å„ªç§€ ğŸ˜Š\"\n",
    "        elif overall_score >= 60:\n",
    "            health_status = \"è‰¯å¥½ ğŸ™‚\"\n",
    "        elif overall_score >= 40:\n",
    "            health_status = \"ä¸€èˆ¬ ğŸ˜\"\n",
    "        else:\n",
    "            health_status = \"éœ€è¦é—œæ³¨ ğŸ˜°\"\n",
    "        \n",
    "        print(f\"å¥åº·åº¦ç­‰ç´š: {health_status}\")\n",
    "        \n",
    "        # è©³ç´°æŒ‡æ¨™\n",
    "        print(\"\\nğŸ“Š è©³ç´°æŒ‡æ¨™:\")\n",
    "        print(f\"  CPU ä½¿ç”¨ç‡: {avg_metrics['avg_cpu']:.1f}% (è©•åˆ†: {cpu_score:.1f})\")\n",
    "        print(f\"  è¨˜æ†¶é«”ä½¿ç”¨ç‡: {avg_metrics['avg_memory']:.1f}% (è©•åˆ†: {memory_score:.1f})\")\n",
    "        print(f\"  ç£ç¢Ÿä½¿ç”¨ç‡: {avg_metrics['avg_disk']:.1f}% (è©•åˆ†: {disk_score:.1f})\")\n",
    "        print(f\"  ç¶²è·¯å»¶é²: {avg_metrics['avg_latency']:.1f}ms (è©•åˆ†: {latency_score:.1f})\")\n",
    "        print(f\"  éŒ¯èª¤ç‡: {avg_metrics['avg_error_rate']:.1f}% (è©•åˆ†: {error_score:.1f})\")\n",
    "        \n",
    "        # å»ºè­°\n",
    "        print(\"\\nğŸ’¡ å„ªåŒ–å»ºè­°:\")\n",
    "        recommendations = []\n",
    "        \n",
    "        if avg_metrics['avg_cpu'] > 70:\n",
    "            recommendations.append(\"CPU ä½¿ç”¨ç‡åé«˜ï¼Œå»ºè­°å„ªåŒ–ç¨‹å¼ç¢¼æˆ–å¢åŠ æœå‹™å™¨è³‡æº\")\n",
    "        \n",
    "        if avg_metrics['avg_memory'] > 80:\n",
    "            recommendations.append(\"è¨˜æ†¶é«”ä½¿ç”¨ç‡åé«˜ï¼Œå»ºè­°æª¢æŸ¥è¨˜æ†¶é«”æ´©æ¼æˆ–å¢åŠ è¨˜æ†¶é«”\")\n",
    "        \n",
    "        if avg_metrics['avg_disk'] > 85:\n",
    "            recommendations.append(\"ç£ç¢Ÿä½¿ç”¨ç‡åé«˜ï¼Œå»ºè­°æ¸…ç†æˆ–æ“´å±•ç£ç¢Ÿç©ºé–“\")\n",
    "        \n",
    "        if avg_metrics['avg_latency'] > 50:\n",
    "            recommendations.append(\"ç¶²è·¯å»¶é²è¼ƒé«˜ï¼Œå»ºè­°æª¢æŸ¥ç¶²è·¯é…ç½®æˆ–å„ªåŒ–ç¶²è·¯æ‹“æ’²\")\n",
    "        \n",
    "        if avg_metrics['avg_error_rate'] > 3:\n",
    "            recommendations.append(\"éŒ¯èª¤ç‡åé«˜ï¼Œå»ºè­°æª¢æŸ¥æ‡‰ç”¨ç¨‹å¼æ—¥èªŒå’ŒéŒ¯èª¤è™•ç†\")\n",
    "        \n",
    "        if recommendations:\n",
    "            for i, rec in enumerate(recommendations, 1):\n",
    "                print(f\"  {i}. {rec}\")\n",
    "        else:\n",
    "            print(\"  âœ… ç³»çµ±é‹è¡Œè‰¯å¥½ï¼Œæš«ç„¡ç‰¹åˆ¥å»ºè­°\")\n",
    "        \n",
    "        return {\n",
    "            'overall_score': overall_score,\n",
    "            'health_status': health_status,\n",
    "            'avg_metrics': avg_metrics,\n",
    "            'recommendations': recommendations\n",
    "        }\n",
    "\n",
    "# åŸ·è¡Œç›£æ§ç³»çµ±\n",
    "print(\"\\n=== å°ˆæ¡ˆäºŒï¼šå³æ™‚ç›£æ§ç³»çµ± ===\")\n",
    "\n",
    "monitoring_system = MonitoringSystem(spark)\n",
    "metrics_data = monitoring_system.generate_metrics_data(20000)\n",
    "\n",
    "# é¡¯ç¤ºæ¨£æœ¬è³‡æ–™\n",
    "print(\"\\nç³»çµ±æŒ‡æ¨™æ¨£æœ¬:\")\n",
    "metrics_data.show(10)\n",
    "\n",
    "# åŸ·è¡Œç›£æ§åˆ†æ\n",
    "alerts = monitoring_system.monitor_alerts()\n",
    "service_health, server_health = monitoring_system.system_health_dashboard()\n",
    "hourly_trends, anomalies = monitoring_system.trend_analysis()\n",
    "monitoring_report = monitoring_system.generate_monitoring_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å°ˆæ¡ˆä¸‰ï¼šæ¨è–¦ç³»çµ±\n",
    "\n",
    "### å°ˆæ¡ˆèƒŒæ™¯\n",
    "å»ºç«‹ä¸€å€‹åŸºæ–¼å”åŒéæ¿¾çš„å•†å“æ¨è–¦ç³»çµ±ï¼š\n",
    "- åˆ†æç”¨æˆ¶è¡Œç‚ºå’Œè³¼è²·æ­·å²\n",
    "- ä½¿ç”¨å”åŒéæ¿¾æ¼”ç®—æ³•ç”Ÿæˆæ¨è–¦\n",
    "- è©•ä¼°æ¨è–¦ç³»çµ±çš„æ€§èƒ½\n",
    "- æä¾›å€‹æ€§åŒ–æ¨è–¦çµæœ\n",
    "\n",
    "### æŠ€è¡“è¦é»\n",
    "- å”åŒéæ¿¾ (Collaborative Filtering)\n",
    "- çŸ©é™£åˆ†è§£ (Matrix Factorization)\n",
    "- æ¨è–¦è©•ä¼°æŒ‡æ¨™\n",
    "- å¤§è¦æ¨¡æ¨è–¦ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°ˆæ¡ˆä¸‰ï¼šæ¨è–¦ç³»çµ±\n",
    "\n",
    "class RecommendationSystem:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        self.ratings_df = None\n",
    "        self.products_df = None\n",
    "        self.users_df = None\n",
    "        self.model = None\n",
    "    \n",
    "    def generate_sample_data(self, num_users=1000, num_products=500, num_ratings=50000):\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆæ¨¡æ“¬çš„ç”¨æˆ¶è©•åˆ†è³‡æ–™\n",
    "        \"\"\"\n",
    "        print(f\"ç”Ÿæˆæ¨è–¦ç³»çµ±è³‡æ–™: {num_users} ç”¨æˆ¶, {num_products} å•†å“, {num_ratings} è©•åˆ†...\")\n",
    "        \n",
    "        # ç”Ÿæˆå•†å“è³‡æ–™\n",
    "        categories = ['Electronics', 'Books', 'Clothing', 'Sports', 'Home', 'Beauty', 'Automotive']\n",
    "        products = []\n",
    "        \n",
    "        for i in range(num_products):\n",
    "            products.append((\n",
    "                i + 1,  # product_id\n",
    "                f\"Product {i+1}\",\n",
    "                random.choice(categories),\n",
    "                random.uniform(10, 500),  # price\n",
    "                random.uniform(3.0, 5.0)  # avg_rating\n",
    "            ))\n",
    "        \n",
    "        products_schema = StructType([\n",
    "            StructField(\"product_id\", IntegerType(), True),\n",
    "            StructField(\"product_name\", StringType(), True),\n",
    "            StructField(\"category\", StringType(), True),\n",
    "            StructField(\"price\", DoubleType(), True),\n",
    "            StructField(\"avg_rating\", DoubleType(), True)\n",
    "        ])\n",
    "        \n",
    "        self.products_df = self.spark.createDataFrame(products, products_schema)\n",
    "        \n",
    "        # ç”Ÿæˆç”¨æˆ¶è³‡æ–™\n",
    "        users = []\n",
    "        age_groups = ['18-25', '26-35', '36-45', '46-55', '56+']\n",
    "        genders = ['M', 'F']\n",
    "        \n",
    "        for i in range(num_users):\n",
    "            users.append((\n",
    "                i + 1,  # user_id\n",
    "                f\"User {i+1}\",\n",
    "                random.choice(age_groups),\n",
    "                random.choice(genders),\n",
    "                random.choice(categories)  # preferred_category\n",
    "            ))\n",
    "        \n",
    "        users_schema = StructType([\n",
    "            StructField(\"user_id\", IntegerType(), True),\n",
    "            StructField(\"user_name\", StringType(), True),\n",
    "            StructField(\"age_group\", StringType(), True),\n",
    "            StructField(\"gender\", StringType(), True),\n",
    "            StructField(\"preferred_category\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        self.users_df = self.spark.createDataFrame(users, users_schema)\n",
    "        \n",
    "        # ç”Ÿæˆè©•åˆ†è³‡æ–™\n",
    "        ratings = []\n",
    "        \n",
    "        for _ in range(num_ratings):\n",
    "            user_id = random.randint(1, num_users)\n",
    "            product_id = random.randint(1, num_products)\n",
    "            \n",
    "            # æ¨¡æ“¬ç”¨æˆ¶åå¥½ï¼ˆç”¨æˆ¶æ›´å¯èƒ½å°åå¥½é¡åˆ¥çš„å•†å“çµ¦é«˜åˆ†ï¼‰\n",
    "            user_preferred_category = users[user_id - 1][4]\n",
    "            product_category = products[product_id - 1][2]\n",
    "            \n",
    "            if user_preferred_category == product_category:\n",
    "                # åå¥½é¡åˆ¥ï¼Œæ›´é«˜çš„è©•åˆ†\n",
    "                rating = random.choices([3, 4, 5], weights=[0.2, 0.4, 0.4])[0]\n",
    "            else:\n",
    "                # éåå¥½é¡åˆ¥ï¼Œæ›´åˆ†æ•£çš„è©•åˆ†\n",
    "                rating = random.choices([1, 2, 3, 4, 5], weights=[0.1, 0.2, 0.4, 0.2, 0.1])[0]\n",
    "            \n",
    "            timestamp = datetime.now() - timedelta(days=random.randint(0, 365))\n",
    "            \n",
    "            ratings.append((\n",
    "                user_id,\n",
    "                product_id,\n",
    "                float(rating),\n",
    "                timestamp\n",
    "            ))\n",
    "        \n",
    "        ratings_schema = StructType([\n",
    "            StructField(\"user_id\", IntegerType(), True),\n",
    "            StructField(\"product_id\", IntegerType(), True),\n",
    "            StructField(\"rating\", FloatType(), True),\n",
    "            StructField(\"timestamp\", TimestampType(), True)\n",
    "        ])\n",
    "        \n",
    "        self.ratings_df = self.spark.createDataFrame(ratings, ratings_schema)\n",
    "        \n",
    "        print(f\"ç”Ÿæˆå®Œæˆ: {self.users_df.count()} ç”¨æˆ¶, {self.products_df.count()} å•†å“, {self.ratings_df.count()} è©•åˆ†\")\n",
    "        \n",
    "        return self.ratings_df, self.products_df, self.users_df\n",
    "    \n",
    "    def exploratory_analysis(self):\n",
    "        \"\"\"\n",
    "        æ¢ç´¢æ€§è³‡æ–™åˆ†æ\n",
    "        \"\"\"\n",
    "        print(\"\\n=== æ¢ç´¢æ€§è³‡æ–™åˆ†æ ===\")\n",
    "        \n",
    "        # åŸºæœ¬çµ±è¨ˆ\n",
    "        total_ratings = self.ratings_df.count()\n",
    "        unique_users = self.ratings_df.select(\"user_id\").distinct().count()\n",
    "        unique_products = self.ratings_df.select(\"product_id\").distinct().count()\n",
    "        \n",
    "        print(f\"ç¸½è©•åˆ†æ•¸: {total_ratings:,}\")\n",
    "        print(f\"æ´»èºç”¨æˆ¶æ•¸: {unique_users:,}\")\n",
    "        print(f\"è¢«è©•åˆ†å•†å“æ•¸: {unique_products:,}\")\n",
    "        \n",
    "        # è©•åˆ†åˆ†ä½ˆ\n",
    "        print(\"\\nè©•åˆ†åˆ†ä½ˆ:\")\n",
    "        rating_dist = self.ratings_df.groupBy(\"rating\").count().orderBy(\"rating\")\n",
    "        rating_dist.show()\n",
    "        \n",
    "        # ç”¨æˆ¶æ´»èºåº¦åˆ†æ\n",
    "        print(\"\\nç”¨æˆ¶æ´»èºåº¦åˆ†æ:\")\n",
    "        user_activity = self.ratings_df.groupBy(\"user_id\").count().alias(\"rating_count\")\n",
    "        user_activity_stats = user_activity.describe(\"count\")\n",
    "        user_activity_stats.show()\n",
    "        \n",
    "        # å•†å“å—æ­¡è¿ç¨‹åº¦\n",
    "        print(\"\\næœ€å—æ­¡è¿çš„å•†å“:\")\n",
    "        popular_products = self.ratings_df.groupBy(\"product_id\").agg(\n",
    "            count(\"rating\").alias(\"rating_count\"),\n",
    "            avg(\"rating\").alias(\"avg_rating\")\n",
    "        ).join(self.products_df, \"product_id\") \\\n",
    "         .orderBy(col(\"rating_count\").desc())\n",
    "        \n",
    "        popular_products.select(\"product_name\", \"category\", \"rating_count\", \"avg_rating\").show(10)\n",
    "        \n",
    "        # åˆ†é¡åå¥½åˆ†æ\n",
    "        print(\"\\nåˆ†é¡åå¥½åˆ†æ:\")\n",
    "        category_preference = self.ratings_df.join(self.products_df, \"product_id\") \\\n",
    "                                           .groupBy(\"category\").agg(\n",
    "                                               count(\"rating\").alias(\"rating_count\"),\n",
    "                                               avg(\"rating\").alias(\"avg_rating\")\n",
    "                                           ).orderBy(col(\"rating_count\").desc())\n",
    "        \n",
    "        category_preference.show()\n",
    "        \n",
    "        # è¦–è¦ºåŒ–\n",
    "        rating_data = rating_dist.toPandas()\n",
    "        category_data = category_preference.toPandas()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # è©•åˆ†åˆ†ä½ˆ\n",
    "        axes[0, 0].bar(rating_data['rating'], rating_data['count'])\n",
    "        axes[0, 0].set_title('è©•åˆ†åˆ†ä½ˆ')\n",
    "        axes[0, 0].set_xlabel('è©•åˆ†')\n",
    "        axes[0, 0].set_ylabel('æ•¸é‡')\n",
    "        \n",
    "        # åˆ†é¡è©•åˆ†æ•¸é‡\n",
    "        axes[0, 1].bar(category_data['category'], category_data['rating_count'])\n",
    "        axes[0, 1].set_title('å„åˆ†é¡è©•åˆ†æ•¸é‡')\n",
    "        axes[0, 1].set_xlabel('åˆ†é¡')\n",
    "        axes[0, 1].set_ylabel('è©•åˆ†æ•¸é‡')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # åˆ†é¡å¹³å‡è©•åˆ†\n",
    "        axes[1, 0].bar(category_data['category'], category_data['avg_rating'], color='green')\n",
    "        axes[1, 0].set_title('å„åˆ†é¡å¹³å‡è©•åˆ†')\n",
    "        axes[1, 0].set_xlabel('åˆ†é¡')\n",
    "        axes[1, 0].set_ylabel('å¹³å‡è©•åˆ†')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # ç”¨æˆ¶æ´»èºåº¦åˆ†ä½ˆ\n",
    "        user_activity_data = user_activity.toPandas()\n",
    "        axes[1, 1].hist(user_activity_data['count'], bins=20, alpha=0.7)\n",
    "        axes[1, 1].set_title('ç”¨æˆ¶æ´»èºåº¦åˆ†ä½ˆ')\n",
    "        axes[1, 1].set_xlabel('è©•åˆ†æ•¸é‡')\n",
    "        axes[1, 1].set_ylabel('ç”¨æˆ¶æ•¸é‡')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return popular_products, category_preference\n",
    "    \n",
    "    def build_recommendation_model(self):\n",
    "        \"\"\"\n",
    "        å»ºç«‹æ¨è–¦æ¨¡å‹\n",
    "        \"\"\"\n",
    "        print(\"\\n=== å»ºç«‹æ¨è–¦æ¨¡å‹ ===\")\n",
    "        \n",
    "        # åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦è³‡æ–™\n",
    "        train_df, test_df = self.ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
    "        \n",
    "        print(f\"è¨“ç·´è³‡æ–™: {train_df.count()} ç­†\")\n",
    "        print(f\"æ¸¬è©¦è³‡æ–™: {test_df.count()} ç­†\")\n",
    "        \n",
    "        # ä½¿ç”¨ ALS (Alternating Least Squares) å»ºç«‹æ¨¡å‹\n",
    "        als = ALS(\n",
    "            userCol=\"user_id\",\n",
    "            itemCol=\"product_id\",\n",
    "            ratingCol=\"rating\",\n",
    "            rank=50,  # æ½›åœ¨å› å­æ•¸é‡\n",
    "            maxIter=10,\n",
    "            regParam=0.01,\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\nè¨“ç·´ ALS æ¨¡å‹...\")\n",
    "        self.model = als.fit(train_df)\n",
    "        \n",
    "        # åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°æ¨¡å‹\n",
    "        predictions = self.model.transform(test_df)\n",
    "        \n",
    "        # è¨ˆç®— RMSE\n",
    "        evaluator = RegressionEvaluator(\n",
    "            metricName=\"rmse\",\n",
    "            labelCol=\"rating\",\n",
    "            predictionCol=\"prediction\"\n",
    "        )\n",
    "        \n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        print(f\"\\næ¨¡å‹ RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        # é¡¯ç¤ºä¸€äº›é æ¸¬çµæœ\n",
    "        print(\"\\né æ¸¬çµæœæ¨£æœ¬:\")\n",
    "        predictions.select(\"user_id\", \"product_id\", \"rating\", \"prediction\").show(10)\n",
    "        \n",
    "        return self.model, rmse\n",
    "    \n",
    "    def generate_recommendations(self, user_id, num_recommendations=10):\n",
    "        \"\"\"\n",
    "        ç‚ºç‰¹å®šç”¨æˆ¶ç”Ÿæˆæ¨è–¦\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== ç‚ºç”¨æˆ¶ {user_id} ç”Ÿæˆæ¨è–¦ ===\")\n",
    "        \n",
    "        # ç²å–ç”¨æˆ¶å·²è©•åˆ†çš„å•†å“\n",
    "        user_ratings = self.ratings_df.filter(col(\"user_id\") == user_id)\n",
    "        rated_products = user_ratings.select(\"product_id\").rdd.map(lambda x: x[0]).collect()\n",
    "        \n",
    "        print(f\"ç”¨æˆ¶ {user_id} å·²è©•åˆ†å•†å“æ•¸: {len(rated_products)}\")\n",
    "        \n",
    "        # é¡¯ç¤ºç”¨æˆ¶çš„è©•åˆ†æ­·å²\n",
    "        print(\"\\nç”¨æˆ¶è©•åˆ†æ­·å²:\")\n",
    "        user_history = user_ratings.join(self.products_df, \"product_id\") \\\n",
    "                                  .select(\"product_name\", \"category\", \"rating\") \\\n",
    "                                  .orderBy(col(\"rating\").desc())\n",
    "        user_history.show(10)\n",
    "        \n",
    "        # ç”Ÿæˆæ¨è–¦\n",
    "        user_df = self.spark.createDataFrame([(user_id,)], [\"user_id\"])\n",
    "        user_recommendations = self.model.recommendForUserSubset(user_df, num_recommendations)\n",
    "        \n",
    "        # è§£ææ¨è–¦çµæœ\n",
    "        recommendations = user_recommendations.select(\"user_id\", \"recommendations\").collect()[0]\n",
    "        \n",
    "        print(f\"\\nç‚ºç”¨æˆ¶ {user_id} æ¨è–¦çš„å•†å“:\")\n",
    "        \n",
    "        recommended_products = []\n",
    "        for rec in recommendations['recommendations']:\n",
    "            product_id = rec['product_id']\n",
    "            predicted_rating = rec['rating']\n",
    "            \n",
    "            # ç²å–å•†å“è³‡è¨Š\n",
    "            product_info = self.products_df.filter(col(\"product_id\") == product_id).collect()[0]\n",
    "            \n",
    "            recommended_products.append({\n",
    "                'product_id': product_id,\n",
    "                'product_name': product_info['product_name'],\n",
    "                'category': product_info['category'],\n",
    "                'price': product_info['price'],\n",
    "                'predicted_rating': predicted_rating\n",
    "            })\n",
    "        \n",
    "        # é¡¯ç¤ºæ¨è–¦çµæœ\n",
    "        rec_df = self.spark.createDataFrame(recommended_products)\n",
    "        rec_df.show(num_recommendations, truncate=False)\n",
    "        \n",
    "        return recommended_products\n",
    "    \n",
    "    def analyze_recommendations(self):\n",
    "        \"\"\"\n",
    "        åˆ†ææ¨è–¦ç³»çµ±çš„æ•´é«”è¡¨ç¾\n",
    "        \"\"\"\n",
    "        print(\"\\n=== æ¨è–¦ç³»çµ±åˆ†æ ===\")\n",
    "        \n",
    "        # ç‚ºå¤šå€‹ç”¨æˆ¶ç”Ÿæˆæ¨è–¦\n",
    "        sample_users = self.ratings_df.select(\"user_id\").distinct().limit(10).rdd.map(lambda x: x[0]).collect()\n",
    "        \n",
    "        print(f\"ç‚º {len(sample_users)} å€‹ç”¨æˆ¶ç”Ÿæˆæ¨è–¦...\")\n",
    "        \n",
    "        all_recommendations = []\n",
    "        for user_id in sample_users:\n",
    "            recommendations = self.generate_recommendations(user_id, 5)\n",
    "            all_recommendations.extend(recommendations)\n",
    "        \n",
    "        # åˆ†ææ¨è–¦çš„å¤šæ¨£æ€§\n",
    "        recommended_categories = defaultdict(int)\n",
    "        for rec in all_recommendations:\n",
    "            recommended_categories[rec['category']] += 1\n",
    "        \n",
    "        print(\"\\næ¨è–¦åˆ†é¡åˆ†ä½ˆ:\")\n",
    "        for category, count in sorted(recommended_categories.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {category}: {count} æ¬¡æ¨è–¦\")\n",
    "        \n",
    "        # åˆ†ææ¨è–¦çš„åƒ¹æ ¼åˆ†ä½ˆ\n",
    "        recommended_prices = [rec['price'] for rec in all_recommendations]\n",
    "        \n",
    "        print(\"\\næ¨è–¦åƒ¹æ ¼çµ±è¨ˆ:\")\n",
    "        print(f\"  å¹³å‡åƒ¹æ ¼: ${np.mean(recommended_prices):.2f}\")\n",
    "        print(f\"  åƒ¹æ ¼ä¸­ä½æ•¸: ${np.median(recommended_prices):.2f}\")\n",
    "        print(f\"  æœ€ä½åƒ¹æ ¼: ${np.min(recommended_prices):.2f}\")\n",
    "        print(f\"  æœ€é«˜åƒ¹æ ¼: ${np.max(recommended_prices):.2f}\")\n",
    "        \n",
    "        # è¦–è¦ºåŒ–æ¨è–¦åˆ†æ\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # æ¨è–¦åˆ†é¡åˆ†ä½ˆ\n",
    "        categories = list(recommended_categories.keys())\n",
    "        counts = list(recommended_categories.values())\n",
    "        \n",
    "        axes[0].bar(categories, counts)\n",
    "        axes[0].set_title('æ¨è–¦åˆ†é¡åˆ†ä½ˆ')\n",
    "        axes[0].set_xlabel('åˆ†é¡')\n",
    "        axes[0].set_ylabel('æ¨è–¦æ¬¡æ•¸')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # æ¨è–¦åƒ¹æ ¼åˆ†ä½ˆ\n",
    "        axes[1].hist(recommended_prices, bins=20, alpha=0.7)\n",
    "        axes[1].set_title('æ¨è–¦åƒ¹æ ¼åˆ†ä½ˆ')\n",
    "        axes[1].set_xlabel('åƒ¹æ ¼ ($)')\n",
    "        axes[1].set_ylabel('é »ç‡')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return all_recommendations\n",
    "    \n",
    "    def evaluate_recommendation_quality(self):\n",
    "        \"\"\"\n",
    "        è©•ä¼°æ¨è–¦ç³»çµ±å“è³ª\n",
    "        \"\"\"\n",
    "        print(\"\\n=== æ¨è–¦ç³»çµ±å“è³ªè©•ä¼° ===\")\n",
    "        \n",
    "        # è¨ˆç®—è¦†è“‹ç‡ï¼ˆæ¨è–¦ç³»çµ±èƒ½æ¨è–¦å¤šå°‘æ¯”ä¾‹çš„å•†å“ï¼‰\n",
    "        total_products = self.products_df.count()\n",
    "        \n",
    "        # ç‚ºæ‰€æœ‰ç”¨æˆ¶ç”Ÿæˆæ¨è–¦\n",
    "        all_users = self.ratings_df.select(\"user_id\").distinct()\n",
    "        all_recommendations = self.model.recommendForAllUsers(5)\n",
    "        \n",
    "        # æå–æ‰€æœ‰è¢«æ¨è–¦çš„å•†å“\n",
    "        recommended_products = all_recommendations.select(\n",
    "            explode(\"recommendations\").alias(\"recommendation\")\n",
    "        ).select(\n",
    "            col(\"recommendation.product_id\").alias(\"product_id\")\n",
    "        ).distinct()\n",
    "        \n",
    "        coverage = recommended_products.count() / total_products\n",
    "        print(f\"å•†å“è¦†è“‹ç‡: {coverage:.2%}\")\n",
    "        \n",
    "        # è¨ˆç®—æ¨è–¦çš„å¤šæ¨£æ€§\n",
    "        category_diversity = recommended_products.join(\n",
    "            self.products_df, \"product_id\"\n",
    "        ).select(\"category\").distinct().count()\n",
    "        \n",
    "        total_categories = self.products_df.select(\"category\").distinct().count()\n",
    "        diversity = category_diversity / total_categories\n",
    "        \n",
    "        print(f\"åˆ†é¡å¤šæ¨£æ€§: {diversity:.2%}\")\n",
    "        \n",
    "        # è¨ˆç®—æ–°ç©æ€§ï¼ˆæ¨è–¦ä¸å¤ªå—æ­¡è¿çš„å•†å“çš„èƒ½åŠ›ï¼‰\n",
    "        product_popularity = self.ratings_df.groupBy(\"product_id\").count().alias(\"popularity\")\n",
    "        \n",
    "        recommended_popularity = recommended_products.join(\n",
    "            product_popularity, \"product_id\"\n",
    "        ).select(\"count\")\n",
    "        \n",
    "        avg_recommended_popularity = recommended_popularity.agg(avg(\"count\")).collect()[0][0]\n",
    "        avg_overall_popularity = product_popularity.agg(avg(\"count\")).collect()[0][0]\n",
    "        \n",
    "        novelty = 1 - (avg_recommended_popularity / avg_overall_popularity)\n",
    "        print(f\"æ–°ç©æ€§æŒ‡æ¨™: {novelty:.2%}\")\n",
    "        \n",
    "        # ç”Ÿæˆå“è³ªå ±å‘Š\n",
    "        quality_metrics = {\n",
    "            'coverage': coverage,\n",
    "            'diversity': diversity,\n",
    "            'novelty': novelty\n",
    "        }\n",
    "        \n",
    "        print(\"\\nğŸ“Š æ¨è–¦ç³»çµ±å“è³ªç¸½çµ:\")\n",
    "        print(f\"  è¦†è“‹ç‡: {coverage:.2%} (æ¨è–¦äº† {coverage*100:.1f}% çš„å•†å“)\")\n",
    "        print(f\"  å¤šæ¨£æ€§: {diversity:.2%} (è¦†è“‹äº† {diversity*100:.1f}% çš„åˆ†é¡)\")\n",
    "        print(f\"  æ–°ç©æ€§: {novelty:.2%} (å‚¾å‘æ¨è–¦ {'è¼ƒä¸å—æ­¡è¿' if novelty > 0 else 'è¼ƒå—æ­¡è¿'} çš„å•†å“)\")\n",
    "        \n",
    "        # æ•´é«”å“è³ªè©•åˆ†\n",
    "        overall_quality = (coverage + diversity + abs(novelty)) / 3\n",
    "        print(f\"  æ•´é«”å“è³ªè©•åˆ†: {overall_quality:.2%}\")\n",
    "        \n",
    "        return quality_metrics\n",
    "\n",
    "# åŸ·è¡Œæ¨è–¦ç³»çµ±\n",
    "print(\"\\n=== å°ˆæ¡ˆä¸‰ï¼šæ¨è–¦ç³»çµ± ===\")\n",
    "\n",
    "recommendation_system = RecommendationSystem(spark)\n",
    "ratings_data, products_data, users_data = recommendation_system.generate_sample_data()\n",
    "\n",
    "# é¡¯ç¤ºæ¨£æœ¬è³‡æ–™\n",
    "print(\"\\nè©•åˆ†è³‡æ–™æ¨£æœ¬:\")\n",
    "ratings_data.show(10)\n",
    "\n",
    "print(\"\\nå•†å“è³‡æ–™æ¨£æœ¬:\")\n",
    "products_data.show(10)\n",
    "\n",
    "# åŸ·è¡Œæ¨è–¦ç³»çµ±åˆ†æ\n",
    "popular_products, category_preference = recommendation_system.exploratory_analysis()\n",
    "model, rmse = recommendation_system.build_recommendation_model()\n",
    "\n",
    "# ç‚ºå¹¾å€‹ç”¨æˆ¶ç”Ÿæˆæ¨è–¦\n",
    "sample_user_ids = [1, 10, 50, 100]\n",
    "for user_id in sample_user_ids:\n",
    "    recommendations = recommendation_system.generate_recommendations(user_id, 5)\n",
    "\n",
    "# åˆ†ææ¨è–¦ç³»çµ±æ•´é«”è¡¨ç¾\n",
    "all_recommendations = recommendation_system.analyze_recommendations()\n",
    "quality_metrics = recommendation_system.evaluate_recommendation_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¸½çµ\n",
    "\n",
    "æœ¬ç« é€šéä¸‰å€‹å¯¦æˆ°å°ˆæ¡ˆå±•ç¤ºäº† Spark åœ¨ä¸åŒé ˜åŸŸçš„æ‡‰ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦æˆ°å°ˆæ¡ˆç¸½çµ\n",
    "def projects_summary():\n",
    "    \"\"\"\n",
    "    å¯¦æˆ°å°ˆæ¡ˆç¸½çµ\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"å¯¦æˆ°å°ˆæ¡ˆç¸½çµ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    projects = {\n",
    "        \"å°ˆæ¡ˆä¸€ï¼šæ—¥èªŒåˆ†æç³»çµ±\": {\n",
    "            \"æ ¸å¿ƒæŠ€è¡“\": [\n",
    "                \"æ—¥èªŒè§£æå’Œè³‡æ–™æ¸…ç†\",\n",
    "                \"æ™‚é–“åºåˆ—åˆ†æ\",\n",
    "                \"ç•°å¸¸æª¢æ¸¬å’Œå®‰å…¨åˆ†æ\",\n",
    "                \"è³‡æ–™è¦–è¦ºåŒ–\"\n",
    "            ],\n",
    "            \"æ‡‰ç”¨å ´æ™¯\": [\n",
    "                \"ç¶²ç«™è¨ªå•æ—¥èªŒåˆ†æ\",\n",
    "                \"å®‰å…¨å¨è„…æª¢æ¸¬\",\n",
    "                \"ç³»çµ±æ€§èƒ½ç›£æ§\",\n",
    "                \"ç”¨æˆ¶è¡Œç‚ºåˆ†æ\"\n",
    "            ],\n",
    "            \"é—œéµæ”¶ç©«\": [\n",
    "                \"å­¸æœƒè™•ç†åŠçµæ§‹åŒ–è³‡æ–™\",\n",
    "                \"æŒæ¡ç•°å¸¸æª¢æ¸¬æŠ€è¡“\",\n",
    "                \"äº†è§£å®‰å…¨åˆ†ææ–¹æ³•\",\n",
    "                \"ç†Ÿæ‚‰æ™‚é–“åºåˆ—è™•ç†\"\n",
    "            ]\n",
    "        },\n",
    "        \"å°ˆæ¡ˆäºŒï¼šå³æ™‚ç›£æ§ç³»çµ±\": {\n",
    "            \"æ ¸å¿ƒæŠ€è¡“\": [\n",
    "                \"å³æ™‚è³‡æ–™è™•ç†\",\n",
    "                \"é–¾å€¼ç›£æ§å’Œè­¦å ±\",\n",
    "                \"è¶¨å‹¢åˆ†æ\",\n",
    "                \"å¥åº·åº¦è©•ä¼°\"\n",
    "            ],\n",
    "            \"æ‡‰ç”¨å ´æ™¯\": [\n",
    "                \"ç³»çµ±ç›£æ§\",\n",
    "                \"æ‡‰ç”¨ç¨‹å¼ç›£æ§\",\n",
    "                \"åŸºç¤è¨­æ–½ç›£æ§\",\n",
    "                \"æ¥­å‹™æŒ‡æ¨™ç›£æ§\"\n",
    "            ],\n",
    "            \"é—œéµæ”¶ç©«\": [\n",
    "                \"æŒæ¡å³æ™‚ç›£æ§æŠ€è¡“\",\n",
    "                \"å­¸æœƒè¨­è¨ˆè­¦å ±ç³»çµ±\",\n",
    "                \"äº†è§£è¶¨å‹¢åˆ†ææ–¹æ³•\",\n",
    "                \"ç†Ÿæ‚‰ç³»çµ±å¥åº·è©•ä¼°\"\n",
    "            ]\n",
    "        },\n",
    "        \"å°ˆæ¡ˆä¸‰ï¼šæ¨è–¦ç³»çµ±\": {\n",
    "            \"æ ¸å¿ƒæŠ€è¡“\": [\n",
    "                \"å”åŒéæ¿¾æ¼”ç®—æ³•\",\n",
    "                \"çŸ©é™£åˆ†è§£ (ALS)\",\n",
    "                \"æ¨è–¦å“è³ªè©•ä¼°\",\n",
    "                \"å€‹æ€§åŒ–æ¨è–¦\"\n",
    "            ],\n",
    "            \"æ‡‰ç”¨å ´æ™¯\": [\n",
    "                \"é›»å•†å•†å“æ¨è–¦\",\n",
    "                \"å…§å®¹æ¨è–¦\",\n",
    "                \"ç¤¾äº¤ç¶²çµ¡æ¨è–¦\",\n",
    "                \"å»£å‘Šæ¨è–¦\"\n",
    "            ],\n",
    "            \"é—œéµæ”¶ç©«\": [\n",
    "                \"æŒæ¡æ¨è–¦ç³»çµ±åŸç†\",\n",
    "                \"å­¸æœƒä½¿ç”¨ MLlib ALS\",\n",
    "                \"äº†è§£æ¨è–¦è©•ä¼°æŒ‡æ¨™\",\n",
    "                \"ç†Ÿæ‚‰å¤§è¦æ¨¡æ¨è–¦ç³»çµ±\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for project_name, details in projects.items():\n",
    "        print(f\"\\n{project_name}:\")\n",
    "        print(\"-\" * len(project_name))\n",
    "        \n",
    "        for category, items in details.items():\n",
    "            print(f\"\\n{category}:\")\n",
    "            for item in items:\n",
    "                print(f\"  â€¢ {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"æ•´é«”å­¸ç¿’æˆæœ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    achievements = {\n",
    "        \"æŠ€è¡“èƒ½åŠ›\": [\n",
    "            \"ç†Ÿç·´æŒæ¡ Spark æ ¸å¿ƒ API\",\n",
    "            \"å…·å‚™å¤§è³‡æ–™è™•ç†èƒ½åŠ›\",\n",
    "            \"æŒæ¡æ©Ÿå™¨å­¸ç¿’æ‡‰ç”¨\",\n",
    "            \"å…·å‚™ç³»çµ±è¨­è¨ˆæ€ç¶­\"\n",
    "        ],\n",
    "        \"å¯¦æˆ°ç¶“é©—\": [\n",
    "            \"å®Œæˆç«¯åˆ°ç«¯å°ˆæ¡ˆé–‹ç™¼\",\n",
    "            \"è§£æ±ºå¯¦éš›æ¥­å‹™å•é¡Œ\",\n",
    "            \"æŒæ¡æ€§èƒ½å„ªåŒ–æŠ€å·§\",\n",
    "            \"å…·å‚™å•é¡Œè¨ºæ–·èƒ½åŠ›\"\n",
    "        ],\n",
    "        \"é ˜åŸŸçŸ¥è­˜\": [\n",
    "            \"æ—¥èªŒåˆ†æå’Œå®‰å…¨ç›£æ§\",\n",
    "            \"ç³»çµ±ç›£æ§å’Œé‹ç¶­\",\n",
    "            \"æ¨è–¦ç³»çµ±å’Œæ©Ÿå™¨å­¸ç¿’\",\n",
    "            \"è³‡æ–™ç§‘å­¸å’Œåˆ†æ\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in achievements.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  âœ“ {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ä¸‹ä¸€æ­¥å»ºè­°\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    next_steps = [\n",
    "        \"åœ¨å¯¦éš›é …ç›®ä¸­æ‡‰ç”¨æ‰€å­¸æŠ€èƒ½\",\n",
    "        \"æ·±å…¥å­¸ç¿’ç‰¹å®šé ˜åŸŸçš„é€²éšæŠ€è¡“\",\n",
    "        \"æ¢ç´¢ Spark çš„æ–°ç‰¹æ€§å’Œæ›´æ–°\",\n",
    "        \"å­¸ç¿’ç›¸é—œçš„å¤§è³‡æ–™ç”Ÿæ…‹ç³»çµ±å·¥å…·\",\n",
    "        \"åƒèˆ‡é–‹æºé …ç›®ï¼Œè²¢ç»ç¤¾å€\",\n",
    "        \"æŒçºŒé—œæ³¨æŠ€è¡“ç™¼å±•è¶¨å‹¢\"\n",
    "    ]\n",
    "    \n",
    "    for i, step in enumerate(next_steps, 1):\n",
    "        print(f\"  {i}. {step}\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ æ­å–œæ‚¨å®Œæˆäº† Spark 101 çš„æ‰€æœ‰èª²ç¨‹ï¼\")\n",
    "    print(\"æ‚¨ç¾åœ¨å…·å‚™äº†ä½¿ç”¨ Apache Spark è§£æ±ºå¯¦éš›å•é¡Œçš„èƒ½åŠ›ã€‚\")\n",
    "    print(\"ç¹¼çºŒå­¸ç¿’ï¼Œä¸æ–·å¯¦è¸ï¼Œæˆç‚ºå¤§è³‡æ–™è™•ç†å°ˆå®¶ï¼\")\n",
    "\n",
    "# åŸ·è¡Œç¸½çµ\n",
    "projects_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†è³‡æº\n",
    "print(\"\\næ¸…ç†è³‡æº...\")\n",
    "spark.stop()\n",
    "print(\"Spark æœƒè©±å·²çµæŸ\")\n",
    "print(\"\\næ„Ÿè¬æ‚¨å®Œæˆæ‰€æœ‰å¯¦æˆ°å°ˆæ¡ˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}