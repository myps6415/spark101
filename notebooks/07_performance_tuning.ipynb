{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第七章：性能調優\n",
    "\n",
    "Apache Spark 的性能調優是確保應用程式高效運行的關鍵。本章將介紹各種調優技術和最佳實踐。\n",
    "\n",
    "## 學習目標\n",
    "- 理解 Spark 的執行模型和性能瓶頸\n",
    "- 學習記憶體管理和垃圾回收調優\n",
    "- 掌握分區策略和數據傾斜處理\n",
    "- 了解快取和持久化最佳實踐\n",
    "- 學習 SQL 查詢優化技術\n",
    "- 掌握監控和診斷工具的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入必要的庫\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 建立 SparkSession（使用調優後的配置）\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Performance Tuning\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 設定日誌級別\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark 版本: {spark.version}\")\n",
    "print(f\"可用核心數: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"默認分區數: {spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 理解 Spark 執行模型\n",
    "\n",
    "了解 Spark 的基本執行模型對於性能調優至關重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 Spark 配置\n",
    "def show_spark_config():\n",
    "    \"\"\"\n",
    "    顯示重要的 Spark 配置\n",
    "    \"\"\"\n",
    "    important_configs = [\n",
    "        'spark.sql.adaptive.enabled',\n",
    "        'spark.sql.adaptive.coalescePartitions.enabled',\n",
    "        'spark.sql.adaptive.skewJoin.enabled',\n",
    "        'spark.sql.shuffle.partitions',\n",
    "        'spark.sql.execution.arrow.pyspark.enabled',\n",
    "        'spark.serializer',\n",
    "        'spark.sql.adaptive.advisoryPartitionSizeInBytes',\n",
    "        'spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold'\n",
    "    ]\n",
    "    \n",
    "    print(\"重要的 Spark 配置:\")\n",
    "    for config in important_configs:\n",
    "        try:\n",
    "            value = spark.conf.get(config)\n",
    "            print(f\"{config}: {value}\")\n",
    "        except:\n",
    "            print(f\"{config}: 未設定\")\n",
    "\n",
    "show_spark_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立測試資料集\n",
    "def create_test_data():\n",
    "    \"\"\"\n",
    "    建立用於性能測試的資料集\n",
    "    \"\"\"\n",
    "    print(\"建立測試資料集...\")\n",
    "    \n",
    "    # 建立大型資料集（模擬真實場景）\n",
    "    num_records = 1000000\n",
    "    \n",
    "    # 建立銷售資料\n",
    "    sales_data = []\n",
    "    categories = ['Electronics', 'Clothing', 'Books', 'Sports', 'Home']\n",
    "    regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        sales_data.append((\n",
    "            i,\n",
    "            f\"customer_{i % 50000}\",\n",
    "            random.choice(categories),\n",
    "            random.choice(regions),\n",
    "            random.uniform(10, 1000),\n",
    "            random.randint(1, 10),\n",
    "            datetime.now() - timedelta(days=random.randint(0, 365))\n",
    "        ))\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"transaction_id\", IntegerType(), True),\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"category\", StringType(), True),\n",
    "        StructField(\"region\", StringType(), True),\n",
    "        StructField(\"amount\", DoubleType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"date\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    sales_df = spark.createDataFrame(sales_data, schema)\n",
    "    \n",
    "    print(f\"建立了 {sales_df.count()} 筆銷售記錄\")\n",
    "    print(f\"資料分區數: {sales_df.rdd.getNumPartitions()}\")\n",
    "    \n",
    "    return sales_df\n",
    "\n",
    "# 建立測試資料\n",
    "sales_df = create_test_data()\n",
    "sales_df.show(10)\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 分區策略優化\n",
    "\n",
    "正確的分區策略是 Spark 性能調優的基礎。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分區分析\n",
    "def analyze_partitions(df, name):\n",
    "    \"\"\"\n",
    "    分析 DataFrame 的分區情況\n",
    "    \"\"\"\n",
    "    num_partitions = df.rdd.getNumPartitions()\n",
    "    partition_sizes = df.rdd.mapPartitions(lambda x: [sum(1 for _ in x)]).collect()\n",
    "    \n",
    "    print(f\"\\n{name} 分區分析:\")\n",
    "    print(f\"分區數: {num_partitions}\")\n",
    "    print(f\"每個分區大小: {partition_sizes}\")\n",
    "    print(f\"最大分區大小: {max(partition_sizes)}\")\n",
    "    print(f\"最小分區大小: {min(partition_sizes)}\")\n",
    "    print(f\"平均分區大小: {sum(partition_sizes) / len(partition_sizes):.2f}\")\n",
    "    \n",
    "    # 視覺化分區分佈\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(range(len(partition_sizes)), partition_sizes)\n",
    "    plt.xlabel('分區編號')\n",
    "    plt.ylabel('記錄數')\n",
    "    plt.title(f'{name} 分區大小分佈')\n",
    "    plt.show()\n",
    "\n",
    "analyze_partitions(sales_df, \"原始資料\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新分區策略\n",
    "def test_repartitioning_strategies():\n",
    "    \"\"\"\n",
    "    測試不同的重新分區策略\n",
    "    \"\"\"\n",
    "    print(\"測試不同的重新分區策略...\")\n",
    "    \n",
    "    # 1. 按數量重新分區\n",
    "    start_time = time.time()\n",
    "    repartitioned_df = sales_df.repartition(8)\n",
    "    count1 = repartitioned_df.count()\n",
    "    time1 = time.time() - start_time\n",
    "    \n",
    "    analyze_partitions(repartitioned_df, \"按數量重新分區 (8)\")\n",
    "    print(f\"執行時間: {time1:.2f} 秒\")\n",
    "    \n",
    "    # 2. 按欄位重新分區\n",
    "    start_time = time.time()\n",
    "    partitioned_by_region = sales_df.repartition(\"region\")\n",
    "    count2 = partitioned_by_region.count()\n",
    "    time2 = time.time() - start_time\n",
    "    \n",
    "    analyze_partitions(partitioned_by_region, \"按地區重新分區\")\n",
    "    print(f\"執行時間: {time2:.2f} 秒\")\n",
    "    \n",
    "    # 3. 合併分區\n",
    "    start_time = time.time()\n",
    "    coalesced_df = sales_df.coalesce(4)\n",
    "    count3 = coalesced_df.count()\n",
    "    time3 = time.time() - start_time\n",
    "    \n",
    "    analyze_partitions(coalesced_df, \"合併分區 (4)\")\n",
    "    print(f\"執行時間: {time3:.2f} 秒\")\n",
    "    \n",
    "    return repartitioned_df, partitioned_by_region, coalesced_df\n",
    "\n",
    "repartitioned_df, partitioned_by_region, coalesced_df = test_repartitioning_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 快取和持久化優化\n",
    "\n",
    "正確使用快取可以顯著提升重複計算的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快取策略比較\n",
    "def test_caching_strategies():\n",
    "    \"\"\"\n",
    "    測試不同的快取策略\n",
    "    \"\"\"\n",
    "    print(\"測試不同的快取策略...\")\n",
    "    \n",
    "    # 建立一個需要複雜計算的 DataFrame\n",
    "    complex_df = sales_df.filter(col(\"amount\") > 100) \\\n",
    "                        .withColumn(\"total_value\", col(\"amount\") * col(\"quantity\")) \\\n",
    "                        .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "                        .withColumn(\"year\", year(col(\"date\")))\n",
    "    \n",
    "    # 測試不同的存儲級別\n",
    "    storage_levels = [\n",
    "        (StorageLevel.MEMORY_ONLY, \"MEMORY_ONLY\"),\n",
    "        (StorageLevel.MEMORY_AND_DISK, \"MEMORY_AND_DISK\"),\n",
    "        (StorageLevel.MEMORY_AND_DISK_SER, \"MEMORY_AND_DISK_SER\"),\n",
    "        (StorageLevel.DISK_ONLY, \"DISK_ONLY\")\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for storage_level, name in storage_levels:\n",
    "        print(f\"\\n測試 {name} 存儲級別:\")\n",
    "        \n",
    "        # 清除之前的快取\n",
    "        spark.catalog.clearCache()\n",
    "        \n",
    "        # 設定快取\n",
    "        cached_df = complex_df.persist(storage_level)\n",
    "        \n",
    "        # 第一次執行（觸發快取）\n",
    "        start_time = time.time()\n",
    "        count1 = cached_df.count()\n",
    "        first_run_time = time.time() - start_time\n",
    "        \n",
    "        # 第二次執行（使用快取）\n",
    "        start_time = time.time()\n",
    "        count2 = cached_df.count()\n",
    "        second_run_time = time.time() - start_time\n",
    "        \n",
    "        # 第三次執行（確認快取效果）\n",
    "        start_time = time.time()\n",
    "        avg_amount = cached_df.agg(avg(\"amount\")).collect()[0][0]\n",
    "        third_run_time = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'storage_level': name,\n",
    "            'first_run': first_run_time,\n",
    "            'second_run': second_run_time,\n",
    "            'third_run': third_run_time,\n",
    "            'speedup': first_run_time / second_run_time\n",
    "        })\n",
    "        \n",
    "        print(f\"第一次執行: {first_run_time:.2f} 秒\")\n",
    "        print(f\"第二次執行: {second_run_time:.2f} 秒\")\n",
    "        print(f\"第三次執行: {third_run_time:.2f} 秒\")\n",
    "        print(f\"加速比: {first_run_time / second_run_time:.2f}x\")\n",
    "    \n",
    "    # 視覺化結果\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 執行時間比較\n",
    "    x = np.arange(len(results_df))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax1.bar(x - width, results_df['first_run'], width, label='第一次執行', alpha=0.8)\n",
    "    ax1.bar(x, results_df['second_run'], width, label='第二次執行', alpha=0.8)\n",
    "    ax1.bar(x + width, results_df['third_run'], width, label='第三次執行', alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('存儲級別')\n",
    "    ax1.set_ylabel('執行時間 (秒)')\n",
    "    ax1.set_title('不同存儲級別的執行時間比較')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(results_df['storage_level'], rotation=45)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 加速比比較\n",
    "    ax2.bar(results_df['storage_level'], results_df['speedup'], color='green', alpha=0.7)\n",
    "    ax2.set_xlabel('存儲級別')\n",
    "    ax2.set_ylabel('加速比')\n",
    "    ax2.set_title('快取加速比')\n",
    "    ax2.set_xticklabels(results_df['storage_level'], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "cache_results = test_caching_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Join 優化\n",
    "\n",
    "Join 操作是 Spark 中最消耗資源的操作之一，需要特別注意優化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立用於 Join 測試的資料\n",
    "def create_join_test_data():\n",
    "    \"\"\"\n",
    "    建立用於 Join 測試的資料集\n",
    "    \"\"\"\n",
    "    # 客戶資料（小表）\n",
    "    customer_data = []\n",
    "    for i in range(50000):\n",
    "        customer_data.append((\n",
    "            f\"customer_{i}\",\n",
    "            f\"Customer {i}\",\n",
    "            random.choice(['Premium', 'Standard', 'Basic']),\n",
    "            random.choice(['North', 'South', 'East', 'West', 'Central'])\n",
    "        ))\n",
    "    \n",
    "    customer_schema = StructType([\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"customer_name\", StringType(), True),\n",
    "        StructField(\"tier\", StringType(), True),\n",
    "        StructField(\"region\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    customers_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "    \n",
    "    # 產品資料（中等大小表）\n",
    "    product_data = []\n",
    "    categories = ['Electronics', 'Clothing', 'Books', 'Sports', 'Home']\n",
    "    for category in categories:\n",
    "        for i in range(1000):\n",
    "            product_data.append((\n",
    "                f\"product_{category}_{i}\",\n",
    "                f\"Product {i} in {category}\",\n",
    "                category,\n",
    "                random.uniform(10, 500)\n",
    "            ))\n",
    "    \n",
    "    product_schema = StructType([\n",
    "        StructField(\"product_id\", StringType(), True),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"category\", StringType(), True),\n",
    "        StructField(\"price\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    products_df = spark.createDataFrame(product_data, product_schema)\n",
    "    \n",
    "    return customers_df, products_df\n",
    "\n",
    "customers_df, products_df = create_join_test_data()\n",
    "\n",
    "print(f\"客戶資料: {customers_df.count()} 筆\")\n",
    "print(f\"產品資料: {products_df.count()} 筆\")\n",
    "\n",
    "customers_df.show(5)\n",
    "products_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試不同的 Join 策略\n",
    "def test_join_strategies():\n",
    "    \"\"\"\n",
    "    測試不同的 Join 策略和優化技術\n",
    "    \"\"\"\n",
    "    print(\"測試不同的 Join 策略...\")\n",
    "    \n",
    "    # 1. 普通 Join（未優化）\n",
    "    print(\"\\n1. 普通 Join:\")\n",
    "    start_time = time.time()\n",
    "    normal_join = sales_df.join(customers_df, \"customer_id\", \"inner\")\n",
    "    count1 = normal_join.count()\n",
    "    time1 = time.time() - start_time\n",
    "    print(f\"結果數量: {count1}\")\n",
    "    print(f\"執行時間: {time1:.2f} 秒\")\n",
    "    \n",
    "    # 2. 廣播 Join（小表優化）\n",
    "    print(\"\\n2. 廣播 Join:\")\n",
    "    start_time = time.time()\n",
    "    broadcast_join = sales_df.join(broadcast(customers_df), \"customer_id\", \"inner\")\n",
    "    count2 = broadcast_join.count()\n",
    "    time2 = time.time() - start_time\n",
    "    print(f\"結果數量: {count2}\")\n",
    "    print(f\"執行時間: {time2:.2f} 秒\")\n",
    "    print(f\"加速比: {time1 / time2:.2f}x\")\n",
    "    \n",
    "    # 3. 預分區 Join\n",
    "    print(\"\\n3. 預分區 Join:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 按相同的 key 進行分區\n",
    "    partitioned_sales = sales_df.repartition(\"customer_id\")\n",
    "    partitioned_customers = customers_df.repartition(\"customer_id\")\n",
    "    \n",
    "    partitioned_join = partitioned_sales.join(partitioned_customers, \"customer_id\", \"inner\")\n",
    "    count3 = partitioned_join.count()\n",
    "    time3 = time.time() - start_time\n",
    "    print(f\"結果數量: {count3}\")\n",
    "    print(f\"執行時間: {time3:.2f} 秒\")\n",
    "    print(f\"相對普通 Join 加速比: {time1 / time3:.2f}x\")\n",
    "    \n",
    "    # 4. 使用 Hint 的 Join\n",
    "    print(\"\\n4. 使用 Hint 的 Join:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 使用 SQL hint\n",
    "    sales_df.createOrReplaceTempView(\"sales\")\n",
    "    customers_df.createOrReplaceTempView(\"customers\")\n",
    "    \n",
    "    hint_join = spark.sql(\"\"\"\n",
    "        SELECT /*+ BROADCAST(c) */ s.*, c.customer_name, c.tier\n",
    "        FROM sales s\n",
    "        JOIN customers c ON s.customer_id = c.customer_id\n",
    "    \"\"\")\n",
    "    \n",
    "    count4 = hint_join.count()\n",
    "    time4 = time.time() - start_time\n",
    "    print(f\"結果數量: {count4}\")\n",
    "    print(f\"執行時間: {time4:.2f} 秒\")\n",
    "    print(f\"相對普通 Join 加速比: {time1 / time4:.2f}x\")\n",
    "    \n",
    "    # 結果比較\n",
    "    join_results = pd.DataFrame({\n",
    "        'Join Type': ['Normal', 'Broadcast', 'Pre-partitioned', 'With Hint'],\n",
    "        'Time (seconds)': [time1, time2, time3, time4],\n",
    "        'Count': [count1, count2, count3, count4]\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(join_results['Join Type'], join_results['Time (seconds)'], color=['red', 'green', 'blue', 'orange'])\n",
    "    plt.xlabel('Join 類型')\n",
    "    plt.ylabel('執行時間 (秒)')\n",
    "    plt.title('不同 Join 策略的性能比較')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 在柱狀圖上顯示數值\n",
    "    for i, v in enumerate(join_results['Time (seconds)']):\n",
    "        plt.text(i, v + 0.01, f'{v:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return join_results\n",
    "\n",
    "join_results = test_join_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 數據傾斜處理\n",
    "\n",
    "數據傾斜是 Spark 性能問題的常見原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬數據傾斜情況\n",
    "def create_skewed_data():\n",
    "    \"\"\"\n",
    "    建立有數據傾斜的資料集\n",
    "    \"\"\"\n",
    "    print(\"建立有數據傾斜的資料集...\")\n",
    "    \n",
    "    skewed_data = []\n",
    "    \n",
    "    # 創建嚴重傾斜的資料：80% 的資料都屬於一個 key\n",
    "    num_records = 100000\n",
    "    \n",
    "    # 80% 的資料使用 \"popular_key\"\n",
    "    for i in range(int(num_records * 0.8)):\n",
    "        skewed_data.append((\n",
    "            \"popular_key\",\n",
    "            f\"value_{i}\",\n",
    "            random.randint(1, 100)\n",
    "        ))\n",
    "    \n",
    "    # 20% 的資料平均分佈在其他 key 上\n",
    "    other_keys = [f\"key_{i}\" for i in range(10)]\n",
    "    for i in range(int(num_records * 0.2)):\n",
    "        skewed_data.append((\n",
    "            random.choice(other_keys),\n",
    "            f\"value_{i}\",\n",
    "            random.randint(1, 100)\n",
    "        ))\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"key\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True),\n",
    "        StructField(\"amount\", IntegerType(), True)\n",
    "    ])\n",
    "    \n",
    "    skewed_df = spark.createDataFrame(skewed_data, schema)\n",
    "    \n",
    "    # 分析數據傾斜程度\n",
    "    key_distribution = skewed_df.groupBy(\"key\").count().orderBy(col(\"count\").desc())\n",
    "    print(\"\\nKey 分佈情況:\")\n",
    "    key_distribution.show()\n",
    "    \n",
    "    # 視覺化數據傾斜\n",
    "    key_dist_pandas = key_distribution.toPandas()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(key_dist_pandas['key'], key_dist_pandas['count'])\n",
    "    plt.xlabel('Key')\n",
    "    plt.ylabel('記錄數')\n",
    "    plt.title('數據傾斜情況')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    return skewed_df\n",
    "\n",
    "skewed_df = create_skewed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 數據傾斜處理技術\n",
    "def handle_data_skew():\n",
    "    \"\"\"\n",
    "    示範處理數據傾斜的技術\n",
    "    \"\"\"\n",
    "    print(\"測試數據傾斜處理技術...\")\n",
    "    \n",
    "    # 1. 直接聚合（有傾斜問題）\n",
    "    print(\"\\n1. 直接聚合:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    direct_agg = skewed_df.groupBy(\"key\").agg(\n",
    "        sum(\"amount\").alias(\"total_amount\"),\n",
    "        count(\"*\").alias(\"count\")\n",
    "    )\n",
    "    \n",
    "    count1 = direct_agg.count()\n",
    "    time1 = time.time() - start_time\n",
    "    print(f\"結果數量: {count1}\")\n",
    "    print(f\"執行時間: {time1:.2f} 秒\")\n",
    "    \n",
    "    # 2. 加鹽技術（Salt）\n",
    "    print(\"\\n2. 加鹽技術:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 第一步：加鹽並進行初步聚合\n",
    "    salt_factor = 10\n",
    "    salted_df = skewed_df.withColumn(\"salt\", (rand() * salt_factor).cast(IntegerType())) \\\n",
    "                         .withColumn(\"salted_key\", concat(col(\"key\"), lit(\"_\"), col(\"salt\")))\n",
    "    \n",
    "    # 第一次聚合\n",
    "    first_agg = salted_df.groupBy(\"salted_key\", \"key\").agg(\n",
    "        sum(\"amount\").alias(\"partial_sum\"),\n",
    "        count(\"*\").alias(\"partial_count\")\n",
    "    )\n",
    "    \n",
    "    # 第二次聚合\n",
    "    final_agg = first_agg.groupBy(\"key\").agg(\n",
    "        sum(\"partial_sum\").alias(\"total_amount\"),\n",
    "        sum(\"partial_count\").alias(\"count\")\n",
    "    )\n",
    "    \n",
    "    count2 = final_agg.count()\n",
    "    time2 = time.time() - start_time\n",
    "    print(f\"結果數量: {count2}\")\n",
    "    print(f\"執行時間: {time2:.2f} 秒\")\n",
    "    print(f\"加速比: {time1 / time2:.2f}x\")\n",
    "    \n",
    "    # 3. 兩階段聚合\n",
    "    print(\"\\n3. 兩階段聚合:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 使用 AQE（Adaptive Query Execution）\n",
    "    spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "    \n",
    "    # 先進行局部聚合\n",
    "    local_agg = skewed_df.groupBy(\"key\").agg(\n",
    "        sum(\"amount\").alias(\"total_amount\"),\n",
    "        count(\"*\").alias(\"count\")\n",
    "    )\n",
    "    \n",
    "    count3 = local_agg.count()\n",
    "    time3 = time.time() - start_time\n",
    "    print(f\"結果數量: {count3}\")\n",
    "    print(f\"執行時間: {time3:.2f} 秒\")\n",
    "    print(f\"相對直接聚合加速比: {time1 / time3:.2f}x\")\n",
    "    \n",
    "    # 驗證結果一致性\n",
    "    print(\"\\n驗證結果一致性:\")\n",
    "    result1 = direct_agg.orderBy(\"key\").collect()\n",
    "    result2 = final_agg.orderBy(\"key\").collect()\n",
    "    \n",
    "    print(f\"直接聚合結果: {result1[:3]}\")\n",
    "    print(f\"加鹽聚合結果: {result2[:3]}\")\n",
    "    \n",
    "    # 結果比較\n",
    "    skew_results = pd.DataFrame({\n",
    "        'Method': ['Direct Aggregation', 'Salt Technique', 'Two-Stage Aggregation'],\n",
    "        'Time (seconds)': [time1, time2, time3]\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(skew_results['Method'], skew_results['Time (seconds)'], color=['red', 'green', 'blue'])\n",
    "    plt.xlabel('處理方法')\n",
    "    plt.ylabel('執行時間 (秒)')\n",
    "    plt.title('數據傾斜處理方法性能比較')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for i, v in enumerate(skew_results['Time (seconds)']):\n",
    "        plt.text(i, v + 0.01, f'{v:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return skew_results\n",
    "\n",
    "skew_results = handle_data_skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SQL 查詢優化\n",
    "\n",
    "使用 Catalyst 優化器和 SQL 技巧來提升查詢性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 查詢優化技術\n",
    "def sql_optimization_techniques():\n",
    "    \"\"\"\n",
    "    示範 SQL 查詢優化技術\n",
    "    \"\"\"\n",
    "    print(\"SQL 查詢優化技術...\")\n",
    "    \n",
    "    # 建立臨時視圖\n",
    "    sales_df.createOrReplaceTempView(\"sales\")\n",
    "    customers_df.createOrReplaceTempView(\"customers\")\n",
    "    \n",
    "    # 1. 未優化的查詢\n",
    "    print(\"\\n1. 未優化的查詢:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    unoptimized_query = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            c.tier,\n",
    "            s.category,\n",
    "            SUM(s.amount) as total_sales,\n",
    "            COUNT(*) as transaction_count\n",
    "        FROM sales s\n",
    "        JOIN customers c ON s.customer_id = c.customer_id\n",
    "        WHERE s.amount > 50\n",
    "        GROUP BY c.tier, s.category\n",
    "        ORDER BY total_sales DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    result1 = unoptimized_query.collect()\n",
    "    time1 = time.time() - start_time\n",
    "    print(f\"結果數量: {len(result1)}\")\n",
    "    print(f\"執行時間: {time1:.2f} 秒\")\n",
    "    \n",
    "    # 2. 使用 BROADCAST hint 優化\n",
    "    print(\"\\n2. 使用 BROADCAST hint:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    optimized_query = spark.sql(\"\"\"\n",
    "        SELECT /*+ BROADCAST(c) */\n",
    "            c.tier,\n",
    "            s.category,\n",
    "            SUM(s.amount) as total_sales,\n",
    "            COUNT(*) as transaction_count\n",
    "        FROM sales s\n",
    "        JOIN customers c ON s.customer_id = c.customer_id\n",
    "        WHERE s.amount > 50\n",
    "        GROUP BY c.tier, s.category\n",
    "        ORDER BY total_sales DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    result2 = optimized_query.collect()\n",
    "    time2 = time.time() - start_time\n",
    "    print(f\"結果數量: {len(result2)}\")\n",
    "    print(f\"執行時間: {time2:.2f} 秒\")\n",
    "    print(f\"加速比: {time1 / time2:.2f}x\")\n",
    "    \n",
    "    # 3. 使用 CTE（Common Table Expression）優化\n",
    "    print(\"\\n3. 使用 CTE 優化:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    cte_query = spark.sql(\"\"\"\n",
    "        WITH filtered_sales AS (\n",
    "            SELECT customer_id, category, amount\n",
    "            FROM sales\n",
    "            WHERE amount > 50\n",
    "        ),\n",
    "        sales_summary AS (\n",
    "            SELECT /*+ BROADCAST(c) */\n",
    "                c.tier,\n",
    "                fs.category,\n",
    "                SUM(fs.amount) as total_sales,\n",
    "                COUNT(*) as transaction_count\n",
    "            FROM filtered_sales fs\n",
    "            JOIN customers c ON fs.customer_id = c.customer_id\n",
    "            GROUP BY c.tier, fs.category\n",
    "        )\n",
    "        SELECT * FROM sales_summary\n",
    "        ORDER BY total_sales DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    result3 = cte_query.collect()\n",
    "    time3 = time.time() - start_time\n",
    "    print(f\"結果數量: {len(result3)}\")\n",
    "    print(f\"執行時間: {time3:.2f} 秒\")\n",
    "    print(f\"相對未優化查詢加速比: {time1 / time3:.2f}x\")\n",
    "    \n",
    "    # 4. 使用窗口函數優化\n",
    "    print(\"\\n4. 使用窗口函數:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    window_query = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            tier,\n",
    "            category,\n",
    "            total_sales,\n",
    "            transaction_count,\n",
    "            ROW_NUMBER() OVER (PARTITION BY tier ORDER BY total_sales DESC) as rank_in_tier\n",
    "        FROM (\n",
    "            SELECT /*+ BROADCAST(c) */\n",
    "                c.tier,\n",
    "                s.category,\n",
    "                SUM(s.amount) as total_sales,\n",
    "                COUNT(*) as transaction_count\n",
    "            FROM sales s\n",
    "            JOIN customers c ON s.customer_id = c.customer_id\n",
    "            WHERE s.amount > 50\n",
    "            GROUP BY c.tier, s.category\n",
    "        ) t\n",
    "        WHERE rank_in_tier <= 3\n",
    "    \"\"\")\n",
    "    \n",
    "    result4 = window_query.collect()\n",
    "    time4 = time.time() - start_time\n",
    "    print(f\"結果數量: {len(result4)}\")\n",
    "    print(f\"執行時間: {time4:.2f} 秒\")\n",
    "    \n",
    "    # 查看查詢計劃\n",
    "    print(\"\\n查看優化後的查詢計劃:\")\n",
    "    optimized_query.explain(mode=\"cost\")\n",
    "    \n",
    "    # 結果比較\n",
    "    sql_results = pd.DataFrame({\n",
    "        'Query Type': ['Unoptimized', 'With BROADCAST', 'With CTE', 'With Window Function'],\n",
    "        'Time (seconds)': [time1, time2, time3, time4],\n",
    "        'Result Count': [len(result1), len(result2), len(result3), len(result4)]\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(sql_results['Query Type'], sql_results['Time (seconds)'], color=['red', 'green', 'blue', 'orange'])\n",
    "    plt.xlabel('查詢類型')\n",
    "    plt.ylabel('執行時間 (秒)')\n",
    "    plt.title('SQL 查詢優化技術性能比較')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for i, v in enumerate(sql_results['Time (seconds)']):\n",
    "        plt.text(i, v + 0.01, f'{v:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return sql_results\n",
    "\n",
    "sql_results = sql_optimization_techniques()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 監控和診斷\n",
    "\n",
    "使用 Spark UI 和其他工具來監控和診斷性能問題。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 監控和診斷工具\n",
    "def monitoring_and_diagnostics():\n",
    "    \"\"\"\n",
    "    示範監控和診斷技術\n",
    "    \"\"\"\n",
    "    print(\"監控和診斷技術...\")\n",
    "    \n",
    "    # 1. 查看 Spark 配置\n",
    "    print(\"\\n1. 當前 Spark 配置:\")\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    all_configs = conf.getAll()\n",
    "    \n",
    "    important_configs = [\n",
    "        'spark.app.name',\n",
    "        'spark.sql.shuffle.partitions',\n",
    "        'spark.sql.adaptive.enabled',\n",
    "        'spark.sql.adaptive.coalescePartitions.enabled',\n",
    "        'spark.sql.adaptive.skewJoin.enabled',\n",
    "        'spark.serializer'\n",
    "    ]\n",
    "    \n",
    "    for config in important_configs:\n",
    "        for key, value in all_configs:\n",
    "            if key == config:\n",
    "                print(f\"{key}: {value}\")\n",
    "                break\n",
    "    \n",
    "    # 2. 查看應用程式資訊\n",
    "    print(\"\\n2. 應用程式資訊:\")\n",
    "    print(f\"應用程式 ID: {spark.sparkContext.applicationId}\")\n",
    "    print(f\"應用程式名稱: {spark.sparkContext.appName}\")\n",
    "    print(f\"Spark UI URL: {spark.sparkContext.uiWebUrl}\")\n",
    "    \n",
    "    # 3. 查看執行器資訊\n",
    "    print(\"\\n3. 執行器資訊:\")\n",
    "    status = spark.sparkContext.statusTracker()\n",
    "    executor_infos = status.getExecutorInfos()\n",
    "    \n",
    "    for executor in executor_infos:\n",
    "        print(f\"執行器 {executor.executorId}:\")\n",
    "        print(f\"  主機: {executor.host}\")\n",
    "        print(f\"  核心數: {executor.totalCores}\")\n",
    "        print(f\"  最大記憶體: {executor.maxMemory / 1024 / 1024:.2f} MB\")\n",
    "        print(f\"  已用記憶體: {executor.memoryUsed / 1024 / 1024:.2f} MB\")\n",
    "        print(f\"  活躍任務數: {executor.activeTasks}\")\n",
    "        print(f\"  已完成任務數: {executor.completedTasks}\")\n",
    "        print(f\"  失敗任務數: {executor.failedTasks}\")\n",
    "        print()\n",
    "    \n",
    "    # 4. 記憶體使用監控\n",
    "    print(\"\\n4. 記憶體使用監控:\")\n",
    "    \n",
    "    # 執行一個需要記憶體的操作\n",
    "    large_df = sales_df.cache()\n",
    "    large_df.count()  # 觸發快取\n",
    "    \n",
    "    # 再次檢查記憶體使用\n",
    "    updated_executor_infos = status.getExecutorInfos()\n",
    "    for executor in updated_executor_infos:\n",
    "        if executor.executorId != 'driver':\n",
    "            memory_used_mb = executor.memoryUsed / 1024 / 1024\n",
    "            max_memory_mb = executor.maxMemory / 1024 / 1024\n",
    "            usage_percent = (memory_used_mb / max_memory_mb) * 100\n",
    "            print(f\"執行器 {executor.executorId}: {memory_used_mb:.2f} MB / {max_memory_mb:.2f} MB ({usage_percent:.1f}%)\")\n",
    "    \n",
    "    # 5. 任務執行統計\n",
    "    print(\"\\n5. 任務執行統計:\")\n",
    "    \n",
    "    # 執行一個複雜的操作來生成任務\n",
    "    complex_operation = sales_df.groupBy(\"region\", \"category\") \\\n",
    "                               .agg(sum(\"amount\").alias(\"total_sales\")) \\\n",
    "                               .orderBy(\"total_sales\", ascending=False)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = complex_operation.collect()\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"操作執行時間: {execution_time:.2f} 秒\")\n",
    "    print(f\"結果數量: {len(result)}\")\n",
    "    \n",
    "    # 6. 查看快取統計\n",
    "    print(\"\\n6. 快取統計:\")\n",
    "    cache_status = spark.sparkContext.statusTracker().getExecutorInfos()\n",
    "    for executor in cache_status:\n",
    "        if executor.executorId != 'driver':\n",
    "            print(f\"執行器 {executor.executorId}:\")\n",
    "            print(f\"  快取記憶體使用: {executor.memoryUsed / 1024 / 1024:.2f} MB\")\n",
    "            print(f\"  磁碟使用: {executor.diskUsed / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    return {\n",
    "        'execution_time': execution_time,\n",
    "        'result_count': len(result),\n",
    "        'executor_count': len(executor_infos)\n",
    "    }\n",
    "\n",
    "monitoring_stats = monitoring_and_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 性能調優最佳實踐\n",
    "\n",
    "總結性能調優的最佳實踐和建議。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能調優最佳實踐總結\n",
    "def performance_tuning_best_practices():\n",
    "    \"\"\"\n",
    "    總結性能調優的最佳實踐\n",
    "    \"\"\"\n",
    "    print(\"性能調優最佳實踐總結:\")\n",
    "    \n",
    "    best_practices = {\n",
    "        \"資料結構優化\": [\n",
    "            \"使用 Parquet 格式儲存資料\",\n",
    "            \"合理選擇資料類型（避免使用 String 代替數值類型）\",\n",
    "            \"使用列式儲存格式\",\n",
    "            \"壓縮資料以減少 I/O\"\n",
    "        ],\n",
    "        \"分區策略\": [\n",
    "            \"合理設定分區數量（通常每個分區 100-200MB）\",\n",
    "            \"使用 coalesce() 而非 repartition() 減少分區數\",\n",
    "            \"按查詢模式進行分區（如按日期分區）\",\n",
    "            \"避免小檔案問題\"\n",
    "        ],\n",
    "        \"快取策略\": [\n",
    "            \"對重複使用的 DataFrame 進行快取\",\n",
    "            \"選擇合適的儲存級別\",\n",
    "            \"及時清理不需要的快取\",\n",
    "            \"監控記憶體使用情況\"\n",
    "        ],\n",
    "        \"Join 優化\": [\n",
    "            \"使用 broadcast join 處理小表\",\n",
    "            \"預先按 join key 分區\",\n",
    "            \"使用 SQL hints 指導優化器\",\n",
    "            \"避免不必要的 shuffle 操作\"\n",
    "        ],\n",
    "        \"數據傾斜處理\": [\n",
    "            \"使用加鹽技術分散熱點資料\",\n",
    "            \"採用兩階段聚合\",\n",
    "            \"啟用 AQE 的傾斜處理\",\n",
    "            \"監控任務執行時間分佈\"\n",
    "        ],\n",
    "        \"配置調優\": [\n",
    "            \"啟用 Adaptive Query Execution (AQE)\",\n",
    "            \"使用 Kryo 序列化器\",\n",
    "            \"調整 shuffle 分區數\",\n",
    "            \"優化記憶體分配\"\n",
    "        ],\n",
    "        \"SQL 優化\": [\n",
    "            \"使用列剪裁和謂詞下推\",\n",
    "            \"避免使用 UDF，優先使用內建函數\",\n",
    "            \"合理使用 CTE 和子查詢\",\n",
    "            \"使用窗口函數替代複雜的 join\"\n",
    "        ],\n",
    "        \"監控和診斷\": [\n",
    "            \"定期檢查 Spark UI\",\n",
    "            \"監控記憶體和 CPU 使用情況\",\n",
    "            \"分析任務執行時間\",\n",
    "            \"記錄和分析慢查詢\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, practices in best_practices.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for i, practice in enumerate(practices, 1):\n",
    "            print(f\"  {i}. {practice}\")\n",
    "    \n",
    "    # 創建性能調優檢查清單\n",
    "    print(\"\\n性能調優檢查清單:\")\n",
    "    checklist = [\n",
    "        \"✓ 資料格式是否為 Parquet 或其他列式格式？\",\n",
    "        \"✓ 分區數是否合理（每個分區 100-200MB）？\",\n",
    "        \"✓ 是否對重複使用的 DataFrame 進行快取？\",\n",
    "        \"✓ Join 操作是否使用了 broadcast hint？\",\n",
    "        \"✓ 是否啟用了 AQE？\",\n",
    "        \"✓ 是否使用了 Kryo 序列化器？\",\n",
    "        \"✓ 是否存在數據傾斜問題？\",\n",
    "        \"✓ SQL 查詢是否使用了列剪裁？\",\n",
    "        \"✓ 是否監控了記憶體使用情況？\",\n",
    "        \"✓ 是否分析了任務執行時間？\"\n",
    "    ]\n",
    "    \n",
    "    for item in checklist:\n",
    "        print(f\"  {item}\")\n",
    "    \n",
    "    return best_practices\n",
    "\n",
    "best_practices = performance_tuning_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 實際案例分析\n",
    "\n",
    "通過一個實際的性能問題案例來展示調優過程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實際案例：慢查詢優化\n",
    "def case_study_slow_query_optimization():\n",
    "    \"\"\"\n",
    "    案例研究：慢查詢優化\n",
    "    \"\"\"\n",
    "    print(\"案例研究：慢查詢優化\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 場景：分析客戶的購買行為\n",
    "    print(\"\\n場景：分析客戶購買行為（原始慢查詢）\")\n",
    "    print(\"目標：找出每個地區的高價值客戶及其購買偏好\")\n",
    "    \n",
    "    # 問題查詢（未優化）\n",
    "    print(\"\\n1. 原始查詢（存在性能問題）：\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 模擬複雜的慢查詢\n",
    "    slow_query = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            c.region,\n",
    "            c.customer_id,\n",
    "            c.customer_name,\n",
    "            c.tier,\n",
    "            SUM(s.amount) as total_spent,\n",
    "            COUNT(s.transaction_id) as transaction_count,\n",
    "            COLLECT_LIST(s.category) as categories,\n",
    "            AVG(s.amount) as avg_transaction_amount\n",
    "        FROM sales s\n",
    "        JOIN customers c ON s.customer_id = c.customer_id\n",
    "        GROUP BY c.region, c.customer_id, c.customer_name, c.tier\n",
    "        HAVING SUM(s.amount) > 1000\n",
    "        ORDER BY c.region, total_spent DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    slow_result = slow_query.collect()\n",
    "    slow_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"執行時間: {slow_time:.2f} 秒\")\n",
    "    print(f\"結果數量: {len(slow_result)}\")\n",
    "    print(\"\\n執行計劃分析：\")\n",
    "    slow_query.explain()\n",
    "    \n",
    "    # 優化步驟\n",
    "    print(\"\\n2. 優化步驟：\")\n",
    "    print(\"   a. 使用 broadcast join\")\n",
    "    print(\"   b. 預先過濾資料\")\n",
    "    print(\"   c. 合理使用 CTE\")\n",
    "    print(\"   d. 最佳化聚合操作\")\n",
    "    \n",
    "    # 優化後的查詢\n",
    "    print(\"\\n3. 優化後的查詢：\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    optimized_query = spark.sql(\"\"\"\n",
    "        WITH customer_spending AS (\n",
    "            SELECT \n",
    "                customer_id,\n",
    "                SUM(amount) as total_spent,\n",
    "                COUNT(transaction_id) as transaction_count,\n",
    "                AVG(amount) as avg_transaction_amount,\n",
    "                COLLECT_LIST(category) as categories\n",
    "            FROM sales\n",
    "            GROUP BY customer_id\n",
    "            HAVING SUM(amount) > 1000\n",
    "        ),\n",
    "        high_value_customers AS (\n",
    "            SELECT /*+ BROADCAST(c) */\n",
    "                c.region,\n",
    "                c.customer_id,\n",
    "                c.customer_name,\n",
    "                c.tier,\n",
    "                cs.total_spent,\n",
    "                cs.transaction_count,\n",
    "                cs.categories,\n",
    "                cs.avg_transaction_amount\n",
    "            FROM customer_spending cs\n",
    "            JOIN customers c ON cs.customer_id = c.customer_id\n",
    "        )\n",
    "        SELECT *\n",
    "        FROM high_value_customers\n",
    "        ORDER BY region, total_spent DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    optimized_result = optimized_query.collect()\n",
    "    optimized_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"執行時間: {optimized_time:.2f} 秒\")\n",
    "    print(f\"結果數量: {len(optimized_result)}\")\n",
    "    print(f\"性能提升: {slow_time / optimized_time:.2f}x\")\n",
    "    \n",
    "    print(\"\\n優化後的執行計劃：\")\n",
    "    optimized_query.explain()\n",
    "    \n",
    "    # 進一步優化：使用快取\n",
    "    print(\"\\n4. 進一步優化：使用快取\")\n",
    "    \n",
    "    # 快取中間結果\n",
    "    customer_spending_cached = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            SUM(amount) as total_spent,\n",
    "            COUNT(transaction_id) as transaction_count,\n",
    "            AVG(amount) as avg_transaction_amount,\n",
    "            COLLECT_LIST(category) as categories\n",
    "        FROM sales\n",
    "        GROUP BY customer_id\n",
    "        HAVING SUM(amount) > 1000\n",
    "    \"\"\").cache()\n",
    "    \n",
    "    # 觸發快取\n",
    "    customer_spending_cached.count()\n",
    "    \n",
    "    customer_spending_cached.createOrReplaceTempView(\"customer_spending_cached\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cached_query = spark.sql(\"\"\"\n",
    "        SELECT /*+ BROADCAST(c) */\n",
    "            c.region,\n",
    "            c.customer_id,\n",
    "            c.customer_name,\n",
    "            c.tier,\n",
    "            cs.total_spent,\n",
    "            cs.transaction_count,\n",
    "            cs.categories,\n",
    "            cs.avg_transaction_amount\n",
    "        FROM customer_spending_cached cs\n",
    "        JOIN customers c ON cs.customer_id = c.customer_id\n",
    "        ORDER BY region, total_spent DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    cached_result = cached_query.collect()\n",
    "    cached_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"使用快取後執行時間: {cached_time:.2f} 秒\")\n",
    "    print(f\"相對原始查詢加速比: {slow_time / cached_time:.2f}x\")\n",
    "    \n",
    "    # 結果驗證\n",
    "    print(\"\\n5. 結果驗證：\")\n",
    "    print(f\"原始查詢結果數量: {len(slow_result)}\")\n",
    "    print(f\"優化查詢結果數量: {len(optimized_result)}\")\n",
    "    print(f\"快取查詢結果數量: {len(cached_result)}\")\n",
    "    \n",
    "    # 性能總結\n",
    "    performance_summary = pd.DataFrame({\n",
    "        'Query Type': ['Original (Slow)', 'Optimized', 'With Cache'],\n",
    "        'Execution Time': [slow_time, optimized_time, cached_time],\n",
    "        'Speedup': [1.0, slow_time/optimized_time, slow_time/cached_time]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n6. 性能總結：\")\n",
    "    print(performance_summary.to_string(index=False))\n",
    "    \n",
    "    # 視覺化性能比較\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # 執行時間比較\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(performance_summary['Query Type'], performance_summary['Execution Time'], \n",
    "            color=['red', 'green', 'blue'])\n",
    "    plt.xlabel('查詢類型')\n",
    "    plt.ylabel('執行時間 (秒)')\n",
    "    plt.title('查詢執行時間比較')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 加速比比較\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(performance_summary['Query Type'], performance_summary['Speedup'], \n",
    "            color=['red', 'green', 'blue'])\n",
    "    plt.xlabel('查詢類型')\n",
    "    plt.ylabel('加速比')\n",
    "    plt.title('相對原始查詢的加速比')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return performance_summary\n",
    "\n",
    "case_study_results = case_study_slow_query_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 總結\n",
    "\n",
    "本章涵蓋了 Spark 性能調優的各個方面，從基礎概念到實際應用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能調優總結\n",
    "def performance_tuning_summary():\n",
    "    \"\"\"\n",
    "    性能調優總結\n",
    "    \"\"\"\n",
    "    print(\"Spark 性能調優總結\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    summary_points = {\n",
    "        \"關鍵學習要點\": [\n",
    "            \"理解 Spark 的執行模型是調優的基礎\",\n",
    "            \"分區策略直接影響並行性能\",\n",
    "            \"適當的快取策略可以顯著提升重複計算性能\",\n",
    "            \"Join 優化是大資料處理的重點\",\n",
    "            \"數據傾斜是常見的性能瓶頸\",\n",
    "            \"SQL 查詢優化可以自動提升性能\",\n",
    "            \"監控和診斷是持續優化的基礎\"\n",
    "        ],\n",
    "        \"實用技巧\": [\n",
    "            \"使用 .explain() 分析查詢計劃\",\n",
    "            \"啟用 AQE 獲得自動優化\",\n",
    "            \"使用 broadcast() 優化小表 Join\",\n",
    "            \"採用加鹽技術處理數據傾斜\",\n",
    "            \"合理設定分區數（通常為核心數的 2-3 倍）\",\n",
    "            \"使用列式儲存格式（Parquet）\",\n",
    "            \"定期清理不需要的快取\"\n",
    "        ],\n",
    "        \"常見錯誤\": [\n",
    "            \"過度分區導致任務開銷過大\",\n",
    "            \"不適當的快取策略浪費記憶體\",\n",
    "            \"忽略數據傾斜問題\",\n",
    "            \"使用過多的 UDF 函數\",\n",
    "            \"不合理的 Join 順序\",\n",
    "            \"忽略資料本地性\",\n",
    "            \"不監控資源使用情況\"\n",
    "        ],\n",
    "        \"調優流程\": [\n",
    "            \"1. 監控和識別性能瓶頸\",\n",
    "            \"2. 分析查詢執行計劃\",\n",
    "            \"3. 識別具體問題類型\",\n",
    "            \"4. 應用相應的優化策略\",\n",
    "            \"5. 測試和驗證優化效果\",\n",
    "            \"6. 持續監控和調整\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, points in summary_points.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for point in points:\n",
    "            print(f\"  • {point}\")\n",
    "    \n",
    "    # 性能調優工具箱\n",
    "    print(\"\\n性能調優工具箱:\")\n",
    "    toolbox = {\n",
    "        \"內建工具\": [\n",
    "            \"Spark UI (http://localhost:4040)\",\n",
    "            \"History Server\",\n",
    "            \"SQL Tab\",\n",
    "            \"Storage Tab\",\n",
    "            \"Executors Tab\"\n",
    "        ],\n",
    "        \"程式化工具\": [\n",
    "            \"df.explain()\",\n",
    "            \"df.cache()\",\n",
    "            \"df.repartition()\",\n",
    "            \"broadcast()\",\n",
    "            \"spark.sql.functions.*\"\n",
    "        ],\n",
    "        \"配置選項\": [\n",
    "            \"spark.sql.adaptive.enabled\",\n",
    "            \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
    "            \"spark.sql.adaptive.skewJoin.enabled\",\n",
    "            \"spark.sql.shuffle.partitions\",\n",
    "            \"spark.serializer\"\n",
    "        ],\n",
    "        \"第三方工具\": [\n",
    "            \"Dr. Elephant (LinkedIn)\",\n",
    "            \"Sparklens (Qubole)\",\n",
    "            \"Spark Profiler\",\n",
    "            \"Ganglia\",\n",
    "            \"Grafana + Prometheus\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, tools in toolbox.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for tool in tools:\n",
    "            print(f\"  • {tool}\")\n",
    "    \n",
    "    print(\"\\n下一步建議:\")\n",
    "    next_steps = [\n",
    "        \"在生產環境中實踐這些調優技術\",\n",
    "        \"建立性能監控和報警機制\",\n",
    "        \"學習更高階的調優技術（如 Tungsten、Catalyst）\",\n",
    "        \"了解特定工作負載的最佳實踐\",\n",
    "        \"探索雲端 Spark 服務的調優選項\",\n",
    "        \"參與 Spark 社群，學習最新的優化技術\"\n",
    "    ]\n",
    "    \n",
    "    for i, step in enumerate(next_steps, 1):\n",
    "        print(f\"  {i}. {step}\")\n",
    "\n",
    "performance_tuning_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理資源\n",
    "print(\"清理資源...\")\n",
    "\n",
    "# 清除快取\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# 停止 Spark 會話\n",
    "spark.stop()\n",
    "\n",
    "print(\"Spark 會話已結束\")\n",
    "print(\"\\n感謝您完成 Spark 性能調優課程！\")\n",
    "print(\"記住：性能調優是一個持續的過程，需要根據具體的工作負載和環境進行調整。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}