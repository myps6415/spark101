{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章：Spark SQL 使用指南\n",
    "\n",
    "本章節將深入學習 Spark SQL 的使用，包括 SQL 語法、臨時視圖、複雜查詢等。\n",
    "\n",
    "## 學習目標\n",
    "- 掌握 Spark SQL 的基本語法\n",
    "- 學習創建和使用臨時視圖\n",
    "- 理解 DataFrame 和 SQL 的對應關係\n",
    "- 學習複雜查詢和優化技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, avg, sum as spark_sum, count, max as spark_max, min as spark_min\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, date\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQL學習\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark版本: {spark.version}\")\n",
    "print(f\"Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 準備測試數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建員工數據\n",
    "employees_data = [\n",
    "    (1, \"Alice\", 25, \"Engineering\", 75000, \"2020-01-15\", \"alice@company.com\"),\n",
    "    (2, \"Bob\", 30, \"Sales\", 65000, \"2019-03-20\", \"bob@company.com\"),\n",
    "    (3, \"Charlie\", 35, \"Engineering\", 85000, \"2018-07-10\", \"charlie@company.com\"),\n",
    "    (4, \"Diana\", 28, \"Marketing\", 60000, \"2021-02-05\", \"diana@company.com\"),\n",
    "    (5, \"Eve\", 32, \"Engineering\", 80000, \"2019-11-12\", \"eve@company.com\"),\n",
    "    (6, \"Frank\", 29, \"Sales\", 70000, \"2020-08-30\", \"frank@company.com\"),\n",
    "    (7, \"Grace\", 26, \"Marketing\", 55000, \"2021-05-18\", \"grace@company.com\"),\n",
    "    (8, \"Henry\", 31, \"Engineering\", 90000, \"2017-04-22\", \"henry@company.com\"),\n",
    "    (9, \"Ivy\", 27, \"Sales\", 68000, \"2020-10-15\", \"ivy@company.com\"),\n",
    "    (10, \"Jack\", 33, \"Marketing\", 62000, \"2019-12-01\", \"jack@company.com\")\n",
    "]\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True)\n",
    "])\n",
    "\n",
    "employees_df = spark.createDataFrame(employees_data, employees_schema)\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建部門數據\n",
    "departments_data = [\n",
    "    (\"Engineering\", \"Tech\", \"Building A\", 101),\n",
    "    (\"Sales\", \"Business\", \"Building B\", 102),\n",
    "    (\"Marketing\", \"Business\", \"Building C\", 103),\n",
    "    (\"HR\", \"Support\", \"Building D\", 104),\n",
    "    (\"Finance\", \"Support\", \"Building E\", 105)\n",
    "]\n",
    "\n",
    "departments_df = spark.createDataFrame(departments_data, \n",
    "                                     [\"name\", \"division\", \"location\", \"manager_id\"])\n",
    "departments_df.show()\n",
    "\n",
    "# 創建項目數據\n",
    "projects_data = [\n",
    "    (1, \"Project Alpha\", \"Engineering\", \"2021-01-01\", \"2021-12-31\", \"Active\"),\n",
    "    (2, \"Project Beta\", \"Sales\", \"2020-06-01\", \"2021-06-30\", \"Completed\"),\n",
    "    (3, \"Project Gamma\", \"Marketing\", \"2021-03-01\", \"2021-09-30\", \"Active\"),\n",
    "    (4, \"Project Delta\", \"Engineering\", \"2020-01-01\", \"2020-12-31\", \"Completed\"),\n",
    "    (5, \"Project Epsilon\", \"Sales\", \"2021-07-01\", \"2022-06-30\", \"Active\")\n",
    "]\n",
    "\n",
    "projects_df = spark.createDataFrame(projects_data, \n",
    "                                  [\"id\", \"name\", \"department\", \"start_date\", \"end_date\", \"status\"])\n",
    "projects_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 創建臨時視圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建臨時視圖\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "projects_df.createOrReplaceTempView(\"projects\")\n",
    "\n",
    "# 查看所有臨時視圖\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "print(\"臨時視圖創建完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 基本 SQL 查詢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本 SELECT 查詢\n",
    "print(\"=== 基本 SELECT 查詢 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, age, department, salary\n",
    "    FROM employees\n",
    "    ORDER BY salary DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# 使用 WHERE 條件\n",
    "print(\"\\n=== WHERE 條件查詢 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, age, department, salary\n",
    "    FROM employees\n",
    "    WHERE department = 'Engineering' AND salary > 75000\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚合查詢\n",
    "print(\"=== 聚合查詢 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as employee_count,\n",
    "        AVG(salary) as avg_salary,\n",
    "        MAX(salary) as max_salary,\n",
    "        MIN(salary) as min_salary,\n",
    "        SUM(salary) as total_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# 使用 HAVING 子句\n",
    "print(\"\\n=== HAVING 子句 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as employee_count,\n",
    "        AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    HAVING COUNT(*) > 2\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 複雜查詢和子查詢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 子查詢\n",
    "print(\"=== 子查詢 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    WHERE salary > (\n",
    "        SELECT AVG(salary) \n",
    "        FROM employees\n",
    "    )\n",
    "    ORDER BY salary DESC\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# 相關子查詢\n",
    "print(\"\\n=== 相關子查詢 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT e1.name, e1.department, e1.salary\n",
    "    FROM employees e1\n",
    "    WHERE e1.salary > (\n",
    "        SELECT AVG(e2.salary)\n",
    "        FROM employees e2\n",
    "        WHERE e2.department = e1.department\n",
    "    )\n",
    "    ORDER BY e1.department, e1.salary DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXISTS 子查詢\n",
    "print(\"=== EXISTS 子查詢 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT d.name, d.division, d.location\n",
    "    FROM departments d\n",
    "    WHERE EXISTS (\n",
    "        SELECT 1\n",
    "        FROM employees e\n",
    "        WHERE e.department = d.name\n",
    "    )\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# IN 子查詢\n",
    "print(\"\\n=== IN 子查詢 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    WHERE department IN (\n",
    "        SELECT name\n",
    "        FROM departments\n",
    "        WHERE division = 'Tech'\n",
    "    )\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 連接查詢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INNER JOIN\n",
    "print(\"=== INNER JOIN ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        d.division,\n",
    "        d.location\n",
    "    FROM employees e\n",
    "    INNER JOIN departments d ON e.department = d.name\n",
    "    ORDER BY e.salary DESC\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# LEFT JOIN\n",
    "print(\"\\n=== LEFT JOIN ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        d.division,\n",
    "        d.location\n",
    "    FROM employees e\n",
    "    LEFT JOIN departments d ON e.department = d.name\n",
    "    ORDER BY e.name\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三表連接\n",
    "print(\"=== 三表連接 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.name as employee_name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        d.division,\n",
    "        d.location,\n",
    "        p.name as project_name,\n",
    "        p.status as project_status\n",
    "    FROM employees e\n",
    "    INNER JOIN departments d ON e.department = d.name\n",
    "    INNER JOIN projects p ON e.department = p.department\n",
    "    WHERE p.status = 'Active'\n",
    "    ORDER BY e.department, e.name\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 窗口函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROW_NUMBER, RANK, DENSE_RANK\n",
    "print(\"=== 窗口函數 - 排名 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as row_num,\n",
    "        RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank,\n",
    "        DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dense_rank\n",
    "    FROM employees\n",
    "    ORDER BY department, salary DESC\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# 找出每個部門薪水最高的員工\n",
    "print(\"\\n=== 每個部門薪水最高的員工 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM (\n",
    "        SELECT \n",
    "            name,\n",
    "            department,\n",
    "            salary,\n",
    "            ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rn\n",
    "        FROM employees\n",
    "    ) ranked\n",
    "    WHERE rn = 1\n",
    "    ORDER BY department\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAG 和 LEAD 函數\n",
    "print(\"=== LAG 和 LEAD 函數 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        LAG(salary, 1) OVER (PARTITION BY department ORDER BY salary) as prev_salary,\n",
    "        LEAD(salary, 1) OVER (PARTITION BY department ORDER BY salary) as next_salary,\n",
    "        salary - LAG(salary, 1) OVER (PARTITION BY department ORDER BY salary) as salary_diff\n",
    "    FROM employees\n",
    "    ORDER BY department, salary\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# 累計值\n",
    "print(\"\\n=== 累計值 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        SUM(salary) OVER (PARTITION BY department ORDER BY salary ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cumulative_salary,\n",
    "        AVG(salary) OVER (PARTITION BY department ORDER BY salary ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_avg\n",
    "    FROM employees\n",
    "    ORDER BY department, salary\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 公用表表達式 (CTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 單個 CTE\n",
    "print(\"=== 單個 CTE ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH high_earners AS (\n",
    "        SELECT name, department, salary\n",
    "        FROM employees\n",
    "        WHERE salary > 70000\n",
    "    )\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as high_earner_count,\n",
    "        AVG(salary) as avg_high_salary\n",
    "    FROM high_earners\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_high_salary DESC\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# 多個 CTE\n",
    "print(\"\\n=== 多個 CTE ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH \n",
    "    dept_stats AS (\n",
    "        SELECT \n",
    "            department,\n",
    "            COUNT(*) as emp_count,\n",
    "            AVG(salary) as avg_salary\n",
    "        FROM employees\n",
    "        GROUP BY department\n",
    "    ),\n",
    "    top_depts AS (\n",
    "        SELECT department, avg_salary\n",
    "        FROM dept_stats\n",
    "        WHERE emp_count > 2\n",
    "    )\n",
    "    SELECT \n",
    "        e.name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        t.avg_salary as dept_avg_salary,\n",
    "        e.salary - t.avg_salary as salary_diff\n",
    "    FROM employees e\n",
    "    INNER JOIN top_depts t ON e.department = t.department\n",
    "    ORDER BY e.department, e.salary DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 條件表達式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE WHEN 表達式\n",
    "print(\"=== CASE WHEN 表達式 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        department,\n",
    "        age,\n",
    "        salary,\n",
    "        CASE \n",
    "            WHEN salary >= 85000 THEN 'Senior'\n",
    "            WHEN salary >= 70000 THEN 'Mid'\n",
    "            ELSE 'Junior'\n",
    "        END as salary_level,\n",
    "        CASE \n",
    "            WHEN age < 28 THEN 'Young'\n",
    "            WHEN age < 32 THEN 'Middle'\n",
    "            ELSE 'Senior'\n",
    "        END as age_group\n",
    "    FROM employees\n",
    "    ORDER BY department, salary DESC\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# 複雜的條件邏輯\n",
    "print(\"\\n=== 複雜條件邏輯 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        CASE \n",
    "            WHEN department = 'Engineering' AND salary > 80000 THEN 'Tech Lead'\n",
    "            WHEN department = 'Sales' AND salary > 65000 THEN 'Sales Manager'\n",
    "            WHEN department = 'Marketing' AND salary > 55000 THEN 'Marketing Manager'\n",
    "            ELSE 'Individual Contributor'\n",
    "        END as role_level\n",
    "    FROM employees\n",
    "    ORDER BY department, salary DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 字符串函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常用字符串函數\n",
    "print(\"=== 字符串函數 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        email,\n",
    "        UPPER(name) as name_upper,\n",
    "        LOWER(name) as name_lower,\n",
    "        LENGTH(name) as name_length,\n",
    "        SUBSTR(name, 1, 3) as name_prefix,\n",
    "        CONCAT(name, ' - ', department) as name_dept,\n",
    "        SPLIT(email, '@')[0] as username,\n",
    "        SPLIT(email, '@')[1] as domain\n",
    "    FROM employees\n",
    "    ORDER BY name\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# 字符串匹配\n",
    "print(\"\\n=== 字符串匹配 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        email,\n",
    "        department\n",
    "    FROM employees\n",
    "    WHERE name LIKE '%a%'  -- 包含字母 'a'\n",
    "       OR email LIKE '%gmail%'  -- 包含 'gmail'\n",
    "       OR department RLIKE '^[ES].*'  -- 以 E 或 S 開頭\n",
    "    ORDER BY name\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 日期和時間函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日期函數\n",
    "print(\"=== 日期函數 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        hire_date,\n",
    "        TO_DATE(hire_date, 'yyyy-MM-dd') as hire_date_parsed,\n",
    "        YEAR(TO_DATE(hire_date, 'yyyy-MM-dd')) as hire_year,\n",
    "        MONTH(TO_DATE(hire_date, 'yyyy-MM-dd')) as hire_month,\n",
    "        DAYOFWEEK(TO_DATE(hire_date, 'yyyy-MM-dd')) as hire_day_of_week,\n",
    "        DATEDIFF(CURRENT_DATE(), TO_DATE(hire_date, 'yyyy-MM-dd')) as days_since_hire,\n",
    "        DATE_ADD(TO_DATE(hire_date, 'yyyy-MM-dd'), 365) as first_anniversary\n",
    "    FROM employees\n",
    "    ORDER BY hire_date\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# 按年份分組\n",
    "print(\"\\n=== 按入職年份分組 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        YEAR(TO_DATE(hire_date, 'yyyy-MM-dd')) as hire_year,\n",
    "        COUNT(*) as employee_count,\n",
    "        AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY YEAR(TO_DATE(hire_date, 'yyyy-MM-dd'))\n",
    "    ORDER BY hire_year\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 數據透視和解透視"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 數據透視\n",
    "print(\"=== 數據透視 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT \n",
    "            YEAR(TO_DATE(hire_date, 'yyyy-MM-dd')) as hire_year,\n",
    "            department,\n",
    "            salary\n",
    "        FROM employees\n",
    "    ) \n",
    "    PIVOT (\n",
    "        AVG(salary)\n",
    "        FOR department IN ('Engineering', 'Sales', 'Marketing')\n",
    "    )\n",
    "    ORDER BY hire_year\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# 使用 DataFrame API 進行透視\n",
    "print(\"\\n=== 使用 DataFrame API 透視 ===\")\n",
    "pivot_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        YEAR(TO_DATE(hire_date, 'yyyy-MM-dd')) as hire_year,\n",
    "        department,\n",
    "        salary\n",
    "    FROM employees\n",
    "\"\"\").groupBy(\"hire_year\").pivot(\"department\").avg(\"salary\")\n",
    "\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 複雜分析查詢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移動平均\n",
    "print(\"=== 移動平均 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        AVG(salary) OVER (\n",
    "            PARTITION BY department \n",
    "            ORDER BY salary \n",
    "            ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING\n",
    "        ) as moving_avg_salary\n",
    "    FROM employees\n",
    "    ORDER BY department, salary\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# 百分位數\n",
    "print(\"\\n=== 百分位數分析 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        PERCENT_RANK() OVER (PARTITION BY department ORDER BY salary) as percent_rank,\n",
    "        NTILE(4) OVER (PARTITION BY department ORDER BY salary) as quartile\n",
    "    FROM employees\n",
    "    ORDER BY department, salary\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部門間比較分析\n",
    "print(\"=== 部門間比較分析 ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH dept_stats AS (\n",
    "        SELECT \n",
    "            department,\n",
    "            COUNT(*) as emp_count,\n",
    "            AVG(salary) as avg_salary,\n",
    "            STDDEV(salary) as salary_stddev,\n",
    "            MIN(salary) as min_salary,\n",
    "            MAX(salary) as max_salary\n",
    "        FROM employees\n",
    "        GROUP BY department\n",
    "    ),\n",
    "    overall_stats AS (\n",
    "        SELECT \n",
    "            AVG(avg_salary) as overall_avg,\n",
    "            STDDEV(avg_salary) as dept_avg_stddev\n",
    "        FROM dept_stats\n",
    "    )\n",
    "    SELECT \n",
    "        d.department,\n",
    "        d.emp_count,\n",
    "        ROUND(d.avg_salary, 2) as avg_salary,\n",
    "        ROUND(d.salary_stddev, 2) as salary_stddev,\n",
    "        ROUND(d.avg_salary - o.overall_avg, 2) as vs_overall_avg,\n",
    "        CASE \n",
    "            WHEN d.avg_salary > o.overall_avg + o.dept_avg_stddev THEN 'High'\n",
    "            WHEN d.avg_salary < o.overall_avg - o.dept_avg_stddev THEN 'Low'\n",
    "            ELSE 'Average'\n",
    "        END as salary_tier\n",
    "    FROM dept_stats d\n",
    "    CROSS JOIN overall_stats o\n",
    "    ORDER BY d.avg_salary DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 性能優化技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看執行計劃\n",
    "print(\"=== 查看執行計劃 ===\")\n",
    "complex_query = \"\"\"\n",
    "    SELECT \n",
    "        e.name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        d.division,\n",
    "        AVG(e.salary) OVER (PARTITION BY e.department) as dept_avg_salary\n",
    "    FROM employees e\n",
    "    INNER JOIN departments d ON e.department = d.name\n",
    "    WHERE e.salary > 65000\n",
    "    ORDER BY e.department, e.salary DESC\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(complex_query)\n",
    "result_df.explain(True)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 CACHE 優化重複查詢\n",
    "print(\"=== 使用 CACHE 優化 ===\")\n",
    "spark.sql(\"CACHE TABLE employees\")\n",
    "spark.sql(\"CACHE TABLE departments\")\n",
    "\n",
    "# 重複執行相同的查詢會更快\n",
    "result1 = spark.sql(\"SELECT COUNT(*) FROM employees\")\n",
    "result2 = spark.sql(\"SELECT COUNT(*) FROM employees\")\n",
    "\n",
    "print(f\"Employee count: {result1.collect()[0][0]}\")\n",
    "\n",
    "# 清除緩存\n",
    "spark.sql(\"UNCACHE TABLE employees\")\n",
    "spark.sql(\"UNCACHE TABLE departments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 實戰練習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習1：薪資分析儀表板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建薪資分析儀表板\n",
    "print(\"=== 薪資分析儀表板 ===\")\n",
    "\n",
    "# 1. 總體統計\n",
    "overall_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_employees,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        ROUND(STDDEV(salary), 2) as salary_stddev,\n",
    "        MIN(salary) as min_salary,\n",
    "        MAX(salary) as max_salary,\n",
    "        COUNT(DISTINCT department) as dept_count\n",
    "    FROM employees\n",
    "\"\"\")\n",
    "print(\"總體統計:\")\n",
    "overall_stats.show()\n",
    "\n",
    "# 2. 部門薪資分佈\n",
    "dept_distribution = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as emp_count,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        ROUND(MIN(salary), 2) as min_salary,\n",
    "        ROUND(MAX(salary), 2) as max_salary,\n",
    "        ROUND(STDDEV(salary), 2) as salary_stddev\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "print(\"\\n部門薪資分佈:\")\n",
    "dept_distribution.show()\n",
    "\n",
    "# 3. 薪資等級分佈\n",
    "salary_tiers = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN salary >= 80000 THEN '高薪 (80K+)'\n",
    "            WHEN salary >= 65000 THEN '中薪 (65K-80K)'\n",
    "            ELSE '低薪 (<65K)'\n",
    "        END as salary_tier,\n",
    "        COUNT(*) as emp_count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage\n",
    "    FROM employees\n",
    "    GROUP BY \n",
    "        CASE \n",
    "            WHEN salary >= 80000 THEN '高薪 (80K+)'\n",
    "            WHEN salary >= 65000 THEN '中薪 (65K-80K)'\n",
    "            ELSE '低薪 (<65K)'\n",
    "        END\n",
    "    ORDER BY emp_count DESC\n",
    "\"\"\")\n",
    "print(\"\\n薪資等級分佈:\")\n",
    "salary_tiers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習2：員工流失風險分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 員工流失風險分析\n",
    "print(\"=== 員工流失風險分析 ===\")\n",
    "\n",
    "risk_analysis = spark.sql(\"\"\"\n",
    "    WITH emp_analysis AS (\n",
    "        SELECT \n",
    "            e.name,\n",
    "            e.department,\n",
    "            e.age,\n",
    "            e.salary,\n",
    "            DATEDIFF(CURRENT_DATE(), TO_DATE(e.hire_date, 'yyyy-MM-dd')) as days_employed,\n",
    "            AVG(e.salary) OVER (PARTITION BY e.department) as dept_avg_salary,\n",
    "            PERCENT_RANK() OVER (PARTITION BY e.department ORDER BY e.salary) as salary_percentile\n",
    "        FROM employees e\n",
    "    )\n",
    "    SELECT \n",
    "        name,\n",
    "        department,\n",
    "        age,\n",
    "        salary,\n",
    "        ROUND(days_employed / 365.0, 1) as years_employed,\n",
    "        ROUND(dept_avg_salary, 2) as dept_avg_salary,\n",
    "        ROUND(salary_percentile * 100, 1) as salary_percentile,\n",
    "        CASE \n",
    "            WHEN salary_percentile < 0.25 AND days_employed > 365 THEN 'High Risk'\n",
    "            WHEN salary_percentile < 0.5 AND days_employed > 730 THEN 'Medium Risk'\n",
    "            WHEN age > 30 AND salary_percentile < 0.75 THEN 'Medium Risk'\n",
    "            ELSE 'Low Risk'\n",
    "        END as flight_risk\n",
    "    FROM emp_analysis\n",
    "    ORDER BY \n",
    "        CASE flight_risk\n",
    "            WHEN 'High Risk' THEN 1\n",
    "            WHEN 'Medium Risk' THEN 2\n",
    "            ELSE 3\n",
    "        END,\n",
    "        salary_percentile\n",
    "\"\"\")\n",
    "risk_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. DataFrame 與 SQL 的對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較 DataFrame API 和 SQL 的等效操作\n",
    "print(\"=== DataFrame API vs SQL 比較 ===\")\n",
    "\n",
    "# SQL 方式\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT department, AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    WHERE salary > 65000\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL 結果:\")\n",
    "sql_result.show()\n",
    "\n",
    "# DataFrame API 方式\n",
    "df_result = employees_df.filter(col(\"salary\") > 65000) \\\n",
    "                       .groupBy(\"department\") \\\n",
    "                       .agg(avg(\"salary\").alias(\"avg_salary\")) \\\n",
    "                       .orderBy(col(\"avg_salary\").desc())\n",
    "\n",
    "print(\"DataFrame API 結果:\")\n",
    "df_result.show()\n",
    "\n",
    "# 驗證結果相同\n",
    "print(f\"結果相同: {sql_result.collect() == df_result.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. 總結和清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示學習總結\n",
    "print(\"=== Spark SQL 學習總結 ===\")\n",
    "print(\"✓ 基本 SQL 查詢：SELECT, WHERE, GROUP BY, ORDER BY\")\n",
    "print(\"✓ 高級查詢：子查詢, CTE, 窗口函數\")\n",
    "print(\"✓ 連接操作：INNER JOIN, LEFT JOIN, 多表連接\")\n",
    "print(\"✓ 字符串和日期函數\")\n",
    "print(\"✓ 條件表達式：CASE WHEN\")\n",
    "print(\"✓ 數據透視和解透視\")\n",
    "print(\"✓ 性能優化：CACHE, EXPLAIN\")\n",
    "print(\"✓ 實戰分析：薪資分析, 風險評估\")\n",
    "print(\"✓ DataFrame API 與 SQL 對比\")\n",
    "\n",
    "# 清理臨時視圖\n",
    "spark.sql(\"DROP VIEW IF EXISTS employees\")\n",
    "spark.sql(\"DROP VIEW IF EXISTS departments\")\n",
    "spark.sql(\"DROP VIEW IF EXISTS projects\")\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()\n",
    "print(\"\\nSpark session 已停止\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 學習重點回顧\n",
    "\n",
    "### 核心概念\n",
    "1. **臨時視圖**：createOrReplaceTempView() 創建可查詢的表\n",
    "2. **SQL 語法**：完整的 SQL 支持，包括複雜查詢\n",
    "3. **執行計劃**：explain() 方法優化查詢性能\n",
    "4. **緩存策略**：CACHE TABLE 提升重複查詢性能\n",
    "\n",
    "### 高級特性\n",
    "1. **窗口函數**：ROW_NUMBER, RANK, LAG, LEAD\n",
    "2. **CTE**：WITH 子句簡化複雜查詢\n",
    "3. **數據透視**：PIVOT 進行數據重組\n",
    "4. **條件邏輯**：CASE WHEN 實現複雜業務邏輯\n",
    "\n",
    "### 實戰應用\n",
    "1. **數據分析**：統計分析、分組聚合\n",
    "2. **報表生成**：儀表板、KPI 指標\n",
    "3. **業務邏輯**：風險評估、分類分析\n",
    "4. **性能優化**：查詢調優、執行計劃分析\n",
    "\n",
    "掌握 Spark SQL 後，您可以用熟悉的 SQL 語法處理大規模數據，並享受 Spark 的分散式計算能力。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}