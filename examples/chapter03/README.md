# ç¬¬3ç« ï¼šDataFrame å’Œ Dataset API

## ğŸ“š å­¸ç¿’ç›®æ¨™

- ç†è§£ DataFrame çš„æ¦‚å¿µå’Œå„ªå‹¢
- æŒæ¡ Schema å®šç¾©å’Œæ“ä½œ
- å­¸æœƒå„ç¨®æ•¸æ“šè®€å¯«æ“ä½œ
- ç†Ÿæ‚‰ DataFrame çš„å¸¸ç”¨æ“ä½œ

## ğŸ¯ æœ¬ç« å…§å®¹

### æ ¸å¿ƒæ¦‚å¿µ
- **DataFrame** - çµæ§‹åŒ–æ•¸æ“šæŠ½è±¡
- **Schema** - æ•¸æ“šçµæ§‹å®šç¾©
- **Column** - åˆ—æ“ä½œ
- **Row** - è¡Œæ•¸æ“š

### æª”æ¡ˆèªªæ˜
- `dataframe_basics.py` - DataFrame åŸºç¤æ“ä½œ
- `schema_operations.py` - Schema å®šç¾©å’Œé¡å‹è½‰æ›
- `data_io_operations.py` - æ•¸æ“šè®€å¯«æ“ä½œ

## ğŸš€ é–‹å§‹å­¸ç¿’

### åŸ·è¡Œç¯„ä¾‹

```bash
# åŸ·è¡Œæ‰€æœ‰ç¯„ä¾‹
poetry run python examples/chapter03/dataframe_basics.py
poetry run python examples/chapter03/schema_operations.py
poetry run python examples/chapter03/data_io_operations.py

# æˆ–ä½¿ç”¨ Makefile
make run-chapter03
```

## ğŸ” æ·±å…¥ç†è§£

### DataFrame vs RDD

| ç‰¹æ€§ | DataFrame | RDD |
|------|-----------|-----|
| çµæ§‹ | çµæ§‹åŒ–æ•¸æ“š | éçµæ§‹åŒ–æ•¸æ“š |
| å„ªåŒ– | Catalyst å„ªåŒ–å™¨ | æ‰‹å‹•å„ªåŒ– |
| API | é«˜ç´š API | ä½ç´š API |
| æ€§èƒ½ | æ›´é«˜ | ç›¸å°è¼ƒä½ |
| æ˜“ç”¨æ€§ | æ›´æ˜“ç”¨ | æ›´éˆæ´» |

### ä¸»è¦æ“ä½œé¡å‹

#### 1. æ•¸æ“šå‰µå»º
```python
# å¾åˆ—è¡¨å‰µå»º
df = spark.createDataFrame(data, schema)

# å¾æ–‡ä»¶å‰µå»º
df = spark.read.json("path/to/file.json")
```

#### 2. æ•¸æ“šæŸ¥è©¢
```python
# é¸æ“‡åˆ—
df.select("name", "age")

# éæ¿¾è¡Œ
df.filter(df.age > 25)

# åˆ†çµ„èšåˆ
df.groupBy("department").avg("salary")
```

#### 3. æ•¸æ“šè½‰æ›
```python
# æ·»åŠ åˆ—
df.withColumn("new_col", df.col1 + df.col2)

# é‡å‘½ååˆ—
df.withColumnRenamed("old_name", "new_name")

# é¡å‹è½‰æ›
df.withColumn("age", df.age.cast("integer"))
```

## ğŸ“Š Schema æ“ä½œ

### å®šç¾© Schema
```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True)
])
```

### Schema é©—è­‰
```python
# æª¢æŸ¥ Schema
df.printSchema()

# é©—è­‰æ•¸æ“šé¡å‹
df.dtypes
```

## ğŸ’¾ æ•¸æ“šè®€å¯«

### æ”¯æŒçš„æ ¼å¼
- **JSON** - åŠçµæ§‹åŒ–æ•¸æ“š
- **CSV** - é€—è™Ÿåˆ†éš”å€¼
- **Parquet** - åˆ—å¼å­˜å„²ï¼ˆæ¨è–¦ï¼‰
- **Avro** - åºåˆ—åŒ–æ ¼å¼
- **ORC** - å„ªåŒ–çš„è¡Œåˆ—å­˜å„²

### è®€å–ç¯„ä¾‹
```python
# JSON
df = spark.read.json("data.json")

# CSV
df = spark.read.option("header", "true").csv("data.csv")

# Parquet
df = spark.read.parquet("data.parquet")
```

### å¯«å…¥ç¯„ä¾‹
```python
# å¯«å…¥ Parquet
df.write.mode("overwrite").parquet("output.parquet")

# å¯«å…¥ CSV
df.write.option("header", "true").csv("output.csv")

# åˆ†å€å¯«å…¥
df.write.partitionBy("year").parquet("partitioned_data")
```

## ğŸ“ ç·´ç¿’å»ºè­°

### åŸºç¤ç·´ç¿’
1. å‰µå»ºä¸åŒé¡å‹çš„ DataFrame
2. ç·´ç¿’å„ç¨®æŸ¥è©¢æ“ä½œ
3. å˜—è©¦ä¸åŒçš„æ•¸æ“šæ ¼å¼è®€å¯«

### é€²éšç·´ç¿’
1. è¨­è¨ˆè¤‡é›œçš„ Schema
2. è™•ç†åµŒå¥—æ•¸æ“šçµæ§‹
3. å¯¦æ–½æ•¸æ“šæ¸…æ´—æµç¨‹

## ğŸ› ï¸ å¯¦ç”¨æŠ€å·§

### 1. æ€§èƒ½å„ªåŒ–
```python
# æŸ¥çœ‹åŸ·è¡Œè¨ˆåŠƒ
df.explain()

# ç·©å­˜ DataFrame
df.cache()

# é‡åˆ†å€
df.repartition(4)
```

### 2. æ•¸æ“šå“è³ª
```python
# æª¢æŸ¥ç©ºå€¼
df.filter(df.column.isNull()).count()

# å»é‡
df.distinct()

# çµ±è¨ˆæè¿°
df.describe()
```

### 3. è¤‡é›œæ“ä½œ
```python
# çª—å£å‡½æ•¸
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

window = Window.partitionBy("department").orderBy("salary")
df.withColumn("rank", row_number().over(window))
```

## ğŸ”§ ç–‘é›£æ’è§£

### å¸¸è¦‹å•é¡Œ

**Q: å¦‚ä½•è™•ç† Schema ä¸ä¸€è‡´çš„å•é¡Œï¼Ÿ**
A: ä½¿ç”¨ `spark.read.option("multiline", "true")` æˆ–é å®šç¾© Schemaã€‚

**Q: è®€å–å¤§æ–‡ä»¶æ™‚è¨˜æ†¶é«”ä¸è¶³ï¼Ÿ**
A: èª¿æ•´åˆ†å€æ•¸æˆ–ä½¿ç”¨æµå¼è®€å–ã€‚

**Q: å¦‚ä½•è™•ç†ç‰¹æ®Šå­—ç¬¦ï¼Ÿ**
A: ä½¿ç”¨é©ç•¶çš„ç·¨ç¢¼è¨­ç½®å’Œè½‰ç¾©å­—ç¬¦ã€‚

## ğŸ’¡ æœ€ä½³å¯¦è¸

1. **ä½¿ç”¨ Parquet æ ¼å¼** - æœ€ä½³çš„æ€§èƒ½å’Œå£“ç¸®æ¯”
2. **åˆç†è¨­è¨ˆ Schema** - é¿å…éåº¦è¤‡é›œçš„åµŒå¥—çµæ§‹
3. **åˆ©ç”¨åˆ†å€** - æé«˜æŸ¥è©¢æ€§èƒ½
4. **æ•¸æ“šé©—è­‰** - ç¢ºä¿æ•¸æ“šå“è³ª
5. **é©ç•¶ç·©å­˜** - å°æ–¼é‡è¤‡ä½¿ç”¨çš„ DataFrame

## ğŸ“– ç›¸é—œæ–‡æª”

- [DataFrame API Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Data Sources](https://spark.apache.org/docs/latest/sql-data-sources.html)
- [PySpark SQL Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)

## â¡ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬ç« å¾Œï¼Œå»ºè­°ç¹¼çºŒå­¸ç¿’ï¼š
- [ç¬¬4ç« ï¼šSpark SQL](../chapter04/README.md)
- æ·±å…¥å­¸ç¿’ SQL æŸ¥è©¢èªæ³•
- äº†è§£ Catalyst å„ªåŒ–å™¨

## ğŸ¯ å­¸ç¿’æª¢æ ¸

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œä½ æ‡‰è©²èƒ½å¤ ï¼š
- [ ] å‰µå»ºå’Œæ“ä½œ DataFrame
- [ ] å®šç¾©å’Œä½¿ç”¨ Schema
- [ ] é€²è¡Œå„ç¨®æ•¸æ“šè®€å¯«æ“ä½œ
- [ ] è™•ç†è¤‡é›œçš„æ•¸æ“šè½‰æ›
- [ ] å„ªåŒ– DataFrame æ€§èƒ½

## ğŸ—‚ï¸ ç« ç¯€æ–‡ä»¶ç¸½è¦½

### dataframe_basics.py
- DataFrame å‰µå»ºæ–¹æ³•
- åŸºæœ¬æŸ¥è©¢æ“ä½œ
- æ•¸æ“šè½‰æ›å’Œæ¸…æ´—
- èšåˆå’Œåˆ†çµ„æ“ä½œ

### schema_operations.py
- Schema å®šç¾©å’Œé©—è­‰
- é¡å‹è½‰æ›æ“ä½œ
- è¤‡é›œæ•¸æ“šçµæ§‹è™•ç†
- Schema æ¼”åŒ–è™•ç†

### data_io_operations.py
- å¤šæ ¼å¼æ•¸æ“šè®€å¯«
- åˆ†å€ç­–ç•¥
- æ€§èƒ½å„ªåŒ–é…ç½®
- éŒ¯èª¤è™•ç†æ©Ÿåˆ¶