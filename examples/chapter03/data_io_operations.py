#!/usr/bin/env python3
"""
ç¬¬3ç« ï¼šDataFrame å’Œ Dataset API - è³‡æ–™è®€å¯«æ“ä½œ
å­¸ç¿’å„ç¨®è³‡æ–™æ ¼å¼çš„è®€å¯«æ“ä½œ
"""

import os
import tempfile

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when
from pyspark.sql.types import (DoubleType, IntegerType, StringType,
                               StructField, StructType)


def main():
    # å‰µå»º SparkSession
    spark = (
        SparkSession.builder.appName("Data I/O Operations")
        .master("local[*]")
        .getOrCreate()
    )

    print("ğŸ“ è³‡æ–™è®€å¯«æ“ä½œç¤ºç¯„")
    print("=" * 40)

    # æº–å‚™ç¤ºä¾‹æ•¸æ“š
    sample_data = [
        ("Alice", 25, "Engineer", 75000.0, "IT"),
        ("Bob", 30, "Manager", 85000.0, "Sales"),
        ("Charlie", 35, "Designer", 65000.0, "Marketing"),
        ("Diana", 28, "Analyst", 70000.0, "Finance"),
        ("Eve", 32, "Engineer", 80000.0, "IT"),
    ]

    columns = ["name", "age", "job", "salary", "department"]
    df = spark.createDataFrame(sample_data, columns)

    print("åŸå§‹ DataFrame:")
    df.show()

    # å‰µå»ºè‡¨æ™‚ç›®éŒ„ç”¨æ–¼ç¤ºä¾‹
    temp_dir = tempfile.mkdtemp()
    print(f"è‡¨æ™‚ç›®éŒ„: {temp_dir}")

    # 1. CSV æ–‡ä»¶æ“ä½œ
    print("\n1ï¸âƒ£ CSV æ–‡ä»¶æ“ä½œ")

    # å¯«å…¥ CSV
    csv_path = os.path.join(temp_dir, "employees.csv")
    df.coalesce(1).write.mode("overwrite").option("header", "true").option(
        "sep", ","
    ).csv(csv_path)
    print(f"DataFrame å·²å¯«å…¥ CSV: {csv_path}")

    # è®€å– CSV
    df_csv = (
        spark.read.option("header", "true")
        .option("inferSchema", "true")
        .option("sep", ",")
        .csv(csv_path)
    )

    print("å¾ CSV è®€å–çš„ DataFrame:")
    df_csv.show()
    df_csv.printSchema()

    # 2. JSON æ–‡ä»¶æ“ä½œ
    print("\n2ï¸âƒ£ JSON æ–‡ä»¶æ“ä½œ")

    # å¯«å…¥ JSON
    json_path = os.path.join(temp_dir, "employees.json")
    df.coalesce(1).write.mode("overwrite").json(json_path)
    print(f"DataFrame å·²å¯«å…¥ JSON: {json_path}")

    # è®€å– JSON
    df_json = spark.read.json(json_path)
    print("å¾ JSON è®€å–çš„ DataFrame:")
    df_json.show()
    df_json.printSchema()

    # 3. Parquet æ–‡ä»¶æ“ä½œ (æ¨è–¦æ ¼å¼)
    print("\n3ï¸âƒ£ Parquet æ–‡ä»¶æ“ä½œ")

    # å¯«å…¥ Parquet
    parquet_path = os.path.join(temp_dir, "employees.parquet")
    df.write.mode("overwrite").parquet(parquet_path)
    print(f"DataFrame å·²å¯«å…¥ Parquet: {parquet_path}")

    # è®€å– Parquet
    df_parquet = spark.read.parquet(parquet_path)
    print("å¾ Parquet è®€å–çš„ DataFrame:")
    df_parquet.show()
    df_parquet.printSchema()

    # 4. åˆ†å€å¯«å…¥
    print("\n4ï¸âƒ£ åˆ†å€å¯«å…¥")

    # æŒ‰éƒ¨é–€åˆ†å€å¯«å…¥
    partitioned_path = os.path.join(temp_dir, "employees_partitioned")
    df.write.mode("overwrite").partitionBy("department").parquet(partitioned_path)
    print(f"åˆ†å€ DataFrame å·²å¯«å…¥: {partitioned_path}")

    # è®€å–åˆ†å€æ•¸æ“š
    df_partitioned = spark.read.parquet(partitioned_path)
    print("å¾åˆ†å€è®€å–çš„ DataFrame:")
    df_partitioned.show()

    # è®€å–ç‰¹å®šåˆ†å€
    df_it_partition = spark.read.parquet(f"{partitioned_path}/department=IT")
    print("IT éƒ¨é–€çš„æ•¸æ“š:")
    df_it_partition.show()

    # 5. é«˜ç´šè®€å¯«é¸é …
    print("\n5ï¸âƒ£ é«˜ç´šè®€å¯«é¸é …")

    # å‰µå»ºåŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ•¸æ“š
    special_data = [
        ("Alice,Jr", 25, "Engineer", 75000.0),
        ("Bob\nSmith", 30, "Manager", 85000.0),
        ('Charlie"Brown', 35, "Designer", 65000.0),
    ]

    special_df = spark.createDataFrame(special_data, ["name", "age", "job", "salary"])
    print("åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„ DataFrame:")
    special_df.show()

    # ä½¿ç”¨è‡ªå®šç¾©åˆ†éš”ç¬¦å’Œå¼•è™Ÿ
    special_csv_path = os.path.join(temp_dir, "special_employees.csv")
    special_df.coalesce(1).write.mode("overwrite").option("header", "true").option(
        "sep", "|"
    ).option("quote", "'").option("escape", "\\").csv(special_csv_path)

    # è®€å–è‡ªå®šç¾©æ ¼å¼çš„ CSV
    df_special_csv = (
        spark.read.option("header", "true")
        .option("inferSchema", "true")
        .option("sep", "|")
        .option("quote", "'")
        .option("escape", "\\")
        .csv(special_csv_path)
    )

    print("è®€å–è‡ªå®šç¾©æ ¼å¼ CSV:")
    df_special_csv.show()

    # 6. è³‡æ–™å£“ç¸®
    print("\n6ï¸âƒ£ è³‡æ–™å£“ç¸®")

    # ä½¿ç”¨ä¸åŒå£“ç¸®æ ¼å¼
    compression_formats = ["none", "gzip", "snappy"]

    for compression in compression_formats:
        compressed_path = os.path.join(temp_dir, f"employees_{compression}")
        df.write.mode("overwrite").option("compression", compression).parquet(
            compressed_path
        )
        print(f"ä½¿ç”¨ {compression} å£“ç¸®æ ¼å¼å¯«å…¥: {compressed_path}")

    # 7. å¤šæª”æ¡ˆè®€å–
    print("\n7ï¸âƒ£ å¤šæª”æ¡ˆè®€å–")

    # å‰µå»ºå¤šå€‹æ–‡ä»¶
    for i in range(3):
        file_data = [
            (f"User_{i}_{j}", 20 + i + j, f"Job_{i}", 50000.0 + i * 1000 + j * 100)
            for j in range(2)
        ]
        file_df = spark.createDataFrame(file_data, ["name", "age", "job", "salary"])
        file_path = os.path.join(temp_dir, f"batch_{i}.json")
        file_df.coalesce(1).write.mode("overwrite").json(file_path)

    # è®€å–æ‰€æœ‰ JSON æ–‡ä»¶
    all_files_path = os.path.join(temp_dir, "batch_*.json")
    df_all = spark.read.json(all_files_path)
    print("è®€å–æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶:")
    df_all.show()

    # 8. éŒ¯èª¤è™•ç†å’Œè³‡æ–™å“è³ª
    print("\n8ï¸âƒ£ éŒ¯èª¤è™•ç†å’Œè³‡æ–™å“è³ª")

    # å‰µå»ºåŒ…å«éŒ¯èª¤æ•¸æ“šçš„ CSV
    bad_csv_content = """name,age,salary
Alice,25,75000
Bob,thirty,85000
Charlie,35,not_a_number
Diana,28,70000"""

    bad_csv_path = os.path.join(temp_dir, "bad_data.csv")
    with open(bad_csv_path, "w") as f:
        f.write(bad_csv_content)

    # å˜—è©¦è®€å–éŒ¯èª¤æ•¸æ“š
    print("å˜—è©¦è®€å–åŒ…å«éŒ¯èª¤çš„ CSV:")
    try:
        # è¨­ç½®æ¨¡å¼ç‚º PERMISSIVE (é è¨­)
        df_bad = (
            spark.read.option("header", "true")
            .option("mode", "PERMISSIVE")
            .option("columnNameOfCorruptRecord", "_corrupt_record")
            .csv(bad_csv_path)
        )

        print("PERMISSIVE æ¨¡å¼è®€å–çµæœ:")
        df_bad.show()

        # éæ¿¾å‡ºéŒ¯èª¤è¨˜éŒ„
        corrupt_records = df_bad.filter(col("_corrupt_record").isNotNull())
        if corrupt_records.count() > 0:
            print("éŒ¯èª¤è¨˜éŒ„:")
            corrupt_records.show()

    except Exception as e:
        print(f"è®€å–éŒ¯èª¤: {e}")

    # 9. è³‡æ–™æ ¼å¼è½‰æ›
    print("\n9ï¸âƒ£ è³‡æ–™æ ¼å¼è½‰æ›")

    # CSV è½‰ Parquet
    df_csv.write.mode("overwrite").parquet(os.path.join(temp_dir, "csv_to_parquet"))
    print("CSV è½‰ Parquet å®Œæˆ")

    # JSON è½‰ CSV
    df_json.coalesce(1).write.mode("overwrite").option("header", "true").csv(
        os.path.join(temp_dir, "json_to_csv")
    )
    print("JSON è½‰ CSV å®Œæˆ")

    # 10. è³‡æ–™å“è³ªæª¢æŸ¥
    print("\nğŸ”Ÿ è³‡æ–™å“è³ªæª¢æŸ¥")

    # æª¢æŸ¥ç©ºå€¼
    print("ç©ºå€¼çµ±è¨ˆ:")
    df.select([col(c).isNull().cast("int").alias(c) for c in df.columns]).agg(
        *[col(c).sum().alias(f"{c}_nulls") for c in df.columns]
    ).show()

    # æª¢æŸ¥é‡è¤‡è¨˜éŒ„
    print(f"ç¸½è¨˜éŒ„æ•¸: {df.count()}")
    print(f"å»é‡å¾Œè¨˜éŒ„æ•¸: {df.distinct().count()}")

    # æª¢æŸ¥æ•¸å€¼ç¯„åœ
    print("æ•¸å€¼åˆ—çµ±è¨ˆ:")
    df.select("age", "salary").describe().show()

    # æ¸…ç†è‡¨æ™‚ç›®éŒ„
    import shutil

    shutil.rmtree(temp_dir)
    print(f"æ¸…ç†è‡¨æ™‚ç›®éŒ„: {temp_dir}")

    # åœæ­¢ SparkSession
    spark.stop()
    print("\nâœ… è³‡æ–™è®€å¯«æ“ä½œç¤ºç¯„å®Œæˆ")


if __name__ == "__main__":
    main()
