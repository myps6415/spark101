#!/usr/bin/env python3
"""
ç¬¬3ç« ï¼šDataFrame å’Œ Dataset API - DataFrame åŸºç¤æ“ä½œ
å­¸ç¿’ DataFrame çš„å‰µå»ºã€Schema å®šç¾©å’ŒåŸºæœ¬æ“ä½œ
"""

import os

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, lower, regexp_replace, upper, when
from pyspark.sql.types import (DoubleType, IntegerType, StringType,
                               StructField, StructType)


def main():
    # å‰µå»º SparkSession
    spark = (
        SparkSession.builder.appName("DataFrame Basics")
        .master("local[*]")
        .getOrCreate()
    )

    print("ğŸ“Š DataFrame åŸºç¤æ“ä½œç¤ºç¯„")
    print("=" * 40)

    # 1. å‰µå»º DataFrame çš„ä¸åŒæ–¹å¼
    print("\n1ï¸âƒ£ å‰µå»º DataFrame")

    # æ–¹å¼1: å¾åˆ—è¡¨å‰µå»º
    data = [
        ("Alice", 25, "Engineer", 75000.0),
        ("Bob", 30, "Manager", 85000.0),
        ("Charlie", 35, "Designer", 65000.0),
        ("Diana", 28, "Analyst", 70000.0),
        ("Eve", 32, "Engineer", 80000.0),
    ]
    columns = ["name", "age", "job", "salary"]
    df = spark.createDataFrame(data, columns)

    print("å¾åˆ—è¡¨å‰µå»ºçš„ DataFrame:")
    df.show()

    # æ–¹å¼2: ä½¿ç”¨ Schema å®šç¾©
    print("\n2ï¸âƒ£ ä½¿ç”¨ Schema å®šç¾©")
    schema = StructType(
        [
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True),
            StructField("job", StringType(), True),
            StructField("salary", DoubleType(), True),
        ]
    )

    df_with_schema = spark.createDataFrame(data, schema)
    print("DataFrame çµæ§‹:")
    df_with_schema.printSchema()

    # 3. åŸºæœ¬è³‡è¨ŠæŸ¥çœ‹
    print("\n3ï¸âƒ£ DataFrame åŸºæœ¬è³‡è¨Š")
    print(f"è¡Œæ•¸: {df.count()}")
    print(f"åˆ—æ•¸: {len(df.columns)}")
    print(f"åˆ—å: {df.columns}")

    # çµ±è¨ˆæ‘˜è¦
    print("\næ•¸å€¼åˆ—çµ±è¨ˆæ‘˜è¦:")
    df.describe().show()

    # 4. é¸æ“‡å’Œéæ¿¾æ“ä½œ
    print("\n4ï¸âƒ£ é¸æ“‡å’Œéæ¿¾æ“ä½œ")

    # é¸æ“‡ç‰¹å®šåˆ—
    print("é¸æ“‡ name å’Œ salary åˆ—:")
    df.select("name", "salary").show()

    # ä½¿ç”¨ col å‡½æ•¸
    print("ä½¿ç”¨ col å‡½æ•¸é¸æ“‡:")
    df.select(col("name"), col("salary")).show()

    # éæ¿¾æ“ä½œ
    print("å¹´é½¡å¤§æ–¼ 30 çš„å“¡å·¥:")
    df.filter(col("age") > 30).show()

    print("å·¥ç¨‹å¸«è·ä½çš„å“¡å·¥:")
    df.filter(col("job") == "Engineer").show()

    # å¤šæ¢ä»¶éæ¿¾
    print("å¹´é½¡å¤§æ–¼ 25 ä¸”è–ªè³‡å¤§æ–¼ 70000 çš„å“¡å·¥:")
    df.filter((col("age") > 25) & (col("salary") > 70000)).show()

    # 5. æ·»åŠ å’Œä¿®æ”¹åˆ—
    print("\n5ï¸âƒ£ æ·»åŠ å’Œä¿®æ”¹åˆ—")

    # æ·»åŠ æ–°åˆ—
    df_with_bonus = df.withColumn("bonus", col("salary") * 0.1)
    print("æ·»åŠ çé‡‘åˆ—:")
    df_with_bonus.show()

    # ä¿®æ”¹ç¾æœ‰åˆ—
    df_upper_name = df.withColumn("name", upper(col("name")))
    print("å§“åè½‰å¤§å¯«:")
    df_upper_name.show()

    # æ¢ä»¶åˆ—
    df_with_level = df.withColumn(
        "level",
        when(col("age") < 30, "Junior")
        .when(col("age") < 35, "Senior")
        .otherwise("Expert"),
    )
    print("æ·»åŠ ç´šåˆ¥åˆ—:")
    df_with_level.show()

    # 6. æ’åºæ“ä½œ
    print("\n6ï¸âƒ£ æ’åºæ“ä½œ")

    # æŒ‰å¹´é½¡æ’åº
    print("æŒ‰å¹´é½¡å‡åº:")
    df.orderBy(col("age")).show()

    # æŒ‰è–ªè³‡é™åº
    print("æŒ‰è–ªè³‡é™åº:")
    df.orderBy(col("salary").desc()).show()

    # å¤šåˆ—æ’åº
    print("æŒ‰è·ä½å‡åºï¼Œè–ªè³‡é™åº:")
    df.orderBy(col("job").asc(), col("salary").desc()).show()

    # 7. åˆ†çµ„å’Œèšåˆæ“ä½œ
    print("\n7ï¸âƒ£ åˆ†çµ„å’Œèšåˆæ“ä½œ")

    # æŒ‰è·ä½åˆ†çµ„çµ±è¨ˆ
    print("æŒ‰è·ä½åˆ†çµ„çµ±è¨ˆ:")
    df.groupBy("job").count().show()

    # æŒ‰è·ä½è¨ˆç®—å¹³å‡è–ªè³‡
    print("æŒ‰è·ä½è¨ˆç®—å¹³å‡è–ªè³‡:")
    df.groupBy("job").avg("salary").show()

    # å¤šå€‹èšåˆå‡½æ•¸
    from pyspark.sql.functions import avg, max, min
    from pyspark.sql.functions import sum as spark_sum

    print("æŒ‰è·ä½çš„è©³ç´°çµ±è¨ˆ:")
    df.groupBy("job").agg(
        avg("salary").alias("avg_salary"),
        max("salary").alias("max_salary"),
        min("salary").alias("min_salary"),
        spark_sum("salary").alias("total_salary"),
    ).show()

    # 8. å­—ä¸²æ“ä½œ
    print("\n8ï¸âƒ£ å­—ä¸²æ“ä½œ")

    # å¤§å°å¯«è½‰æ›
    df_string_ops = df.select(
        col("name"),
        upper(col("name")).alias("upper_name"),
        lower(col("name")).alias("lower_name"),
    )
    print("å­—ä¸²å¤§å°å¯«è½‰æ›:")
    df_string_ops.show()

    # 9. è™•ç†ç©ºå€¼
    print("\n9ï¸âƒ£ è™•ç†ç©ºå€¼")

    # å‰µå»ºåŒ…å«ç©ºå€¼çš„æ•¸æ“š
    data_with_nulls = [
        ("Alice", 25, "Engineer", 75000.0),
        ("Bob", None, "Manager", 85000.0),
        ("Charlie", 35, None, 65000.0),
        ("Diana", 28, "Analyst", None),
    ]

    df_nulls = spark.createDataFrame(data_with_nulls, columns)
    print("åŒ…å«ç©ºå€¼çš„ DataFrame:")
    df_nulls.show()

    # åˆªé™¤åŒ…å«ç©ºå€¼çš„è¡Œ
    print("åˆªé™¤åŒ…å«ç©ºå€¼çš„è¡Œ:")
    df_nulls.dropna().show()

    # å¡«å……ç©ºå€¼
    print("å¡«å……ç©ºå€¼:")
    df_filled = df_nulls.fillna({"age": 0, "job": "Unknown", "salary": 0.0})
    df_filled.show()

    # 10. è®€å¯« CSV æ–‡ä»¶
    print("\nğŸ”Ÿ è®€å¯« CSV æ–‡ä»¶")

    # å¯«å…¥ CSV
    output_path = "/tmp/employees.csv"
    df.coalesce(1).write.mode("overwrite").option("header", "true").csv(output_path)
    print(f"DataFrame å·²å¯«å…¥ {output_path}")

    # è®€å– CSV
    if os.path.exists(output_path):
        df_read = (
            spark.read.option("header", "true")
            .option("inferSchema", "true")
            .csv(output_path)
        )
        print("å¾ CSV è®€å–çš„ DataFrame:")
        df_read.show()

    # åœæ­¢ SparkSession
    spark.stop()
    print("\nâœ… DataFrame åŸºç¤æ“ä½œç¤ºç¯„å®Œæˆ")


if __name__ == "__main__":
    main()
