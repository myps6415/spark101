#!/usr/bin/env python3
"""
ç¬¬2ç« ï¼šSpark Core åŸºæœ¬æ“ä½œ - RDD åŸºç¤
å­¸ç¿’ RDD çš„å‰µå»ºå’ŒåŸºæœ¬æ“ä½œ
"""

from pyspark import SparkConf, SparkContext


def main():
    # å‰µå»º SparkContext
    conf = SparkConf().setAppName("RDD Basics").setMaster("local[*]")
    sc = SparkContext(conf=conf)

    print("ğŸ¯ RDD åŸºç¤æ“ä½œç¤ºç¯„")
    print("=" * 30)

    # 1. å¾é›†åˆå‰µå»º RDD
    numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    numbers_rdd = sc.parallelize(numbers)

    print(f"åŸå§‹æ•¸æ“š: {numbers}")
    print(f"RDD åˆ†å€æ•¸: {numbers_rdd.getNumPartitions()}")

    # 2. Transformation æ“ä½œ
    print("\nğŸ”„ Transformation æ“ä½œ:")

    # éæ¿¾å¶æ•¸
    even_rdd = numbers_rdd.filter(lambda x: x % 2 == 0)
    print(f"å¶æ•¸: {even_rdd.collect()}")

    # æ˜ å°„æ“ä½œ - å¹³æ–¹
    squared_rdd = numbers_rdd.map(lambda x: x**2)
    print(f"å¹³æ–¹: {squared_rdd.collect()}")

    # æ‰å¹³åŒ–æ˜ å°„
    words = ["hello world", "spark is awesome", "big data processing"]
    words_rdd = sc.parallelize(words)
    flat_words_rdd = words_rdd.flatMap(lambda x: x.split())
    print(f"åˆ†è©çµæœ: {flat_words_rdd.collect()}")

    # 3. Action æ“ä½œ
    print("\nâš¡ Action æ“ä½œ:")

    # è¨ˆæ•¸
    count = numbers_rdd.count()
    print(f"å…ƒç´ ç¸½æ•¸: {count}")

    # æ±‚å’Œ
    total = numbers_rdd.reduce(lambda x, y: x + y)
    print(f"ç¸½å’Œ: {total}")

    # å–å‰ N å€‹å…ƒç´ 
    first_three = numbers_rdd.take(3)
    print(f"å‰ä¸‰å€‹å…ƒç´ : {first_three}")

    # çµ±è¨ˆ
    stats = numbers_rdd.stats()
    print(f"çµ±è¨ˆä¿¡æ¯: å¹³å‡å€¼={stats.mean():.2f}, æ¨™æº–å·®={stats.stdev():.2f}")

    # 4. éµå€¼å° RDD
    print("\nğŸ”‘ éµå€¼å° RDD æ“ä½œ:")

    # å‰µå»ºéµå€¼å°
    pairs_rdd = numbers_rdd.map(lambda x: (x % 3, x))
    print(f"éµå€¼å°: {pairs_rdd.collect()}")

    # æŒ‰éµåˆ†çµ„
    grouped_rdd = pairs_rdd.groupByKey()
    grouped_result = grouped_rdd.mapValues(list).collect()
    print(f"æŒ‰éµåˆ†çµ„: {grouped_result}")

    # æŒ‰éµæ±‚å’Œ
    sum_by_key = pairs_rdd.reduceByKey(lambda x, y: x + y)
    print(f"æŒ‰éµæ±‚å’Œ: {sum_by_key.collect()}")

    # åœæ­¢ SparkContext
    sc.stop()
    print("\nâœ… RDD æ“ä½œç¤ºç¯„å®Œæˆ")


if __name__ == "__main__":
    main()
