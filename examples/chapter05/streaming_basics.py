#!/usr/bin/env python3
"""
ç¬¬5ç« ï¼šSpark Streaming - æµå¼è™•ç†åŸºç¤
å­¸ç¿’çµæ§‹åŒ–æµè™•ç†çš„åŸºæœ¬æ¦‚å¿µå’Œæ“ä½œ
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
from pyspark.sql.functions import col, current_timestamp, window, count, sum as spark_sum, avg
import time
import threading
import json
import os
import tempfile
from datetime import datetime, timedelta

def generate_sample_data(output_dir, duration_seconds=30):
    """ç”Ÿæˆç¤ºä¾‹æµæ•¸æ“š"""
    import random
    import json
    import time
    from datetime import datetime
    
    users = ["alice", "bob", "charlie", "diana", "eve"]
    products = ["laptop", "phone", "tablet", "watch", "headphones"]
    
    for i in range(duration_seconds):
        # æ¯ç§’ç”Ÿæˆå¹¾ç­†è¨˜éŒ„
        for j in range(random.randint(1, 5)):
            record = {
                "user_id": random.choice(users),
                "product": random.choice(products),
                "quantity": random.randint(1, 3),
                "price": round(random.uniform(10, 1000), 2),
                "timestamp": datetime.now().isoformat()
            }
            
            # å¯«å…¥æ–‡ä»¶
            filename = f"data_{i}_{j}.json"
            with open(os.path.join(output_dir, filename), 'w') as f:
                json.dump(record, f)
        
        time.sleep(1)
    
    print(f"æ•¸æ“šç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {duration_seconds} ç§’çš„æ•¸æ“š")

def main():
    # å‰µå»º SparkSession
    spark = SparkSession.builder \
        .appName("Spark Streaming Basics") \
        .master("local[*]") \
        .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint") \
        .getOrCreate()
    
    # é™ä½æ—¥èªŒç´šåˆ¥
    spark.sparkContext.setLogLevel("WARN")
    
    print("ğŸ“º Spark Streaming åŸºç¤ç¤ºç¯„")
    print("=" * 40)
    
    # å‰µå»ºè‡¨æ™‚ç›®éŒ„
    temp_dir = tempfile.mkdtemp()
    input_dir = os.path.join(temp_dir, "input")
    output_dir = os.path.join(temp_dir, "output")
    os.makedirs(input_dir, exist_ok=True)
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"è‡¨æ™‚ç›®éŒ„: {temp_dir}")
    print(f"è¼¸å…¥ç›®éŒ„: {input_dir}")
    print(f"è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    # 1. å®šç¾©æ•¸æ“šçµæ§‹
    print("\n1ï¸âƒ£ å®šç¾©æ•¸æ“šçµæ§‹")
    
    # å®šç¾©æµæ•¸æ“šçš„ Schema
    schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("product", StringType(), True),
        StructField("quantity", IntegerType(), True),
        StructField("price", IntegerType(), True),
        StructField("timestamp", StringType(), True)
    ])
    
    print("æµæ•¸æ“š Schema:")
    print(schema)
    
    # 2. å‰µå»ºæµæ•¸æ“šæº
    print("\n2ï¸âƒ£ å‰µå»ºæµæ•¸æ“šæº")
    
    # å¾æ–‡ä»¶ç³»çµ±è®€å–æµæ•¸æ“š
    streaming_df = spark.readStream \
        .format("json") \
        .schema(schema) \
        .option("path", input_dir) \
        .load()
    
    print("æµæ•¸æ“šæºå·²å‰µå»º")
    
    # 3. åŸºæœ¬æµè™•ç†
    print("\n3ï¸âƒ£ åŸºæœ¬æµè™•ç†")
    
    # æ·»åŠ è™•ç†æ™‚é–“æˆ³
    processed_df = streaming_df.withColumn("processing_time", current_timestamp())
    
    # å•Ÿå‹•æ•¸æ“šç”Ÿæˆç·šç¨‹
    print("å•Ÿå‹•æ•¸æ“šç”Ÿæˆå™¨...")
    generator_thread = threading.Thread(
        target=generate_sample_data,
        args=(input_dir, 15)  # ç”Ÿæˆ15ç§’çš„æ•¸æ“š
    )
    generator_thread.daemon = True
    generator_thread.start()
    
    # åŸºæœ¬æŸ¥è©¢ - è¼¸å‡ºåˆ°æ§åˆ¶å°
    print("\né–‹å§‹åŸºæœ¬æµè™•ç†...")
    basic_query = processed_df.writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", "false") \
        .trigger(processingTime='5 seconds') \
        .start()
    
    # è®“æŸ¥è©¢é‹è¡Œä¸€æ®µæ™‚é–“
    time.sleep(20)
    basic_query.stop()
    
    # 4. èšåˆæŸ¥è©¢
    print("\n4ï¸âƒ£ èšåˆæŸ¥è©¢")
    
    # æŒ‰ç”¢å“çµ±è¨ˆ
    product_stats = streaming_df.groupBy("product") \
        .agg(
            count("*").alias("order_count"),
            spark_sum("quantity").alias("total_quantity"),
            avg("price").alias("avg_price")
        )
    
    print("é–‹å§‹ç”¢å“çµ±è¨ˆ...")
    aggregation_query = product_stats.writeStream \
        .outputMode("complete") \
        .format("console") \
        .option("truncate", "false") \
        .trigger(processingTime='5 seconds') \
        .start()
    
    # è®“æŸ¥è©¢é‹è¡Œä¸€æ®µæ™‚é–“
    time.sleep(20)
    aggregation_query.stop()
    
    # 5. è¦–çª—æ“ä½œ
    print("\n5ï¸âƒ£ è¦–çª—æ“ä½œ")
    
    # å…ˆå°‡timestampè½‰æ›ç‚ºTimestampType
    from pyspark.sql.functions import to_timestamp
    windowed_df = streaming_df.withColumn(
        "event_time", 
        to_timestamp(col("timestamp"))
    )
    
    # å‰µå»ºæ™‚é–“çª—å£çµ±è¨ˆ
    windowed_stats = windowed_df \
        .withWatermark("event_time", "1 minute") \
        .groupBy(
            window(col("event_time"), "10 seconds", "5 seconds"),
            col("product")
        ) \
        .agg(
            count("*").alias("count"),
            spark_sum("quantity").alias("total_quantity")
        )
    
    print("é–‹å§‹è¦–çª—çµ±è¨ˆ...")
    window_query = windowed_stats.writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", "false") \
        .trigger(processingTime='5 seconds') \
        .start()
    
    # è®“æŸ¥è©¢é‹è¡Œä¸€æ®µæ™‚é–“
    time.sleep(20)
    window_query.stop()
    
    # 6. è¼¸å‡ºåˆ°æ–‡ä»¶
    print("\n6ï¸âƒ£ è¼¸å‡ºåˆ°æ–‡ä»¶")
    
    # å¯«å…¥åˆ° Parquet æ–‡ä»¶
    file_query = processed_df.writeStream \
        .outputMode("append") \
        .format("parquet") \
        .option("path", output_dir) \
        .option("checkpointLocation", "/tmp/checkpoint_file") \
        .trigger(processingTime='10 seconds') \
        .start()
    
    print("é–‹å§‹å¯«å…¥æ–‡ä»¶...")
    time.sleep(15)
    file_query.stop()
    
    # æª¢æŸ¥è¼¸å‡ºæ–‡ä»¶
    try:
        output_df = spark.read.parquet(output_dir)
        print(f"è¼¸å‡ºæ–‡ä»¶è¨˜éŒ„æ•¸: {output_df.count()}")
        print("è¼¸å‡ºæ–‡ä»¶å…§å®¹é è¦½:")
        output_df.show(5)
    except Exception as e:
        print(f"è®€å–è¼¸å‡ºæ–‡ä»¶æ™‚å‡ºéŒ¯: {e}")
    
    # 7. ç‹€æ…‹ç®¡ç†
    print("\n7ï¸âƒ£ ç‹€æ…‹ç®¡ç†")
    
    # å‰µå»ºæœ‰ç‹€æ…‹çš„æµè™•ç†
    from pyspark.sql.functions import struct
    
    # ç”¨æˆ¶ç¸½è³¼è²·çµ±è¨ˆ
    user_totals = streaming_df.groupBy("user_id") \
        .agg(
            count("*").alias("total_orders"),
            spark_sum("quantity").alias("total_items"),
            spark_sum("price").alias("total_spent")
        )
    
    print("é–‹å§‹ç”¨æˆ¶çµ±è¨ˆ...")
    user_query = user_totals.writeStream \
        .outputMode("complete") \
        .format("console") \
        .option("truncate", "false") \
        .trigger(processingTime='5 seconds') \
        .start()
    
    # é‡æ–°å•Ÿå‹•æ•¸æ“šç”Ÿæˆå™¨
    generator_thread2 = threading.Thread(
        target=generate_sample_data,
        args=(input_dir, 10)
    )
    generator_thread2.daemon = True
    generator_thread2.start()
    
    time.sleep(15)
    user_query.stop()
    
    # 8. éŒ¯èª¤è™•ç†å’Œç›£æ§
    print("\n8ï¸âƒ£ éŒ¯èª¤è™•ç†å’Œç›£æ§")
    
    # å‰µå»ºä¸€å€‹åŒ…å«éŒ¯èª¤è™•ç†çš„æŸ¥è©¢
    try:
        # éæ¿¾ç•°å¸¸æ•¸æ“š
        filtered_df = streaming_df.filter(
            (col("quantity") > 0) & 
            (col("price") > 0) & 
            (col("user_id").isNotNull())
        )
        
        monitoring_query = filtered_df.writeStream \
            .outputMode("append") \
            .format("console") \
            .option("truncate", "false") \
            .trigger(processingTime='5 seconds') \
            .start()
        
        print("é–‹å§‹ç›£æ§æŸ¥è©¢...")
        time.sleep(10)
        
        # æŸ¥çœ‹æŸ¥è©¢ç‹€æ…‹
        print(f"æŸ¥è©¢ç‹€æ…‹: {monitoring_query.status}")
        print(f"æŸ¥è©¢é€²åº¦: {monitoring_query.lastProgress}")
        
        monitoring_query.stop()
        
    except Exception as e:
        print(f"ç›£æ§æŸ¥è©¢éŒ¯èª¤: {e}")
    
    # 9. å¤šè¼¸å‡ºæŸ¥è©¢
    print("\n9ï¸âƒ£ å¤šè¼¸å‡ºæŸ¥è©¢")
    
    # å‰µå»ºå¤šå€‹è¼¸å‡ºæµ
    console_query = processed_df.writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", "false") \
        .trigger(processingTime='5 seconds') \
        .queryName("console_output") \
        .start()
    
    # åŒæ™‚çµ±è¨ˆ
    stats_query = product_stats.writeStream \
        .outputMode("complete") \
        .format("memory") \
        .trigger(processingTime='5 seconds') \
        .queryName("product_stats") \
        .start()
    
    # é‡æ–°å•Ÿå‹•æ•¸æ“šç”Ÿæˆå™¨
    generator_thread3 = threading.Thread(
        target=generate_sample_data,
        args=(input_dir, 8)
    )
    generator_thread3.daemon = True
    generator_thread3.start()
    
    time.sleep(12)
    
    # æŸ¥çœ‹å…§å­˜è¡¨
    try:
        memory_df = spark.sql("SELECT * FROM product_stats")
        print("å…§å­˜è¡¨å…§å®¹:")
        memory_df.show()
    except Exception as e:
        print(f"æŸ¥çœ‹å…§å­˜è¡¨éŒ¯èª¤: {e}")
    
    # åœæ­¢æ‰€æœ‰æŸ¥è©¢
    console_query.stop()
    stats_query.stop()
    
    # 10. æµè™•ç†æ€§èƒ½èª¿å„ª
    print("\nğŸ”Ÿ æµè™•ç†æ€§èƒ½èª¿å„ª")
    
    # å„ªåŒ–é…ç½®çš„æµè™•ç†
    optimized_df = streaming_df \
        .coalesce(2) \
        .filter(col("price") > 50) \
        .groupBy("product") \
        .agg(count("*").alias("high_value_orders"))
    
    print("é–‹å§‹å„ªåŒ–æŸ¥è©¢...")
    optimized_query = optimized_df.writeStream \
        .outputMode("complete") \
        .format("console") \
        .option("truncate", "false") \
        .trigger(processingTime='3 seconds') \
        .start()
    
    # æœ€å¾Œä¸€æ¬¡æ•¸æ“šç”Ÿæˆ
    generator_thread4 = threading.Thread(
        target=generate_sample_data,
        args=(input_dir, 5)
    )
    generator_thread4.daemon = True
    generator_thread4.start()
    
    time.sleep(10)
    optimized_query.stop()
    
    # 11. æŸ¥è©¢ç®¡ç†
    print("\n1ï¸âƒ£1ï¸âƒ£ æŸ¥è©¢ç®¡ç†")
    
    # é¡¯ç¤ºæ‰€æœ‰æ´»å‹•æŸ¥è©¢
    active_queries = spark.streams.active
    print(f"æ´»å‹•æŸ¥è©¢æ•¸é‡: {len(active_queries)}")
    
    # ç­‰å¾…æ‰€æœ‰æŸ¥è©¢å®Œæˆ
    spark.streams.awaitAnyTermination(timeout=5)
    
    # æ¸…ç†è³‡æº
    print("\n1ï¸âƒ£2ï¸âƒ£ æ¸…ç†è³‡æº")
    
    # æ¸…ç†è‡¨æ™‚ç›®éŒ„
    import shutil
    shutil.rmtree(temp_dir)
    print(f"æ¸…ç†è‡¨æ™‚ç›®éŒ„: {temp_dir}")
    
    # åœæ­¢ SparkSession
    spark.stop()
    print("\nâœ… Spark Streaming åŸºç¤ç¤ºç¯„å®Œæˆ")

if __name__ == "__main__":
    main()