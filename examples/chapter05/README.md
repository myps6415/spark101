# ç¬¬5ç« ï¼šSpark Streaming

## ğŸ“š å­¸ç¿’ç›®æ¨™

- ç†è§£æµå¼è™•ç†çš„åŸºæœ¬æ¦‚å¿µ
- æŒæ¡çµæ§‹åŒ–æµè™•ç† (Structured Streaming)
- å­¸æœƒèˆ‡ Kafka ç­‰æ¶ˆæ¯ç³»çµ±çš„æ•´åˆ
- äº†è§£å¯¦æ™‚æ•¸æ“šè™•ç†çš„æœ€ä½³å¯¦è¸

## ğŸ¯ æœ¬ç« å…§å®¹

### æ ¸å¿ƒæ¦‚å¿µ
- **Structured Streaming** - çµæ§‹åŒ–æµè™•ç†
- **Watermark** - æ°´å°æ©Ÿåˆ¶
- **Trigger** - è§¸ç™¼å™¨
- **Output Mode** - è¼¸å‡ºæ¨¡å¼

### æª”æ¡ˆèªªæ˜
- `streaming_basics.py` - æµå¼è™•ç†åŸºç¤
- `kafka_streaming.py` - Kafka æ•´åˆç¤ºä¾‹

## ğŸš€ é–‹å§‹å­¸ç¿’

### åŸ·è¡Œç¯„ä¾‹

```bash
# åŸ·è¡ŒåŸºç¤æµè™•ç†
poetry run python examples/chapter05/streaming_basics.py

# åŸ·è¡Œ Kafka æ•´åˆç¯„ä¾‹
poetry run python examples/chapter05/kafka_streaming.py

# æˆ–ä½¿ç”¨ Makefile
make run-chapter05
```

## ğŸ” æ·±å…¥ç†è§£

### æµè™•ç†æ¶æ§‹

```
æ•¸æ“šæº â†’ æµè™•ç†å¼•æ“ â†’ è¼¸å‡ºæ¥æ”¶å™¨
  â†“         â†“           â†“
File     Structured   Console
Kafka    Streaming    File
Socket     Engine     Memory
```

### åŸºæœ¬æµè™•ç†æ¨¡å¼

#### 1. æ•¸æ“šæ”å–
```python
# å¾æ–‡ä»¶è®€å–æµ
streaming_df = spark.readStream \
    .format("json") \
    .schema(schema) \
    .option("path", input_path) \
    .load()

# å¾ Kafka è®€å–æµ
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "topic_name") \
    .load()
```

#### 2. æµè™•ç†
```python
# åŸºæœ¬è½‰æ›
processed_df = streaming_df \
    .filter(col("status") == "active") \
    .withColumn("processed_time", current_timestamp())

# èšåˆæ“ä½œ
aggregated_df = streaming_df \
    .groupBy("category") \
    .agg(count("*").alias("count"))
```

#### 3. è¼¸å‡º
```python
# è¼¸å‡ºåˆ°æ§åˆ¶å°
query = processed_df.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

# è¼¸å‡ºåˆ°æ–‡ä»¶
query = processed_df.writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", output_path) \
    .start()
```

### æ™‚é–“è¦–çª—æ“ä½œ

#### 1. æ»¾å‹•è¦–çª—
```python
# æ¯5åˆ†é˜çµ±è¨ˆä¸€æ¬¡
windowed_df = streaming_df \
    .groupBy(window(col("timestamp"), "5 minutes")) \
    .agg(count("*").alias("count"))
```

#### 2. æ»‘å‹•è¦–çª—
```python
# æ¯2åˆ†é˜çµ±è¨ˆéå»10åˆ†é˜çš„æ•¸æ“š
windowed_df = streaming_df \
    .groupBy(window(col("timestamp"), "10 minutes", "2 minutes")) \
    .agg(count("*").alias("count"))
```

#### 3. æ°´å°è™•ç†
```python
# è™•ç†å»¶é²æ•¸æ“š
watermarked_df = streaming_df \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(window(col("timestamp"), "5 minutes")) \
    .agg(count("*").alias("count"))
```

## ğŸ›ï¸ è¼¸å‡ºæ¨¡å¼

### 1. Append Mode
- åªè¼¸å‡ºæ–°å¢çš„è¡Œ
- é©ç”¨æ–¼ä¸å¯è®Šæ•¸æ“š

### 2. Update Mode
- è¼¸å‡ºæ›´æ–°çš„è¡Œ
- é©ç”¨æ–¼èšåˆæ“ä½œ

### 3. Complete Mode
- è¼¸å‡ºå®Œæ•´çµæœ
- é©ç”¨æ–¼å°å‹èšåˆçµæœ

## ğŸ”§ è§¸ç™¼å™¨é…ç½®

### 1. è™•ç†æ™‚é–“è§¸ç™¼å™¨
```python
# æ¯10ç§’è§¸ç™¼ä¸€æ¬¡
.trigger(processingTime='10 seconds')
```

### 2. ä¸€æ¬¡æ€§è§¸ç™¼å™¨
```python
# åªåŸ·è¡Œä¸€æ¬¡
.trigger(once=True)
```

### 3. é€£çºŒè§¸ç™¼å™¨
```python
# é€£çºŒè™•ç†
.trigger(continuous='1 second')
```

## ğŸ“Š Kafka æ•´åˆ

### 1. å¾ Kafka è®€å–
```python
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "input_topic") \
    .option("startingOffsets", "latest") \
    .load()

# è§£æ JSON æ•¸æ“š
parsed_df = kafka_df.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")
```

### 2. å¯«å…¥ Kafka
```python
query = processed_df.select(
    to_json(struct(*df.columns)).alias("value")
).writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "output_topic") \
    .start()
```

## ğŸ› ï¸ ç‹€æ…‹ç®¡ç†

### 1. æœ‰ç‹€æ…‹æ“ä½œ
```python
# ç´¯ç©çµ±è¨ˆ
cumulative_df = streaming_df \
    .groupBy("user_id") \
    .agg(
        count("*").alias("total_events"),
        sum("amount").alias("total_amount")
    )
```

### 2. å»é‡æ“ä½œ
```python
# åŸºæ–¼éµå»é‡
deduped_df = streaming_df \
    .withWatermark("timestamp", "1 hour") \
    .dropDuplicates(["user_id", "event_id"])
```

## ğŸ“ ç·´ç¿’å»ºè­°

### åŸºç¤ç·´ç¿’
1. å‰µå»ºç°¡å–®çš„æµè™•ç†æ‡‰ç”¨
2. å˜—è©¦ä¸åŒçš„è¼¸å‡ºæ¨¡å¼
3. å¯¦é©—æ™‚é–“è¦–çª—æ“ä½œ

### é€²éšç·´ç¿’
1. æ•´åˆ Kafka é€²è¡Œç«¯åˆ°ç«¯æµè™•ç†
2. å¯¦ç¾è¤‡é›œçš„ç‹€æ…‹ç®¡ç†
3. è™•ç†å»¶é²æ•¸æ“šå’Œæ°´å°

## ğŸ› ï¸ ç›£æ§å’Œèª¿è©¦

### 1. æŸ¥è©¢ç‹€æ…‹
```python
# æŸ¥çœ‹æŸ¥è©¢ç‹€æ…‹
query.status

# æŸ¥çœ‹è™•ç†é€²åº¦
query.lastProgress
```

### 2. æ€§èƒ½ç›£æ§
```python
# ç›£æ§è™•ç†é€Ÿåº¦
query.lastProgress['inputRowsPerSecond']
query.lastProgress['processedRowsPerSecond']
```

### 3. éŒ¯èª¤è™•ç†
```python
# è¨­ç½®æŸ¥è©¢ç›£è½å™¨
def on_query_progress(event):
    print(f"Batch: {event.progress.batchId}")
    print(f"Records: {event.progress.numInputRows}")

query.awaitTermination()
```

## ğŸ”§ ç–‘é›£æ’è§£

### å¸¸è¦‹å•é¡Œ

**Q: æµè™•ç†æŸ¥è©¢åœæ­¢å·¥ä½œï¼Ÿ**
A: æª¢æŸ¥æª¢æŸ¥é»ç›®éŒ„æ˜¯å¦æœ‰æ¬Šé™å•é¡Œï¼Œç¢ºä¿éŒ¯èª¤è™•ç†æ©Ÿåˆ¶æ­£å¸¸ã€‚

**Q: è™•ç†å»¶é²éé«˜ï¼Ÿ**
A: èª¿æ•´è§¸ç™¼å™¨é–“éš”ï¼Œå„ªåŒ–æŸ¥è©¢é‚è¼¯ï¼Œå¢åŠ ä¸¦è¡Œåº¦ã€‚

**Q: è¨˜æ†¶é«”ä½¿ç”¨éé«˜ï¼Ÿ**
A: é…ç½®é©ç•¶çš„æ°´å°ï¼Œé¿å…ç‹€æ…‹ç„¡é™å¢é•·ã€‚

## ğŸ’¡ æœ€ä½³å¯¦è¸

1. **è¨­ç½®é©ç•¶çš„æ°´å°** - è™•ç†å»¶é²æ•¸æ“š
2. **é¸æ“‡åˆé©çš„è§¸ç™¼å™¨** - å¹³è¡¡å»¶é²å’Œååé‡
3. **ç›£æ§æŸ¥è©¢æ€§èƒ½** - å®šæœŸæª¢æŸ¥è™•ç†æŒ‡æ¨™
4. **éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶** - å¯¦ç¾å®¹éŒ¯è™•ç†
5. **è³‡æºç®¡ç†** - é©ç•¶é…ç½®åŸ·è¡Œå™¨è³‡æº

## ğŸ“– ç›¸é—œæ–‡æª”

- [Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Streaming + Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)
- [Streaming Query Management](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/streaming.html)

## â¡ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬ç« å¾Œï¼Œå»ºè­°ç¹¼çºŒå­¸ç¿’ï¼š
- [ç¬¬6ç« ï¼šMLlib æ©Ÿå™¨å­¸ç¿’](../chapter06/README.md)
- äº†è§£æ©Ÿå™¨å­¸ç¿’åœ¨ Spark ä¸­çš„æ‡‰ç”¨
- å­¸ç¿’ç‰¹å¾µå·¥ç¨‹å’Œæ¨¡å‹è¨“ç·´

## ğŸ¯ å­¸ç¿’æª¢æ ¸

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œä½ æ‡‰è©²èƒ½å¤ ï¼š
- [ ] ç†è§£æµå¼è™•ç†çš„åŸºæœ¬æ¦‚å¿µ
- [ ] å‰µå»ºå’Œç®¡ç†æµè™•ç†æŸ¥è©¢
- [ ] ä½¿ç”¨æ™‚é–“è¦–çª—é€²è¡Œèšåˆ
- [ ] æ•´åˆ Kafka é€²è¡Œå¯¦æ™‚æ•¸æ“šè™•ç†
- [ ] è™•ç†å»¶é²æ•¸æ“šå’Œç‹€æ…‹ç®¡ç†

## ğŸ—‚ï¸ ç« ç¯€æ–‡ä»¶ç¸½è¦½

### streaming_basics.py
- çµæ§‹åŒ–æµè™•ç†åŸºç¤
- æ™‚é–“è¦–çª—æ“ä½œ
- ä¸åŒè¼¸å‡ºæ¨¡å¼
- ç‹€æ…‹ç®¡ç†ç¤ºä¾‹

### kafka_streaming.py
- Kafka æ•´åˆé…ç½®
- å¯¦æ™‚äº‹ä»¶è™•ç†
- ç«¯åˆ°ç«¯æµè™•ç†ç®¡é“
- ç›£æ§å’Œè­¦å ±ç³»çµ±

## ğŸ“‹ ç’°å¢ƒæº–å‚™

### Kafka è¨­ç½®ï¼ˆå¯é¸ï¼‰
```bash
# ä¸‹è¼‰ Kafka
wget https://downloads.apache.org/kafka/2.8.0/kafka_2.13-2.8.0.tgz

# å•Ÿå‹• Kafka
bin/kafka-server-start.sh config/server.properties

# å‰µå»ºä¸»é¡Œ
bin/kafka-topics.sh --create --topic input_topic --bootstrap-server localhost:9092
```

### ä¾è³´åŒ…
```bash
# å®‰è£ Kafka å®¢æˆ¶ç«¯
pip install kafka-python

# æˆ–ä½¿ç”¨ Poetry
poetry add kafka-python
```