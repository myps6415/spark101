#!/usr/bin/env python3
"""
ç¬¬5ç« ï¼šSpark Streaming - Kafka æ•´åˆ
å­¸ç¿’å¦‚ä½•èˆ‡ Kafka æ•´åˆé€²è¡Œå¯¦æ™‚æµè™•ç†
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
from pyspark.sql.functions import (
    col, from_json, to_json, struct, current_timestamp, 
    window, count, sum as spark_sum, avg, max as spark_max, min as spark_min
)
import json
import time
import threading
from datetime import datetime, timedelta

def create_sample_kafka_data():
    """å‰µå»ºç¤ºä¾‹ Kafka æ¶ˆæ¯æ•¸æ“š"""
    import random
    
    # æ¨¡æ“¬ä¸åŒé¡å‹çš„äº‹ä»¶
    event_types = ["user_login", "page_view", "purchase", "logout"]
    users = ["user_001", "user_002", "user_003", "user_004", "user_005"]
    pages = ["home", "product", "cart", "checkout", "profile"]
    products = ["laptop", "phone", "tablet", "watch", "headphones"]
    
    events = []
    
    for i in range(100):
        event_type = random.choice(event_types)
        
        base_event = {
            "event_id": f"evt_{i:05d}",
            "event_type": event_type,
            "user_id": random.choice(users),
            "timestamp": (datetime.now() - timedelta(minutes=random.randint(0, 60))).isoformat(),
            "session_id": f"sess_{random.randint(1000, 9999)}"
        }
        
        # æ ¹æ“šäº‹ä»¶é¡å‹æ·»åŠ ç‰¹å®šå­—æ®µ
        if event_type == "page_view":
            base_event.update({
                "page": random.choice(pages),
                "duration": random.randint(10, 300)
            })
        elif event_type == "purchase":
            base_event.update({
                "product": random.choice(products),
                "quantity": random.randint(1, 5),
                "price": round(random.uniform(10, 1000), 2)
            })
        elif event_type == "user_login":
            base_event.update({
                "device": random.choice(["desktop", "mobile", "tablet"]),
                "location": random.choice(["US", "UK", "CA", "DE", "JP"])
            })
        
        events.append(base_event)
    
    return events

def main():
    # å‰µå»º SparkSession
    spark = SparkSession.builder \
        .appName("Kafka Streaming") \
        .master("local[*]") \
        .config("spark.sql.streaming.checkpointLocation", "/tmp/kafka_checkpoint") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print("ğŸŒŠ Kafka Streaming æ•´åˆç¤ºç¯„")
    print("=" * 40)
    
    # æ³¨æ„ï¼šé€™å€‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Kafkaï¼Œä½†éœ€è¦å¯¦éš›çš„ Kafka æœå‹™å™¨
    # åœ¨å¯¦éš›ä½¿ç”¨ä¸­ï¼Œæ‚¨éœ€è¦ï¼š
    # 1. å•Ÿå‹• Kafka æœå‹™å™¨
    # 2. å‰µå»ºä¸»é¡Œ
    # 3. é…ç½®æ­£ç¢ºçš„ Kafka ä¼ºæœå™¨åœ°å€
    
    print("âš ï¸  æ³¨æ„ï¼šæ­¤ç¤ºä¾‹éœ€è¦é‹è¡Œä¸­çš„ Kafka æœå‹™å™¨")
    print("å¦‚æœæ²’æœ‰ Kafka æœå‹™å™¨ï¼Œè«‹åƒè€ƒæ–‡æª”è¨­ç½®æœ¬åœ° Kafka ç’°å¢ƒ")
    
    # 1. å®šç¾©æ¶ˆæ¯çµæ§‹
    print("\n1ï¸âƒ£ å®šç¾©æ¶ˆæ¯çµæ§‹")
    
    # åŸºæœ¬äº‹ä»¶çµæ§‹
    event_schema = StructType([
        StructField("event_id", StringType(), True),
        StructField("event_type", StringType(), True),
        StructField("user_id", StringType(), True),
        StructField("timestamp", StringType(), True),
        StructField("session_id", StringType(), True),
        StructField("page", StringType(), True),
        StructField("duration", IntegerType(), True),
        StructField("product", StringType(), True),
        StructField("quantity", IntegerType(), True),
        StructField("price", IntegerType(), True),
        StructField("device", StringType(), True),
        StructField("location", StringType(), True)
    ])
    
    print("äº‹ä»¶ Schema:")
    print(event_schema)
    
    # 2. æ¨¡æ“¬ Kafka æµæ•¸æ“šï¼ˆä½¿ç”¨ rate sourceï¼‰
    print("\n2ï¸âƒ£ æ¨¡æ“¬æµæ•¸æ“š")
    
    # ç”±æ–¼æ²’æœ‰å¯¦éš›çš„ Kafkaï¼Œæˆ‘å€‘ä½¿ç”¨ rate source æ¨¡æ“¬
    # åœ¨å¯¦éš›ç’°å¢ƒä¸­ï¼Œé€™è£¡æœƒæ˜¯å¾ Kafka è®€å–
    
    # å‰µå»ºç¤ºä¾‹æ•¸æ“š
    sample_events = create_sample_kafka_data()
    
    # å‰µå»º DataFrame
    events_df = spark.createDataFrame(
        [(json.dumps(event),) for event in sample_events], 
        ["value"]
    )
    
    print("ç¤ºä¾‹äº‹ä»¶æ•¸æ“š:")
    events_df.show(5, truncate=False)
    
    # 3. è§£æ JSON æ¶ˆæ¯
    print("\n3ï¸âƒ£ è§£æ JSON æ¶ˆæ¯")
    
    # è§£æ JSON æ ¼å¼çš„æ¶ˆæ¯
    parsed_df = events_df.select(
        from_json(col("value"), event_schema).alias("data")
    ).select("data.*")
    
    print("è§£æå¾Œçš„æ•¸æ“š:")
    parsed_df.show(5, truncate=False)
    
    # 4. å¯¦æ™‚äº‹ä»¶çµ±è¨ˆ
    print("\n4ï¸âƒ£ å¯¦æ™‚äº‹ä»¶çµ±è¨ˆ")
    
    # æŒ‰äº‹ä»¶é¡å‹çµ±è¨ˆ
    event_stats = parsed_df.groupBy("event_type") \
        .agg(
            count("*").alias("event_count"),
            spark_sum("quantity").alias("total_quantity"),
            avg("price").alias("avg_price")
        )
    
    print("äº‹ä»¶çµ±è¨ˆ:")
    event_stats.show()
    
    # 5. ç”¨æˆ¶è¡Œç‚ºåˆ†æ
    print("\n5ï¸âƒ£ ç”¨æˆ¶è¡Œç‚ºåˆ†æ")
    
    # ç”¨æˆ¶æœƒè©±çµ±è¨ˆ
    user_sessions = parsed_df.groupBy("user_id", "session_id") \
        .agg(
            count("*").alias("events_in_session"),
            spark_sum("duration").alias("total_duration"),
            spark_sum("quantity").alias("items_purchased"),
            spark_sum("price").alias("total_spent")
        )
    
    print("ç”¨æˆ¶æœƒè©±çµ±è¨ˆ:")
    user_sessions.show()
    
    # 6. æ™‚é–“çª—å£åˆ†æ
    print("\n6ï¸âƒ£ æ™‚é–“çª—å£åˆ†æ")
    
    # è½‰æ›æ™‚é–“æˆ³
    from pyspark.sql.functions import to_timestamp
    
    windowed_df = parsed_df.withColumn(
        "event_time", 
        to_timestamp(col("timestamp"))
    )
    
    # 10åˆ†é˜çª—å£çš„äº‹ä»¶çµ±è¨ˆ
    windowed_stats = windowed_df \
        .groupBy(
            window(col("event_time"), "10 minutes"),
            col("event_type")
        ) \
        .agg(
            count("*").alias("event_count"),
            spark_sum("quantity").alias("total_quantity")
        )
    
    print("æ™‚é–“çª—å£çµ±è¨ˆ:")
    windowed_stats.show(truncate=False)
    
    # 7. å¯¦æ™‚ç•°å¸¸æª¢æ¸¬
    print("\n7ï¸âƒ£ å¯¦æ™‚ç•°å¸¸æª¢æ¸¬")
    
    # æª¢æ¸¬ç•°å¸¸è³¼è²·è¡Œç‚º
    anomalies = parsed_df.filter(
        (col("event_type") == "purchase") &
        ((col("quantity") > 10) | (col("price") > 500))
    )
    
    print("ç•°å¸¸è³¼è²·è¡Œç‚º:")
    anomalies.show()
    
    # 8. å¯¦æ™‚æ¨è–¦ç³»çµ±æ•¸æ“š
    print("\n8ï¸âƒ£ å¯¦æ™‚æ¨è–¦ç³»çµ±æ•¸æ“š")
    
    # ç”¢å“ç€è¦½çµ±è¨ˆ
    product_views = parsed_df.filter(col("event_type") == "page_view") \
        .filter(col("page") == "product") \
        .groupBy("user_id") \
        .agg(
            count("*").alias("product_views"),
            avg("duration").alias("avg_view_duration")
        )
    
    print("ç”¢å“ç€è¦½çµ±è¨ˆ:")
    product_views.show()
    
    # 9. å¤šå±¤æ¬¡èšåˆ
    print("\n9ï¸âƒ£ å¤šå±¤æ¬¡èšåˆ")
    
    # åœ°å€å’Œè¨­å‚™çµ±è¨ˆ
    location_device_stats = parsed_df.filter(col("event_type") == "user_login") \
        .groupBy("location", "device") \
        .agg(
            count("*").alias("login_count"),
            col("location").alias("region"),
            col("device").alias("device_type")
        )
    
    print("åœ°å€å’Œè¨­å‚™çµ±è¨ˆ:")
    location_device_stats.show()
    
    # 10. å¯¦æ™‚ A/B æ¸¬è©¦åˆ†æ
    print("\nğŸ”Ÿ å¯¦æ™‚ A/B æ¸¬è©¦åˆ†æ")
    
    # æ¨¡æ“¬ A/B æ¸¬è©¦æ•¸æ“š
    from pyspark.sql.functions import when, rand
    
    ab_test_df = parsed_df.withColumn(
        "test_group",
        when(rand() > 0.5, "A").otherwise("B")
    )
    
    # A/B æ¸¬è©¦è½‰æ›ç‡
    ab_conversion = ab_test_df.groupBy("test_group") \
        .agg(
            count("*").alias("total_events"),
            spark_sum(when(col("event_type") == "purchase", 1).otherwise(0)).alias("purchases"),
            (spark_sum(when(col("event_type") == "purchase", 1).otherwise(0)) * 100.0 / count("*")).alias("conversion_rate")
        )
    
    print("A/B æ¸¬è©¦è½‰æ›ç‡:")
    ab_conversion.show()
    
    # 11. å¯¦æ™‚å„€è¡¨æ¿æ•¸æ“š
    print("\n1ï¸âƒ£1ï¸âƒ£ å¯¦æ™‚å„€è¡¨æ¿æ•¸æ“š")
    
    # å‰µå»ºå¯¦æ™‚å„€è¡¨æ¿éœ€è¦çš„æŒ‡æ¨™
    dashboard_metrics = parsed_df.agg(
        count("*").alias("total_events"),
        spark_sum(when(col("event_type") == "user_login", 1).otherwise(0)).alias("total_logins"),
        spark_sum(when(col("event_type") == "page_view", 1).otherwise(0)).alias("total_page_views"),
        spark_sum(when(col("event_type") == "purchase", 1).otherwise(0)).alias("total_purchases"),
        spark_sum("price").alias("total_revenue"),
        avg("price").alias("avg_order_value")
    )
    
    print("å„€è¡¨æ¿æŒ‡æ¨™:")
    dashboard_metrics.show()
    
    # 12. å¯¦æ™‚è­¦å ±ç³»çµ±
    print("\n1ï¸âƒ£2ï¸âƒ£ å¯¦æ™‚è­¦å ±ç³»çµ±")
    
    # æª¢æ¸¬éœ€è¦è­¦å ±çš„æƒ…æ³
    alerts = parsed_df.filter(
        (col("event_type") == "purchase") & (col("price") > 800)
    ).select(
        col("event_id"),
        col("user_id"),
        col("price"),
        col("timestamp"),
        lit("HIGH_VALUE_PURCHASE").alias("alert_type")
    )
    
    from pyspark.sql.functions import lit
    print("é«˜åƒ¹å€¼è³¼è²·è­¦å ±:")
    alerts.show()
    
    # 13. è³‡æ–™ç®¡é“è¼¸å‡ºæ ¼å¼
    print("\n1ï¸âƒ£3ï¸âƒ£ è³‡æ–™ç®¡é“è¼¸å‡ºæ ¼å¼")
    
    # æº–å‚™è¼¸å‡ºåˆ°ä¸‹æ¸¸ç³»çµ±çš„æ•¸æ“š
    output_format = parsed_df.select(
        to_json(struct(
            col("event_id"),
            col("event_type"),
            col("user_id"),
            col("timestamp"),
            current_timestamp().alias("processed_at")
        )).alias("message_value")
    )
    
    print("è¼¸å‡ºæ ¼å¼:")
    output_format.show(5, truncate=False)
    
    # 14. å¯¦éš› Kafka é…ç½®ç¤ºä¾‹
    print("\n1ï¸âƒ£4ï¸âƒ£ å¯¦éš› Kafka é…ç½®ç¤ºä¾‹")
    
    print("""
    # å¾ Kafka è®€å–çš„å¯¦éš›é…ç½®ï¼š
    kafka_df = spark.readStream \\
        .format("kafka") \\
        .option("kafka.bootstrap.servers", "localhost:9092") \\
        .option("subscribe", "user_events") \\
        .option("startingOffsets", "latest") \\
        .load()
    
    # å¯«å…¥åˆ° Kafka çš„å¯¦éš›é…ç½®ï¼š
    query = processed_df.writeStream \\
        .format("kafka") \\
        .option("kafka.bootstrap.servers", "localhost:9092") \\
        .option("topic", "processed_events") \\
        .option("checkpointLocation", "/tmp/kafka_checkpoint") \\
        .start()
    """)
    
    # 15. æ€§èƒ½ç›£æ§
    print("\n1ï¸âƒ£5ï¸âƒ£ æ€§èƒ½ç›£æ§")
    
    # æ¨¡æ“¬æµè™•ç†æ€§èƒ½æŒ‡æ¨™
    performance_metrics = {
        "total_events_processed": parsed_df.count(),
        "processing_time": "æ¨¡æ“¬è™•ç†æ™‚é–“",
        "throughput": "æ¯ç§’è™•ç†äº‹ä»¶æ•¸",
        "latency": "ç«¯åˆ°ç«¯å»¶é²",
        "memory_usage": "è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"
    }
    
    print("æ€§èƒ½æŒ‡æ¨™:")
    for metric, value in performance_metrics.items():
        print(f"  {metric}: {value}")
    
    # åœæ­¢ SparkSession
    spark.stop()
    print("\nâœ… Kafka Streaming æ•´åˆç¤ºç¯„å®Œæˆ")
    
    print("\nğŸ“ å¯¦éš›éƒ¨ç½²å»ºè­°:")
    print("1. è¨­ç½® Kafka é›†ç¾¤")
    print("2. é…ç½®é©ç•¶çš„åˆ†å€ç­–ç•¥")
    print("3. ç›£æ§æ¶ˆè²»è€…æ»¯å¾Œ")
    print("4. è¨­ç½®é©ç•¶çš„æª¢æŸ¥é»é–“éš”")
    print("5. èª¿æ•´æ‰¹æ¬¡å¤§å°å’Œè§¸ç™¼é–“éš”")
    print("6. å¯¦æ–½éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶")
    print("7. è¨­ç½®ç›£æ§å’Œè­¦å ±")

if __name__ == "__main__":
    main()