#!/usr/bin/env python3
"""
ç¬¬4ç« ï¼šSpark SQL - é€²éšå‡½æ•¸å’ŒåŠŸèƒ½
å­¸ç¿’ Spark SQL çš„é€²éšå‡½æ•¸ã€è‡ªè¨‚å‡½æ•¸å’Œæ€§èƒ½èª¿å„ª
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, MapType
from pyspark.sql.functions import udf, col, explode, collect_list, struct, map_values, map_keys
import json
from datetime import datetime

def main():
    # å‰µå»º SparkSession
    spark = SparkSession.builder \
        .appName("Advanced SQL Functions") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    
    print("ğŸš€ Spark SQL é€²éšå‡½æ•¸ç¤ºç¯„")
    print("=" * 40)
    
    # 1. æº–å‚™è¤‡é›œæ•¸æ“šçµæ§‹
    print("\n1ï¸âƒ£ è¤‡é›œæ•¸æ“šçµæ§‹")
    
    # åŒ…å«åµŒå¥—çµæ§‹çš„æ•¸æ“š
    complex_data = [
        {
            "id": 1,
            "name": "Alice",
            "skills": ["Python", "Spark", "SQL"],
            "projects": [
                {"name": "Project A", "status": "completed", "budget": 50000},
                {"name": "Project B", "status": "in_progress", "budget": 75000}
            ],
            "metadata": {"level": "Senior", "location": "NYC", "remote": True}
        },
        {
            "id": 2,
            "name": "Bob",
            "skills": ["Java", "Scala", "Kafka"],
            "projects": [
                {"name": "Project C", "status": "completed", "budget": 60000},
                {"name": "Project D", "status": "planning", "budget": 80000}
            ],
            "metadata": {"level": "Lead", "location": "SF", "remote": False}
        },
        {
            "id": 3,
            "name": "Charlie",
            "skills": ["JavaScript", "React", "Node.js"],
            "projects": [
                {"name": "Project E", "status": "in_progress", "budget": 45000}
            ],
            "metadata": {"level": "Junior", "location": "LA", "remote": True}
        }
    ]
    
    # å¾JSONå‰µå»ºDataFrame
    df = spark.read.json(spark.sparkContext.parallelize([json.dumps(record) for record in complex_data]))
    
    print("è¤‡é›œæ•¸æ“šçµæ§‹:")
    df.show(truncate=False)
    df.printSchema()
    
    # å‰µå»ºè‡¨æ™‚è¦–åœ–
    df.createOrReplaceTempView("employees")
    
    # 2. é™£åˆ—å‡½æ•¸
    print("\n2ï¸âƒ£ é™£åˆ—å‡½æ•¸")
    
    # é™£åˆ—åŸºæœ¬æ“ä½œ
    result = spark.sql("""
        SELECT name,
               skills,
               SIZE(skills) as skill_count,
               ARRAY_CONTAINS(skills, 'Python') as knows_python,
               skills[0] as first_skill
        FROM employees
    """)
    print("é™£åˆ—åŸºæœ¬æ“ä½œ:")
    result.show(truncate=False)
    
    # é™£åˆ—å±•é–‹
    result = spark.sql("""
        SELECT name, explode(skills) as skill
        FROM employees
    """)
    print("é™£åˆ—å±•é–‹:")
    result.show()
    
    # é™£åˆ—èšåˆ
    result = spark.sql("""
        SELECT explode(skills) as skill,
               COUNT(*) as employee_count
        FROM employees
        GROUP BY skill
        ORDER BY employee_count DESC
    """)
    print("æŠ€èƒ½çµ±è¨ˆ:")
    result.show()
    
    # 3. çµæ§‹é«”å‡½æ•¸
    print("\n3ï¸âƒ£ çµæ§‹é«”å‡½æ•¸")
    
    # å­˜å–çµæ§‹é«”æ¬„ä½
    result = spark.sql("""
        SELECT name,
               metadata.level as level,
               metadata.location as location,
               metadata.remote as is_remote
        FROM employees
    """)
    print("çµæ§‹é«”æ¬„ä½å­˜å–:")
    result.show()
    
    # å±•é–‹çµæ§‹é«”
    result = spark.sql("""
        SELECT name, explode(projects) as project
        FROM employees
    """)
    print("å±•é–‹é …ç›®çµæ§‹é«”:")
    result.show(truncate=False)
    
    # å­˜å–åµŒå¥—çµæ§‹
    result = spark.sql("""
        SELECT name,
               project.name as project_name,
               project.status as project_status,
               project.budget as project_budget
        FROM (
            SELECT name, explode(projects) as project
            FROM employees
        )
    """)
    print("é …ç›®è©³æƒ…:")
    result.show()
    
    # 4. è‡ªè¨‚å‡½æ•¸ (UDF)
    print("\n4ï¸âƒ£ è‡ªè¨‚å‡½æ•¸ (UDF)")
    
    # å®šç¾©Python UDF
    def calculate_skill_score(skills):
        """æ ¹æ“šæŠ€èƒ½è¨ˆç®—åˆ†æ•¸"""
        score_map = {
            "Python": 10, "Spark": 15, "SQL": 8,
            "Java": 12, "Scala": 14, "Kafka": 10,
            "JavaScript": 8, "React": 7, "Node.js": 6
        }
        return sum(score_map.get(skill, 5) for skill in skills)
    
    def format_employee_info(name, level, location):
        """æ ¼å¼åŒ–å“¡å·¥ä¿¡æ¯"""
        return f"{name} ({level}) - {location}"
    
    # è¨»å†ŠUDF
    spark.udf.register("calculate_skill_score", calculate_skill_score, IntegerType())
    spark.udf.register("format_employee_info", format_employee_info, StringType())
    
    # ä½¿ç”¨UDF
    result = spark.sql("""
        SELECT name,
               skills,
               calculate_skill_score(skills) as skill_score,
               format_employee_info(name, metadata.level, metadata.location) as employee_info
        FROM employees
    """)
    print("ä½¿ç”¨è‡ªè¨‚å‡½æ•¸:")
    result.show(truncate=False)
    
    # 5. è¦–çª—å‡½æ•¸é€²éšæ‡‰ç”¨
    print("\n5ï¸âƒ£ è¦–çª—å‡½æ•¸é€²éšæ‡‰ç”¨")
    
    # å‰µå»ºæ›´å¤šç¯„ä¾‹æ•¸æ“š
    sales_data = [
        ("Alice", "Q1", 2024, 15000),
        ("Alice", "Q2", 2024, 18000),
        ("Alice", "Q3", 2024, 22000),
        ("Alice", "Q4", 2024, 25000),
        ("Bob", "Q1", 2024, 12000),
        ("Bob", "Q2", 2024, 16000),
        ("Bob", "Q3", 2024, 19000),
        ("Bob", "Q4", 2024, 21000),
        ("Charlie", "Q1", 2024, 10000),
        ("Charlie", "Q2", 2024, 13000),
        ("Charlie", "Q3", 2024, 15000),
        ("Charlie", "Q4", 2024, 18000)
    ]
    
    sales_df = spark.createDataFrame(sales_data, ["name", "quarter", "year", "sales"])
    sales_df.createOrReplaceTempView("sales")
    
    print("éŠ·å”®æ•¸æ“š:")
    sales_df.show()
    
    # ç§»å‹•å¹³å‡
    result = spark.sql("""
        SELECT name, quarter, sales,
               AVG(sales) OVER (
                   PARTITION BY name 
                   ORDER BY quarter 
                   ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING
               ) as moving_avg,
               sales - LAG(sales, 1) OVER (
                   PARTITION BY name 
                   ORDER BY quarter
               ) as quarter_growth
        FROM sales
    """)
    print("ç§»å‹•å¹³å‡å’Œå­£åº¦å¢é•·:")
    result.show()
    
    # ç´¯è¨ˆç¸½è¨ˆå’Œç™¾åˆ†æ¯”
    result = spark.sql("""
        SELECT name, quarter, sales,
               SUM(sales) OVER (
                   PARTITION BY name 
                   ORDER BY quarter 
                   ROWS UNBOUNDED PRECEDING
               ) as cumulative_sales,
               ROUND(
                   sales * 100.0 / SUM(sales) OVER (PARTITION BY name),
                   2
               ) as percentage_of_total
        FROM sales
    """)
    print("ç´¯è¨ˆéŠ·å”®å’Œç™¾åˆ†æ¯”:")
    result.show()
    
    # 6. æ­£å‰‡è¡¨é”å¼å’Œå­—ä¸²è™•ç†
    print("\n6ï¸âƒ£ æ­£å‰‡è¡¨é”å¼å’Œå­—ä¸²è™•ç†")
    
    # å‰µå»ºåŒ…å«æ–‡æœ¬æ•¸æ“šçš„æ¨£æœ¬
    text_data = [
        ("user001", "Alice Johnson", "alice.johnson@company.com", "Phone: +1-555-0123"),
        ("user002", "Bob Smith Jr.", "bob.smith@company.com", "Phone: +1-555-0456"),
        ("user003", "Charlie Brown", "charlie.brown@company.com", "Phone: +1-555-0789")
    ]
    
    text_df = spark.createDataFrame(text_data, ["user_id", "full_name", "email", "contact"])
    text_df.createOrReplaceTempView("users")
    
    # æ­£å‰‡è¡¨é”å¼æå–
    result = spark.sql("""
        SELECT user_id,
               full_name,
               email,
               REGEXP_EXTRACT(full_name, '^([A-Za-z]+)', 1) as first_name,
               REGEXP_EXTRACT(full_name, '([A-Za-z]+)$', 1) as last_name,
               REGEXP_EXTRACT(email, '([^@]+)@', 1) as username,
               REGEXP_EXTRACT(contact, '\\+(\\d+-\\d+-\\d+)', 1) as phone_number
        FROM users
    """)
    print("æ­£å‰‡è¡¨é”å¼æå–:")
    result.show()
    
    # å­—ä¸²æ›¿æ›å’Œæ ¼å¼åŒ–
    result = spark.sql("""
        SELECT user_id,
               UPPER(REGEXP_REPLACE(full_name, '\\s+', '_')) as username_format,
               REGEXP_REPLACE(email, '@.*', '@newdomain.com') as new_email,
               SPLIT(full_name, ' ')[0] as first_name_split
        FROM users
    """)
    print("å­—ä¸²æ›¿æ›å’Œæ ¼å¼åŒ–:")
    result.show()
    
    # 7. æ¢ä»¶é‚è¼¯å’ŒCASEè¡¨é”å¼
    print("\n7ï¸âƒ£ æ¢ä»¶é‚è¼¯å’ŒCASEè¡¨é”å¼")
    
    # è¤‡é›œæ¢ä»¶åˆ†é¡
    result = spark.sql("""
        SELECT name,
               SIZE(skills) as skill_count,
               calculate_skill_score(skills) as skill_score,
               metadata.level as current_level,
               CASE
                   WHEN calculate_skill_score(skills) >= 40 THEN 'Expert'
                   WHEN calculate_skill_score(skills) >= 25 THEN 'Advanced'
                   WHEN calculate_skill_score(skills) >= 15 THEN 'Intermediate'
                   ELSE 'Beginner'
               END as skill_level_assessment,
               CASE
                   WHEN metadata.remote = true AND SIZE(skills) > 2 THEN 'Remote Expert'
                   WHEN metadata.remote = true THEN 'Remote Worker'
                   WHEN SIZE(skills) > 2 THEN 'Office Expert'
                   ELSE 'Office Worker'
               END as work_classification
        FROM employees
    """)
    print("è¤‡é›œæ¢ä»¶åˆ†é¡:")
    result.show()
    
    # 8. æ—¥æœŸæ™‚é–“å‡½æ•¸
    print("\n8ï¸âƒ£ æ—¥æœŸæ™‚é–“å‡½æ•¸")
    
    # æ·»åŠ ç•¶å‰æ™‚é–“æˆ³
    result = spark.sql("""
        SELECT name,
               CURRENT_TIMESTAMP() as current_time,
               CURRENT_DATE() as current_date,
               DATE_FORMAT(CURRENT_TIMESTAMP(), 'yyyy-MM-dd HH:mm:ss') as formatted_time,
               UNIX_TIMESTAMP() as unix_timestamp,
               FROM_UNIXTIME(UNIX_TIMESTAMP(), 'yyyy-MM-dd') as date_from_unix
        FROM employees
    """)
    print("æ—¥æœŸæ™‚é–“å‡½æ•¸:")
    result.show()
    
    # 9. æ•¸æ“šé€è¦–å’Œé€†é€è¦–
    print("\n9ï¸âƒ£ æ•¸æ“šé€è¦–")
    
    # é€è¦–éŠ·å”®æ•¸æ“š
    result = spark.sql("""
        SELECT name,
               SUM(CASE WHEN quarter = 'Q1' THEN sales END) as Q1_sales,
               SUM(CASE WHEN quarter = 'Q2' THEN sales END) as Q2_sales,
               SUM(CASE WHEN quarter = 'Q3' THEN sales END) as Q3_sales,
               SUM(CASE WHEN quarter = 'Q4' THEN sales END) as Q4_sales,
               SUM(sales) as total_sales
        FROM sales
        GROUP BY name
    """)
    print("éŠ·å”®æ•¸æ“šé€è¦–:")
    result.show()
    
    # 10. æ€§èƒ½èª¿å„ªç›¸é—œæŸ¥è©¢
    print("\nğŸ”Ÿ æ€§èƒ½èª¿å„ªç›¸é—œæŸ¥è©¢")
    
    # æŸ¥è©¢åŸ·è¡Œè¨ˆåŠƒ
    result = spark.sql("SELECT * FROM employees WHERE SIZE(skills) > 2")
    print("æŸ¥è©¢åŸ·è¡Œè¨ˆåŠƒ:")
    result.explain(True)
    
    # å¿«å–è¡¨æ ¼
    spark.sql("CACHE TABLE employees")
    print("è¡¨æ ¼å·²å¿«å–")
    
    # æŸ¥çœ‹å¿«å–ç‹€æ…‹
    cached_tables = spark.sql("SHOW TABLES").collect()
    print(f"å¿«å–çš„è¡¨æ ¼æ•¸é‡: {len(cached_tables)}")
    
    # 11. çµ±è¨ˆå‡½æ•¸
    print("\n1ï¸âƒ£1ï¸âƒ£ çµ±è¨ˆå‡½æ•¸")
    
    # çµ±è¨ˆåˆ†æ
    result = spark.sql("""
        SELECT 
            COUNT(*) as total_employees,
            AVG(SIZE(skills)) as avg_skills,
            STDDEV(SIZE(skills)) as skills_stddev,
            MIN(SIZE(skills)) as min_skills,
            MAX(SIZE(skills)) as max_skills,
            PERCENTILE_APPROX(SIZE(skills), 0.5) as median_skills,
            PERCENTILE_APPROX(SIZE(skills), 0.75) as q3_skills,
            PERCENTILE_APPROX(SIZE(skills), 0.25) as q1_skills
        FROM employees
    """)
    print("çµ±è¨ˆåˆ†æ:")
    result.show()
    
    # 12. æ•¸æ“šå“è³ªæª¢æŸ¥
    print("\n1ï¸âƒ£2ï¸âƒ£ æ•¸æ“šå“è³ªæª¢æŸ¥")
    
    # æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
    result = spark.sql("""
        SELECT 
            COUNT(*) as total_records,
            COUNT(DISTINCT id) as unique_ids,
            COUNT(CASE WHEN name IS NULL OR name = '' THEN 1 END) as null_names,
            COUNT(CASE WHEN SIZE(skills) = 0 THEN 1 END) as no_skills,
            COUNT(CASE WHEN SIZE(projects) = 0 THEN 1 END) as no_projects,
            AVG(SIZE(skills)) as avg_skills_per_person,
            AVG(SIZE(projects)) as avg_projects_per_person
        FROM employees
    """)
    print("æ•¸æ“šå“è³ªæª¢æŸ¥:")
    result.show()
    
    # é‡è¤‡æ•¸æ“šæª¢æŸ¥
    result = spark.sql("""
        SELECT name, COUNT(*) as count
        FROM employees
        GROUP BY name
        HAVING COUNT(*) > 1
    """)
    print("é‡è¤‡æ•¸æ“šæª¢æŸ¥:")
    result.show()
    
    # å–æ¶ˆå¿«å–
    spark.sql("UNCACHE TABLE employees")
    print("å¿«å–å·²æ¸…é™¤")
    
    # åœæ­¢ SparkSession
    spark.stop()
    print("\nâœ… Spark SQL é€²éšå‡½æ•¸ç¤ºç¯„å®Œæˆ")

if __name__ == "__main__":
    main()