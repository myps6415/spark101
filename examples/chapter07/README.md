# ç¬¬7ç« ï¼šæ€§èƒ½èª¿å„ª

## ğŸ“š å­¸ç¿’ç›®æ¨™

- ç†è§£ Spark æ€§èƒ½å„ªåŒ–çš„æ ¸å¿ƒåŸå‰‡
- æŒæ¡è¨˜æ†¶é«”ç®¡ç†å’Œè³‡æºé…ç½®
- å­¸æœƒåˆ†å€ç­–ç•¥å’Œæ•¸æ“šå‚¾æ–œè™•ç†
- äº†è§£ç›£æ§å’Œèª¿è©¦æŠ€å·§

## ğŸ¯ æœ¬ç« å…§å®¹

### æ ¸å¿ƒæ¦‚å¿µ
- **Performance Tuning** - æ€§èƒ½èª¿å„ª
- **Memory Management** - è¨˜æ†¶é«”ç®¡ç†
- **Data Partitioning** - æ•¸æ“šåˆ†å€
- **Cache Strategy** - ç·©å­˜ç­–ç•¥

### æª”æ¡ˆèªªæ˜
- `performance_tuning.py` - æ€§èƒ½èª¿å„ªå¯¦æˆ°ç¤ºä¾‹

## ğŸš€ é–‹å§‹å­¸ç¿’

### åŸ·è¡Œç¯„ä¾‹

```bash
# åŸ·è¡Œæ€§èƒ½èª¿å„ªç¯„ä¾‹
poetry run python examples/chapter07/performance_tuning.py

# æˆ–ä½¿ç”¨ Makefile
make run-chapter07
```

## ğŸ” æ·±å…¥ç†è§£

### æ€§èƒ½èª¿å„ªæ¡†æ¶

```
æ‡‰ç”¨åˆ†æ â†’ ç“¶é ¸è­˜åˆ¥ â†’ å„ªåŒ–ç­–ç•¥ â†’ æ•ˆæœé©—è­‰
    â†“         â†“         â†“         â†“
  Profile   Bottleneck  Optimize  Measure
```

### ä¸»è¦å„ªåŒ–é ˜åŸŸ

#### 1. è³‡æºé…ç½®
```python
# Spark é…ç½®å„ªåŒ–
spark = SparkSession.builder \
    .appName("Performance Tuning") \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "4") \
    .config("spark.executor.instances", "10") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()
```

#### 2. è¨˜æ†¶é«”ç®¡ç†
```python
# ç·©å­˜ç­–ç•¥
from pyspark import StorageLevel

# è¨˜æ†¶é«”ç·©å­˜
df.cache()

# è¨˜æ†¶é«”+ç£ç¢Ÿç·©å­˜
df.persist(StorageLevel.MEMORY_AND_DISK)

# åºåˆ—åŒ–ç·©å­˜
df.persist(StorageLevel.MEMORY_ONLY_SER)
```

#### 3. åˆ†å€å„ªåŒ–
```python
# é‡æ–°åˆ†å€
df_repartitioned = df.repartition(4)

# æŒ‰åˆ—åˆ†å€
df_partitioned = df.repartition(col("category"))

# åˆä½µåˆ†å€
df_coalesced = df.coalesce(2)
```

## ğŸ› ï¸ é—œéµå„ªåŒ–æŠ€è¡“

### 1. å»£æ’­è®Šæ•¸
```python
from pyspark.sql.functions import broadcast

# å»£æ’­å°è¡¨
result = large_df.join(broadcast(small_df), "key")
```

### 2. è‡ªé©æ‡‰æŸ¥è©¢åŸ·è¡Œ (AQE)
```python
# å•Ÿç”¨ AQE
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

### 3. æ•¸æ“šå‚¾æ–œè™•ç†
```python
# åŠ é¹½æŠ€è¡“
salted_df = df.withColumn("salted_key", 
                         concat(col("key"), lit("_"), (rand() * 10).cast("int")))

# é èšåˆ
pre_aggregated = df.groupBy("key").agg(count("*").alias("count"))
```

## ğŸ“Š ç›£æ§å’Œè¨ºæ–·

### 1. Spark UI ç›£æ§
- **Jobs** - ä½œæ¥­åŸ·è¡Œç‹€æ…‹
- **Stages** - éšæ®µåŸ·è¡Œè©³æƒ…
- **Storage** - ç·©å­˜ä½¿ç”¨æƒ…æ³
- **Executors** - åŸ·è¡Œå™¨è³‡æºä½¿ç”¨

### 2. åŸ·è¡Œè¨ˆåŠƒåˆ†æ
```python
# æŸ¥çœ‹åŸ·è¡Œè¨ˆåŠƒ
df.explain()

# è©³ç´°åŸ·è¡Œè¨ˆåŠƒ
df.explain(True)

# æŸ¥çœ‹æˆæœ¬
df.explain("cost")
```

### 3. æ€§èƒ½æŒ‡æ¨™
```python
# æ¸¬é‡åŸ·è¡Œæ™‚é–“
import time

start_time = time.time()
result = df.count()
execution_time = time.time() - start_time
print(f"åŸ·è¡Œæ™‚é–“: {execution_time:.2f} ç§’")
```

## ğŸ¯ å„ªåŒ–ç­–ç•¥

### 1. æ•¸æ“šæ ¼å¼å„ªåŒ–
```python
# ä½¿ç”¨ Parquet æ ¼å¼
df.write.parquet("optimized_data.parquet")

# å•Ÿç”¨å£“ç¸®
df.write.option("compression", "snappy").parquet("compressed_data.parquet")
```

### 2. æŸ¥è©¢å„ªåŒ–
```python
# è¬‚è©ä¸‹æ¨
df.filter(col("date") >= "2024-01-01").select("id", "name")

# åˆ—ä¿®å‰ª
df.select("id", "name").filter(col("status") == "active")
```

### 3. é€£æ¥å„ªåŒ–
```python
# é¸æ“‡åˆé©çš„é€£æ¥é¡å‹
# å»£æ’­é€£æ¥ - å°è¡¨
df1.join(broadcast(df2), "key")

# æ’åºåˆä½µé€£æ¥ - å¤§è¡¨
df1.join(df2, "key")
```

## ğŸ“ˆ è¨˜æ†¶é«”èª¿å„ª

### 1. è¨˜æ†¶é«”é…ç½®
```python
# åŸ·è¡Œå™¨è¨˜æ†¶é«”
spark.conf.set("spark.executor.memory", "4g")

# é©…å‹•ç¨‹å¼è¨˜æ†¶é«”
spark.conf.set("spark.driver.memory", "2g")

# è¨˜æ†¶é«”åˆ†æ•¸
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB")
```

### 2. åƒåœ¾å›æ”¶å„ªåŒ–
```python
# G1 åƒåœ¾å›æ”¶å™¨
spark.conf.set("spark.executor.extraJavaOptions", "-XX:+UseG1GC")

# å †å¤–è¨˜æ†¶é«”
spark.conf.set("spark.executor.memoryOffHeap.enabled", "true")
spark.conf.set("spark.executor.memoryOffHeap.size", "1g")
```

## ğŸ”§ ç¶²è·¯å’Œ I/O å„ªåŒ–

### 1. åºåˆ—åŒ–å„ªåŒ–
```python
# ä½¿ç”¨ Kryo åºåˆ—åŒ–
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
```

### 2. Shuffle å„ªåŒ–
```python
# Shuffle åˆ†å€æ•¸
spark.conf.set("spark.sql.shuffle.partitions", "400")

# Shuffle å£“ç¸®
spark.conf.set("spark.sql.adaptive.shuffle.targetPostShuffleInputSize", "67108864")
```

## ğŸ“ æ€§èƒ½æ¸¬è©¦

### 1. åŸºæº–æ¸¬è©¦
```python
def benchmark_operation(operation, description):
    start_time = time.time()
    result = operation()
    end_time = time.time()
    print(f"{description}: {end_time - start_time:.2f} ç§’")
    return result

# æ¸¬è©¦ä¸åŒçš„ç·©å­˜ç­–ç•¥
benchmark_operation(lambda: df.count(), "ä¸ä½¿ç”¨ç·©å­˜")
df.cache()
benchmark_operation(lambda: df.count(), "ä½¿ç”¨ç·©å­˜")
```

### 2. è³‡æºä½¿ç”¨ç›£æ§
```python
# ç²å–åŸ·è¡Œå™¨ä¿¡æ¯
executor_infos = spark.sparkContext.statusTracker().getExecutorInfos()
for executor in executor_infos:
    print(f"åŸ·è¡Œå™¨ {executor.executorId}: {executor.memoryUsed}/{executor.maxMemory}")
```

## ğŸ’¡ æœ€ä½³å¯¦è¸

### 1. æ•¸æ“šæ ¼å¼å’Œå£“ç¸®
- ä½¿ç”¨ Parquet æ ¼å¼
- å•Ÿç”¨é©ç•¶çš„å£“ç¸®
- é¿å…å°æ–‡ä»¶å•é¡Œ

### 2. åˆ†å€ç­–ç•¥
- åˆç†è¨­ç½®åˆ†å€æ•¸
- ä½¿ç”¨æ•¸æ“šåˆ†å€
- é¿å…æ•¸æ“šå‚¾æ–œ

### 3. ç·©å­˜ç­–ç•¥
- ç·©å­˜é‡è¤‡ä½¿ç”¨çš„æ•¸æ“š
- é¸æ“‡é©ç•¶çš„å­˜å„²ç´šåˆ¥
- åŠæ™‚æ¸…ç†ä¸éœ€è¦çš„ç·©å­˜

### 4. è³‡æºé…ç½®
- æ ¹æ“šé›†ç¾¤è³‡æºèª¿æ•´é…ç½®
- å¹³è¡¡åŸ·è¡Œå™¨æ•¸é‡å’Œè³‡æº
- ç›£æ§è³‡æºä½¿ç”¨æƒ…æ³

## ğŸ”§ ç–‘é›£æ’è§£

### å¸¸è¦‹å•é¡Œ

**Q: ä½œæ¥­åŸ·è¡Œç·©æ…¢ï¼Ÿ**
A: æª¢æŸ¥åˆ†å€ç­–ç•¥ã€æ•¸æ“šå‚¾æ–œã€è³‡æºé…ç½®ã€‚

**Q: è¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤ï¼Ÿ**
A: å¢åŠ åŸ·è¡Œå™¨è¨˜æ†¶é«”ã€èª¿æ•´åˆ†å€æ•¸ã€å„ªåŒ–æ•¸æ“šçµæ§‹ã€‚

**Q: Shuffle æ“ä½œå¾ˆæ…¢ï¼Ÿ**
A: æ¸›å°‘ Shuffle æ“ä½œã€ä½¿ç”¨å»£æ’­é€£æ¥ã€å„ªåŒ–åˆ†å€ç­–ç•¥ã€‚

## ğŸ“Š æ€§èƒ½èª¿å„ªæª¢æ ¸è¡¨

### é…ç½®æª¢æŸ¥
- [ ] åŸ·è¡Œå™¨è¨˜æ†¶é«”å’Œæ ¸å¿ƒæ•¸é…ç½®åˆç†
- [ ] å•Ÿç”¨è‡ªé©æ‡‰æŸ¥è©¢åŸ·è¡Œ
- [ ] åºåˆ—åŒ–å™¨è¨­ç½®ç‚º Kryo
- [ ] åˆé©çš„ Shuffle åˆ†å€æ•¸

### ç¨‹å¼ç¢¼æª¢æŸ¥
- [ ] ä½¿ç”¨ Parquet æ ¼å¼
- [ ] åˆç†çš„ç·©å­˜ç­–ç•¥
- [ ] é¿å…ä¸å¿…è¦çš„ Shuffle
- [ ] å»£æ’­å°è¡¨é€£æ¥

### ç›£æ§æª¢æŸ¥
- [ ] å®šæœŸæŸ¥çœ‹ Spark UI
- [ ] ç›£æ§è³‡æºä½¿ç”¨æƒ…æ³
- [ ] æª¢æŸ¥åŸ·è¡Œè¨ˆåŠƒ
- [ ] æ¸¬é‡é—œéµæ“ä½œæ€§èƒ½

## ğŸ“– ç›¸é—œæ–‡æª”

- [Tuning Guide](https://spark.apache.org/docs/latest/tuning.html)
- [Configuration](https://spark.apache.org/docs/latest/configuration.html)
- [Monitoring](https://spark.apache.org/docs/latest/monitoring.html)

## â¡ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬ç« å¾Œï¼Œå»ºè­°ç¹¼çºŒå­¸ç¿’ï¼š
- [ç¬¬8ç« ï¼šå¯¦æˆ°é …ç›®](../chapter08/README.md)
- æ‡‰ç”¨æ‰€å­¸çŸ¥è­˜åˆ°å¯¦éš›é …ç›®
- å»ºç«‹å®Œæ•´çš„ç”Ÿç”¢ç´šæ‡‰ç”¨

## ğŸ¯ å­¸ç¿’æª¢æ ¸

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œä½ æ‡‰è©²èƒ½å¤ ï¼š
- [ ] è­˜åˆ¥ Spark æ‡‰ç”¨çš„æ€§èƒ½ç“¶é ¸
- [ ] é…ç½®é©ç•¶çš„è³‡æºå’Œè¨˜æ†¶é«”è¨­ç½®
- [ ] å¯¦æ–½æœ‰æ•ˆçš„åˆ†å€å’Œç·©å­˜ç­–ç•¥
- [ ] è™•ç†æ•¸æ“šå‚¾æ–œå•é¡Œ
- [ ] ç›£æ§å’Œèª¿è©¦ Spark æ‡‰ç”¨

## ğŸ—‚ï¸ ç« ç¯€æ–‡ä»¶ç¸½è¦½

### performance_tuning.py
- åˆ†å€ç­–ç•¥å„ªåŒ–
- ç·©å­˜ç­–ç•¥æ¯”è¼ƒ
- å»£æ’­è®Šæ•¸æ‡‰ç”¨
- æ•¸æ“šå‚¾æ–œè™•ç†
- åŸ·è¡Œè¨ˆåŠƒåˆ†æ
- æ€§èƒ½ç›£æ§æŠ€å·§
- è³‡æºé…ç½®å»ºè­°