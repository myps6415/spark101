#!/usr/bin/env python3
"""
ç¬¬7ç« ï¼šæ€§èƒ½èª¿å„ª - Spark æ€§èƒ½å„ªåŒ–
å­¸ç¿’ Spark çš„æ€§èƒ½èª¿å„ªæŠ€å·§å’Œæœ€ä½³å¯¦è¸
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
from pyspark.sql.functions import col, sum as spark_sum, count, avg, max as spark_max, broadcast
from pyspark.sql.functions import rand, randn, when, lit, monotonically_increasing_id
import time
import tempfile
import os

def measure_execution_time(func, description):
    """æ¸¬é‡å‡½æ•¸åŸ·è¡Œæ™‚é–“"""
    start_time = time.time()
    result = func()
    end_time = time.time()
    execution_time = end_time - start_time
    print(f"{description}: {execution_time:.2f} ç§’")
    return result, execution_time

def create_large_dataset(spark, size=100000):
    """å‰µå»ºå¤§å‹æ¸¬è©¦æ•¸æ“šé›†"""
    return spark.range(size) \
        .withColumn("value", (col("id") * 2).cast("double")) \
        .withColumn("category", (col("id") % 100).cast("string")) \
        .withColumn("random_value", rand(seed=42) * 1000) \
        .withColumn("status", when(col("id") % 5 == 0, "active").otherwise("inactive"))

def main():
    # å‰µå»º SparkSession
    spark = SparkSession.builder \
        .appName("Performance Tuning") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print("âš¡ Spark æ€§èƒ½èª¿å„ªç¤ºç¯„")
    print("=" * 40)
    
    # 1. æ•¸æ“šåˆ†å€ç­–ç•¥
    print("\n1ï¸âƒ£ æ•¸æ“šåˆ†å€ç­–ç•¥")
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    df = create_large_dataset(spark, 1000000)
    
    print(f"åŸå§‹æ•¸æ“šåˆ†å€æ•¸: {df.rdd.getNumPartitions()}")
    print(f"æ•¸æ“šè¡Œæ•¸: {df.count()}")
    
    # é‡æ–°åˆ†å€
    df_repartitioned = df.repartition(4)
    print(f"é‡æ–°åˆ†å€å¾Œåˆ†å€æ•¸: {df_repartitioned.rdd.getNumPartitions()}")
    
    # æŒ‰åˆ—åˆ†å€
    df_partitioned_by_col = df.repartition(col("category"))
    print(f"æŒ‰åˆ—åˆ†å€å¾Œåˆ†å€æ•¸: {df_partitioned_by_col.rdd.getNumPartitions()}")
    
    # åˆä½µåˆ†å€
    df_coalesced = df.coalesce(2)
    print(f"åˆä½µåˆ†å€å¾Œåˆ†å€æ•¸: {df_coalesced.rdd.getNumPartitions()}")
    
    # 2. ç·©å­˜ç­–ç•¥
    print("\n2ï¸âƒ£ ç·©å­˜ç­–ç•¥")
    
    # ä¸ä½¿ç”¨ç·©å­˜
    def without_cache():
        result1 = df.filter(col("status") == "active").count()
        result2 = df.filter(col("status") == "active").agg(avg("value")).collect()
        return result1, result2
    
    _, time_without_cache = measure_execution_time(without_cache, "ä¸ä½¿ç”¨ç·©å­˜")
    
    # ä½¿ç”¨ç·©å­˜
    df_cached = df.cache()
    
    def with_cache():
        result1 = df_cached.filter(col("status") == "active").count()
        result2 = df_cached.filter(col("status") == "active").agg(avg("value")).collect()
        return result1, result2
    
    _, time_with_cache = measure_execution_time(with_cache, "ä½¿ç”¨ç·©å­˜")
    
    print(f"ç·©å­˜æ”¹å–„: {((time_without_cache - time_with_cache) / time_without_cache * 100):.1f}%")
    
    # 3. ä¸åŒçš„ç·©å­˜ç´šåˆ¥
    print("\n3ï¸âƒ£ ä¸åŒçš„ç·©å­˜ç´šåˆ¥")
    
    from pyspark import StorageLevel
    
    # å…§å­˜ç·©å­˜
    df_memory = df.persist(StorageLevel.MEMORY_ONLY)
    
    # å…§å­˜+ç£ç›¤ç·©å­˜
    df_memory_disk = df.persist(StorageLevel.MEMORY_AND_DISK)
    
    # åºåˆ—åŒ–ç·©å­˜
    df_serialized = df.persist(StorageLevel.MEMORY_ONLY_SER)
    
    print("ä¸åŒç·©å­˜ç´šåˆ¥å·²è¨­ç½®")
    
    # 4. å»£æ’­è®Šé‡
    print("\n4ï¸âƒ£ å»£æ’­è®Šé‡")
    
    # å‰µå»ºå°è¡¨
    small_df = spark.createDataFrame([
        ("0", "Category A"),
        ("1", "Category B"),
        ("2", "Category C")
    ], ["id", "name"])
    
    # ä¸ä½¿ç”¨å»£æ’­
    def without_broadcast():
        return df.join(small_df, df.category == small_df.id, "left").count()
    
    _, time_without_broadcast = measure_execution_time(without_broadcast, "ä¸ä½¿ç”¨å»£æ’­")
    
    # ä½¿ç”¨å»£æ’­
    def with_broadcast():
        return df.join(broadcast(small_df), df.category == small_df.id, "left").count()
    
    _, time_with_broadcast = measure_execution_time(with_broadcast, "ä½¿ç”¨å»£æ’­")
    
    print(f"å»£æ’­æ”¹å–„: {((time_without_broadcast - time_with_broadcast) / time_without_broadcast * 100):.1f}%")
    
    # 5. æ•¸æ“šå‚¾æ–œè™•ç†
    print("\n5ï¸âƒ£ æ•¸æ“šå‚¾æ–œè™•ç†")
    
    # å‰µå»ºå‚¾æ–œæ•¸æ“š
    skewed_data = spark.range(100000) \
        .withColumn("key", when(col("id") % 100 == 0, "hot_key").otherwise(col("id").cast("string"))) \
        .withColumn("value", rand() * 1000)
    
    print("å‚¾æ–œæ•¸æ“šåˆ†å¸ƒ:")
    skewed_data.groupBy("key").count().orderBy(col("count").desc()).show(10)
    
    # è™•ç†æ•¸æ“šå‚¾æ–œ - åŠ é¹½
    salted_data = skewed_data.withColumn("salted_key", 
                                       when(col("key") == "hot_key", 
                                            col("key") + "_" + (rand() * 10).cast("int").cast("string"))
                                       .otherwise(col("key")))
    
    print("åŠ é¹½å¾Œçš„æ•¸æ“šåˆ†å¸ƒ:")
    salted_data.groupBy("salted_key").count().orderBy(col("count").desc()).show(10)
    
    # 6. åŸ·è¡Œè¨ˆåŠƒå„ªåŒ–
    print("\n6ï¸âƒ£ åŸ·è¡Œè¨ˆåŠƒå„ªåŒ–")
    
    # æŸ¥çœ‹åŸ·è¡Œè¨ˆåŠƒ
    complex_query = df.filter(col("status") == "active") \
        .groupBy("category") \
        .agg(avg("value").alias("avg_value"), count("*").alias("count")) \
        .filter(col("count") > 100) \
        .orderBy("avg_value")
    
    print("æŸ¥è©¢åŸ·è¡Œè¨ˆåŠƒ:")
    complex_query.explain(True)
    
    # 7. è‡ªé©æ‡‰æŸ¥è©¢åŸ·è¡Œ (AQE)
    print("\n7ï¸âƒ£ è‡ªé©æ‡‰æŸ¥è©¢åŸ·è¡Œ (AQE)")
    
    # å‰µå»ºå…©å€‹å¤§å°ä¸åŒçš„è¡¨
    large_df = create_large_dataset(spark, 100000)
    small_df_for_join = spark.range(1000).withColumn("join_key", col("id").cast("string"))
    
    # AQE æœƒè‡ªå‹•å„ªåŒ–é€™å€‹æŸ¥è©¢
    aqe_query = large_df.join(small_df_for_join, large_df.category == small_df_for_join.join_key, "left")
    
    _, aqe_time = measure_execution_time(lambda: aqe_query.count(), "AQE å„ªåŒ–æŸ¥è©¢")
    
    # 8. åºåˆ—åŒ–å„ªåŒ–
    print("\n8ï¸âƒ£ åºåˆ—åŒ–å„ªåŒ–")
    
    # å‰µå»ºè¤‡é›œå°è±¡
    complex_df = df.withColumn("complex_col", 
                              when(col("id") % 2 == 0, "even_" + col("id").cast("string"))
                              .otherwise("odd_" + col("id").cast("string")))
    
    # æ¸¬è©¦åºåˆ—åŒ–æ€§èƒ½
    def serialization_test():
        return complex_df.rdd.map(lambda x: (x.id, x.complex_col)).take(1000)
    
    _, serialization_time = measure_execution_time(serialization_test, "åºåˆ—åŒ–æ¸¬è©¦")
    
    # 9. å…§å­˜ç®¡ç†
    print("\n9ï¸âƒ£ å…§å­˜ç®¡ç†")
    
    # ç²å–å…§å­˜ä½¿ç”¨æƒ…æ³
    storage_level = df_cached.storageLevel
    print(f"ç·©å­˜å­˜å„²ç´šåˆ¥: {storage_level}")
    
    # æª¢æŸ¥ RDD ç·©å­˜ç‹€æ…‹
    rdd_info = spark.sparkContext.statusTracker().getExecutorInfos()
    for info in rdd_info:
        print(f"åŸ·è¡Œå™¨ {info.executorId}: å…§å­˜ä½¿ç”¨ {info.memoryUsed}/{info.maxMemory}")
    
    # 10. åˆ†å€ä¿®å‰ª
    print("\nğŸ”Ÿ åˆ†å€ä¿®å‰ª")
    
    # å‰µå»ºåˆ†å€æ•¸æ“š
    temp_dir = tempfile.mkdtemp()
    partitioned_path = os.path.join(temp_dir, "partitioned_data")
    
    # æŒ‰ç‹€æ…‹åˆ†å€å¯«å…¥
    df.write.mode("overwrite").partitionBy("status").parquet(partitioned_path)
    
    # è®€å–åˆ†å€æ•¸æ“š
    partitioned_df = spark.read.parquet(partitioned_path)
    
    # åˆ†å€ä¿®å‰ªæŸ¥è©¢
    def partition_pruning():
        return partitioned_df.filter(col("status") == "active").count()
    
    _, pruning_time = measure_execution_time(partition_pruning, "åˆ†å€ä¿®å‰ªæŸ¥è©¢")
    
    # 11. é èšåˆå„ªåŒ–
    print("\n1ï¸âƒ£1ï¸âƒ£ é èšåˆå„ªåŒ–")
    
    # é èšåˆæ•¸æ“š
    pre_aggregated = df.groupBy("category", "status") \
        .agg(count("*").alias("count"), avg("value").alias("avg_value")) \
        .cache()
    
    # ä½¿ç”¨é èšåˆæ•¸æ“š
    def with_pre_aggregation():
        return pre_aggregated.filter(col("status") == "active").agg(spark_sum("count")).collect()
    
    _, pre_agg_time = measure_execution_time(with_pre_aggregation, "é èšåˆæŸ¥è©¢")
    
    # 12. åˆ—å¼å­˜å„²å„ªåŒ–
    print("\n1ï¸âƒ£2ï¸âƒ£ åˆ—å¼å­˜å„²å„ªåŒ–")
    
    # å¯«å…¥ Parquet æ ¼å¼
    parquet_path = os.path.join(temp_dir, "parquet_data")
    df.write.mode("overwrite").parquet(parquet_path)
    
    # è®€å– Parquet ä¸¦æ¸¬è©¦åˆ—ä¿®å‰ª
    parquet_df = spark.read.parquet(parquet_path)
    
    def column_pruning():
        return parquet_df.select("id", "value").filter(col("value") > 500).count()
    
    _, column_pruning_time = measure_execution_time(column_pruning, "åˆ—ä¿®å‰ªæŸ¥è©¢")
    
    # 13. é€£æ¥å„ªåŒ–
    print("\n1ï¸âƒ£3ï¸âƒ£ é€£æ¥å„ªåŒ–")
    
    # å‰µå»ºå…©å€‹è¡¨é€²è¡Œé€£æ¥
    df1 = spark.range(10000).withColumn("key", col("id") % 1000)
    df2 = spark.range(1000).withColumn("key", col("id")).withColumn("info", lit("info"))
    
    # æ’åºåˆä½µé€£æ¥
    def sort_merge_join():
        return df1.join(df2, "key", "left").count()
    
    _, smj_time = measure_execution_time(sort_merge_join, "æ’åºåˆä½µé€£æ¥")
    
    # å»£æ’­é›œæ¹Šé€£æ¥
    def broadcast_hash_join():
        return df1.join(broadcast(df2), "key", "left").count()
    
    _, bhj_time = measure_execution_time(broadcast_hash_join, "å»£æ’­é›œæ¹Šé€£æ¥")
    
    print(f"é€£æ¥å„ªåŒ–æ”¹å–„: {((smj_time - bhj_time) / smj_time * 100):.1f}%")
    
    # 14. å‹•æ…‹åˆ†å€ä¿®å‰ª
    print("\n1ï¸âƒ£4ï¸âƒ£ å‹•æ…‹åˆ†å€ä¿®å‰ª")
    
    # å‰µå»ºç¶­åº¦è¡¨
    dim_table = spark.range(10).withColumn("status", when(col("id") % 2 == 0, "active").otherwise("inactive"))
    
    # å‹•æ…‹åˆ†å€ä¿®å‰ªæŸ¥è©¢
    def dynamic_partition_pruning():
        return partitioned_df.join(dim_table, "status", "inner").count()
    
    _, dpp_time = measure_execution_time(dynamic_partition_pruning, "å‹•æ…‹åˆ†å€ä¿®å‰ª")
    
    # 15. æ€§èƒ½ç›£æ§
    print("\n1ï¸âƒ£5ï¸âƒ£ æ€§èƒ½ç›£æ§")
    
    # æŸ¥è©¢æ­·å²è¨˜éŒ„
    print("Spark æ‡‰ç”¨ç¨‹å¼ä¿¡æ¯:")
    print(f"æ‡‰ç”¨ç¨‹å¼åç¨±: {spark.sparkContext.appName}")
    print(f"æ‡‰ç”¨ç¨‹å¼ ID: {spark.sparkContext.applicationId}")
    print(f"Spark ç‰ˆæœ¬: {spark.version}")
    
    # ç²å–åŸ·è¡Œå™¨ä¿¡æ¯
    executor_infos = spark.sparkContext.statusTracker().getExecutorInfos()
    print(f"\nåŸ·è¡Œå™¨æ•¸é‡: {len(executor_infos)}")
    
    for executor in executor_infos:
        print(f"åŸ·è¡Œå™¨ {executor.executorId}:")
        print(f"  - ä¸»æ©Ÿ: {executor.host}")
        print(f"  - æ ¸å¿ƒæ•¸: {executor.totalCores}")
        print(f"  - å…§å­˜ä½¿ç”¨: {executor.memoryUsed}/{executor.maxMemory}")
        print(f"  - æ´»å‹•ä»»å‹™: {executor.activeTasks}")
    
    # 16. è³‡æºé…ç½®å»ºè­°
    print("\n1ï¸âƒ£6ï¸âƒ£ è³‡æºé…ç½®å»ºè­°")
    
    # è¨ˆç®—å»ºè­°çš„é…ç½®
    total_cores = sum(executor.totalCores for executor in executor_infos)
    total_memory = sum(executor.maxMemory for executor in executor_infos)
    
    print("é…ç½®å»ºè­°:")
    print(f"  - ç¸½æ ¸å¿ƒæ•¸: {total_cores}")
    print(f"  - ç¸½å…§å­˜: {total_memory / (1024**3):.2f} GB")
    print(f"  - å»ºè­°åˆ†å€æ•¸: {total_cores * 2} - {total_cores * 4}")
    print(f"  - å»ºè­°åŸ·è¡Œå™¨å…§å­˜: {total_memory / len(executor_infos) / (1024**3):.2f} GB")
    
    # 17. æ¸…ç†è³‡æº
    print("\n1ï¸âƒ£7ï¸âƒ£ æ¸…ç†è³‡æº")
    
    # æ¸…é™¤ç·©å­˜
    df_cached.unpersist()
    df_memory.unpersist()
    df_memory_disk.unpersist()
    df_serialized.unpersist()
    pre_aggregated.unpersist()
    
    # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
    import shutil
    shutil.rmtree(temp_dir)
    
    print("è³‡æºæ¸…ç†å®Œæˆ")
    
    # 18. æ€§èƒ½æœ€ä½³å¯¦è¸ç¸½çµ
    print("\n1ï¸âƒ£8ï¸âƒ£ æ€§èƒ½æœ€ä½³å¯¦è¸ç¸½çµ")
    
    best_practices = [
        "âœ… é©ç•¶çš„åˆ†å€ç­–ç•¥ï¼ˆé¿å…å°æ–‡ä»¶å•é¡Œï¼‰",
        "âœ… åˆç†ä½¿ç”¨ç·©å­˜ï¼ˆé¿å…éåº¦ç·©å­˜ï¼‰",
        "âœ… å»£æ’­å°è¡¨é€²è¡Œé€£æ¥",
        "âœ… è™•ç†æ•¸æ“šå‚¾æ–œå•é¡Œ",
        "âœ… å•Ÿç”¨è‡ªé©æ‡‰æŸ¥è©¢åŸ·è¡Œ (AQE)",
        "âœ… ä½¿ç”¨åˆ—å¼å­˜å„²æ ¼å¼ï¼ˆå¦‚ Parquetï¼‰",
        "âœ… é èšåˆé »ç¹æŸ¥è©¢çš„æ•¸æ“š",
        "âœ… å„ªåŒ–åºåˆ—åŒ–é…ç½®",
        "âœ… èª¿æ•´å…§å­˜åˆ†é…",
        "âœ… ç›£æ§å’Œèª¿å„ªè³‡æºä½¿ç”¨"
    ]
    
    print("æ€§èƒ½å„ªåŒ–æœ€ä½³å¯¦è¸:")
    for practice in best_practices:
        print(f"  {practice}")
    
    # åœæ­¢ SparkSession
    spark.stop()
    print("\nâœ… Spark æ€§èƒ½èª¿å„ªç¤ºç¯„å®Œæˆ")

if __name__ == "__main__":
    main()