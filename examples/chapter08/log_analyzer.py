#!/usr/bin/env python3
"""
ç¬¬8ç« ï¼šå¯¦æˆ°é …ç›® - æ—¥èªŒåˆ†æç³»çµ±
æ§‹å»ºä¸€å€‹å®Œæ•´çš„æ—¥èªŒåˆ†æç³»çµ±ï¼Œå±•ç¤º Spark åœ¨å¯¦éš›é …ç›®ä¸­çš„æ‡‰ç”¨
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
from pyspark.sql.functions import (
    col, regexp_extract, to_timestamp, date_format, hour, dayofweek,
    count, sum as spark_sum, avg, max as spark_max, min as spark_min,
    desc, asc, when, isnan, isnull, split, size, collect_list, window,
    current_timestamp, date_sub, lit, monotonically_increasing_id
)
import re
import tempfile
import os
import json
from datetime import datetime, timedelta
import random

class LogAnalyzer:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.temp_dir = tempfile.mkdtemp()
        print(f"è‡¨æ™‚ç›®éŒ„: {self.temp_dir}")
        
    def generate_sample_logs(self, num_logs=10000):
        """ç”Ÿæˆç¤ºä¾‹æ—¥èªŒæ•¸æ“š"""
        
        # æ—¥èªŒæ¨¡å¼é…ç½®
        ips = [f"192.168.{i}.{j}" for i in range(1, 11) for j in range(1, 26)]
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36",
            "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X)",
            "Mozilla/5.0 (Android 11; Mobile; rv:68.0) Gecko/68.0"
        ]
        
        methods = ["GET", "POST", "PUT", "DELETE"]
        paths = [
            "/", "/home", "/login", "/logout", "/api/users", "/api/products",
            "/api/orders", "/search", "/profile", "/settings", "/admin",
            "/static/css/style.css", "/static/js/app.js", "/favicon.ico"
        ]
        
        status_codes = [200, 200, 200, 200, 200, 301, 302, 400, 401, 403, 404, 500, 502, 503]
        
        logs = []
        base_time = datetime.now() - timedelta(days=7)
        
        for i in range(num_logs):
            # ç”Ÿæˆæ™‚é–“æˆ³ï¼ˆéå»7å¤©å…§ï¼‰
            timestamp = base_time + timedelta(
                days=random.randint(0, 6),
                hours=random.randint(0, 23),
                minutes=random.randint(0, 59),
                seconds=random.randint(0, 59)
            )
            
            # ç”Ÿæˆæ—¥èªŒå­—æ®µ
            ip = random.choice(ips)
            method = random.choice(methods)
            path = random.choice(paths)
            status = random.choice(status_codes)
            size = random.randint(100, 10000)
            user_agent = random.choice(user_agents)
            
            # ç”Ÿæˆæ—¥èªŒè¡Œï¼ˆCommon Log Formatï¼‰
            log_line = f'{ip} - - [{timestamp.strftime("%d/%b/%Y:%H:%M:%S %z")}] "{method} {path} HTTP/1.1" {status} {size} "-" "{user_agent}"'
            logs.append(log_line)
        
        # å¯«å…¥æ—¥èªŒæ–‡ä»¶
        log_file = os.path.join(self.temp_dir, "access.log")
        with open(log_file, 'w') as f:
            for log in logs:
                f.write(log + '\n')
        
        print(f"ç”Ÿæˆäº† {num_logs} æ¢æ—¥èªŒè¨˜éŒ„")
        return log_file
    
    def parse_logs(self, log_file):
        """è§£ææ—¥èªŒæ–‡ä»¶"""
        
        # è®€å–åŸå§‹æ—¥èªŒ
        raw_logs = self.spark.read.text(log_file)
        
        print("åŸå§‹æ—¥èªŒæ¨£æœ¬:")
        raw_logs.show(5, truncate=False)
        
        # å®šç¾©æ—¥èªŒæ ¼å¼æ­£å‰‡è¡¨é”å¼ (Common Log Format)
        log_pattern = r'^(\S+) \S+ \S+ \[(.*?)\] "(\S+) (\S+) \S+" (\d+) (\d+) "([^"]*)" "([^"]*)"'
        
        # è§£ææ—¥èªŒ
        parsed_logs = raw_logs.select(
            regexp_extract(col("value"), log_pattern, 1).alias("ip"),
            regexp_extract(col("value"), log_pattern, 2).alias("timestamp_str"),
            regexp_extract(col("value"), log_pattern, 3).alias("method"),
            regexp_extract(col("value"), log_pattern, 4).alias("path"),
            regexp_extract(col("value"), log_pattern, 5).cast(IntegerType()).alias("status"),
            regexp_extract(col("value"), log_pattern, 6).cast(IntegerType()).alias("size"),
            regexp_extract(col("value"), log_pattern, 7).alias("referer"),
            regexp_extract(col("value"), log_pattern, 8).alias("user_agent")
        ).filter(col("ip") != "")  # éæ¿¾è§£æå¤±æ•—çš„è¨˜éŒ„
        
        # è½‰æ›æ™‚é–“æˆ³
        parsed_logs = parsed_logs.withColumn(
            "timestamp",
            to_timestamp(col("timestamp_str"), "dd/MMM/yyyy:HH:mm:ss Z")
        )
        
        # æ·»åŠ æ™‚é–“ç¶­åº¦
        parsed_logs = parsed_logs.withColumn("hour", hour(col("timestamp"))) \
                                 .withColumn("day_of_week", dayofweek(col("timestamp"))) \
                                 .withColumn("date", date_format(col("timestamp"), "yyyy-MM-dd"))
        
        print("è§£æå¾Œçš„æ—¥èªŒ:")
        parsed_logs.show(5, truncate=False)
        parsed_logs.printSchema()
        
        return parsed_logs
    
    def data_quality_check(self, df):
        """æ•¸æ“šå“è³ªæª¢æŸ¥"""
        
        print("\nğŸ“Š æ•¸æ“šå“è³ªæª¢æŸ¥")
        print("=" * 30)
        
        # åŸºæœ¬çµ±è¨ˆ
        total_records = df.count()
        print(f"ç¸½è¨˜éŒ„æ•¸: {total_records}")
        
        # æª¢æŸ¥ç©ºå€¼
        null_counts = df.select([
            col(c).isNull().cast("int").alias(c) for c in df.columns
        ]).agg(*[
            spark_sum(col(c)).alias(f"{c}_nulls") for c in df.columns
        ]).collect()[0]
        
        print("\nç©ºå€¼çµ±è¨ˆ:")
        for col_name in df.columns:
            null_count = null_counts[f"{col_name}_nulls"]
            if null_count > 0:
                print(f"  {col_name}: {null_count} ({null_count/total_records*100:.2f}%)")
        
        # æª¢æŸ¥ç•°å¸¸å€¼
        print("\nç•°å¸¸å€¼æª¢æŸ¥:")
        
        # ç‹€æ…‹ç¢¼åˆ†å¸ƒ
        status_dist = df.groupBy("status").count().orderBy(desc("count"))
        print("ç‹€æ…‹ç¢¼åˆ†å¸ƒ:")
        status_dist.show()
        
        # ç•°å¸¸ç‹€æ…‹ç¢¼
        error_logs = df.filter(col("status") >= 400)
        error_count = error_logs.count()
        print(f"éŒ¯èª¤æ—¥èªŒæ•¸é‡: {error_count} ({error_count/total_records*100:.2f}%)")
        
        # è«‹æ±‚å¤§å°ç•°å¸¸
        size_stats = df.select("size").describe()
        print("è«‹æ±‚å¤§å°çµ±è¨ˆ:")
        size_stats.show()
        
        return df
    
    def basic_analysis(self, df):
        """åŸºæœ¬åˆ†æ"""
        
        print("\nğŸ“ˆ åŸºæœ¬åˆ†æ")
        print("=" * 20)
        
        # 1. è¨ªå•é‡çµ±è¨ˆ
        print("1. è¨ªå•é‡çµ±è¨ˆ")
        
        # ç¸½è¨ªå•é‡
        total_requests = df.count()
        print(f"ç¸½è«‹æ±‚æ•¸: {total_requests}")
        
        # æ¯æ—¥è¨ªå•é‡
        daily_requests = df.groupBy("date") \
                           .agg(count("*").alias("requests")) \
                           .orderBy("date")
        
        print("æ¯æ—¥è¨ªå•é‡:")
        daily_requests.show()
        
        # æ¯å°æ™‚è¨ªå•é‡
        hourly_requests = df.groupBy("hour") \
                            .agg(count("*").alias("requests")) \
                            .orderBy("hour")
        
        print("æ¯å°æ™‚è¨ªå•é‡:")
        hourly_requests.show()
        
        # 2. ç†±é–€é é¢
        print("\n2. ç†±é–€é é¢")
        
        top_pages = df.groupBy("path") \
                      .agg(count("*").alias("requests")) \
                      .orderBy(desc("requests"))
        
        print("ç†±é–€é é¢ Top 10:")
        top_pages.show(10)
        
        # 3. ç”¨æˆ¶è¡Œç‚ºåˆ†æ
        print("\n3. ç”¨æˆ¶è¡Œç‚ºåˆ†æ")
        
        # ç¨ç«‹è¨ªå®¢
        unique_visitors = df.select("ip").distinct().count()
        print(f"ç¨ç«‹è¨ªå®¢æ•¸: {unique_visitors}")
        
        # æœ€æ´»èºç”¨æˆ¶
        top_users = df.groupBy("ip") \
                      .agg(count("*").alias("requests")) \
                      .orderBy(desc("requests"))
        
        print("æœ€æ´»èºç”¨æˆ¶ Top 10:")
        top_users.show(10)
        
        # 4. ç€è¦½å™¨åˆ†æ
        print("\n4. ç€è¦½å™¨åˆ†æ")
        
        # æå–ç€è¦½å™¨ä¿¡æ¯
        browser_df = df.withColumn("browser", 
                                  when(col("user_agent").contains("Chrome"), "Chrome")
                                  .when(col("user_agent").contains("Firefox"), "Firefox")
                                  .when(col("user_agent").contains("Safari"), "Safari")
                                  .when(col("user_agent").contains("Edge"), "Edge")
                                  .otherwise("Other"))
        
        browser_stats = browser_df.groupBy("browser") \
                                  .agg(count("*").alias("requests")) \
                                  .orderBy(desc("requests"))
        
        print("ç€è¦½å™¨åˆ†å¸ƒ:")
        browser_stats.show()
        
        return df
    
    def advanced_analysis(self, df):
        """é€²éšåˆ†æ"""
        
        print("\nğŸ” é€²éšåˆ†æ")
        print("=" * 20)
        
        # 1. éŒ¯èª¤åˆ†æ
        print("1. éŒ¯èª¤åˆ†æ")
        
        # éŒ¯èª¤æ—¥èªŒçµ±è¨ˆ
        error_analysis = df.filter(col("status") >= 400) \
                           .groupBy("status", "path") \
                           .agg(count("*").alias("error_count")) \
                           .orderBy(desc("error_count"))
        
        print("éŒ¯èª¤çµ±è¨ˆ:")
        error_analysis.show(20)
        
        # 404 éŒ¯èª¤åˆ†æ
        not_found = df.filter(col("status") == 404) \
                      .groupBy("path") \
                      .agg(count("*").alias("not_found_count")) \
                      .orderBy(desc("not_found_count"))
        
        print("404 éŒ¯èª¤é é¢:")
        not_found.show(10)
        
        # 2. æ€§èƒ½åˆ†æ
        print("\n2. æ€§èƒ½åˆ†æ")
        
        # éŸ¿æ‡‰å¤§å°çµ±è¨ˆ
        size_analysis = df.groupBy("status") \
                          .agg(avg("size").alias("avg_size"),
                               spark_max("size").alias("max_size"),
                               spark_min("size").alias("min_size")) \
                          .orderBy("status")
        
        print("éŸ¿æ‡‰å¤§å°çµ±è¨ˆ:")
        size_analysis.show()
        
        # å¤§è«‹æ±‚åˆ†æ
        large_requests = df.filter(col("size") > 5000) \
                           .groupBy("path") \
                           .agg(count("*").alias("large_request_count"),
                                avg("size").alias("avg_size")) \
                           .orderBy(desc("large_request_count"))
        
        print("å¤§è«‹æ±‚åˆ†æ:")
        large_requests.show(10)
        
        # 3. æ™‚é–“åºåˆ—åˆ†æ
        print("\n3. æ™‚é–“åºåˆ—åˆ†æ")
        
        # æŒ‰å°æ™‚çš„æµé‡æ¨¡å¼
        traffic_pattern = df.groupBy("hour", "day_of_week") \
                            .agg(count("*").alias("requests")) \
                            .orderBy("day_of_week", "hour")
        
        print("æµé‡æ¨¡å¼ï¼ˆæŒ‰å°æ™‚å’Œæ˜ŸæœŸï¼‰:")
        traffic_pattern.show(50)
        
        # 4. ç”¨æˆ¶æœƒè©±åˆ†æ
        print("\n4. ç”¨æˆ¶æœƒè©±åˆ†æ")
        
        # ç”¨æˆ¶æœƒè©±çµ±è¨ˆï¼ˆç°¡åŒ–ç‰ˆï¼‰
        user_sessions = df.groupBy("ip") \
                          .agg(count("*").alias("page_views"),
                               spark_max("timestamp").alias("last_visit"),
                               spark_min("timestamp").alias("first_visit")) \
                          .withColumn("session_duration", 
                                    col("last_visit").cast("long") - col("first_visit").cast("long"))
        
        print("ç”¨æˆ¶æœƒè©±çµ±è¨ˆ:")
        user_sessions.show(10)
        
        return df
    
    def real_time_monitoring(self, df):
        """å¯¦æ™‚ç›£æ§æŒ‡æ¨™"""
        
        print("\nğŸ“Š å¯¦æ™‚ç›£æ§æŒ‡æ¨™")
        print("=" * 25)
        
        # 1. é—œéµæŒ‡æ¨™
        metrics = df.agg(
            count("*").alias("total_requests"),
            (spark_sum(when(col("status") >= 400, 1).otherwise(0)) * 100.0 / count("*")).alias("error_rate"),
            (spark_sum(when(col("status") >= 500, 1).otherwise(0)) * 100.0 / count("*")).alias("server_error_rate"),
            avg("size").alias("avg_response_size"),
            spark_max("size").alias("max_response_size")
        )
        
        print("é—œéµæŒ‡æ¨™:")
        metrics.show()
        
        # 2. ç•°å¸¸æª¢æ¸¬
        print("\nç•°å¸¸æª¢æ¸¬:")
        
        # é«˜éŒ¯èª¤ç‡é é¢
        high_error_pages = df.groupBy("path") \
                             .agg(count("*").alias("total_requests"),
                                  spark_sum(when(col("status") >= 400, 1).otherwise(0)).alias("error_requests")) \
                             .withColumn("error_rate", col("error_requests") * 100.0 / col("total_requests")) \
                             .filter(col("error_rate") > 10) \
                             .orderBy(desc("error_rate"))
        
        print("é«˜éŒ¯èª¤ç‡é é¢:")
        high_error_pages.show(10)
        
        # 3. å®‰å…¨åˆ†æ
        print("\nå®‰å…¨åˆ†æ:")
        
        # å¯ç–‘IPï¼ˆé«˜é »è¨ªå•ï¼‰
        suspicious_ips = df.groupBy("ip") \
                           .agg(count("*").alias("requests")) \
                           .filter(col("requests") > 100) \
                           .orderBy(desc("requests"))
        
        print("å¯ç–‘ IPï¼ˆé«˜é »è¨ªå•ï¼‰:")
        suspicious_ips.show(10)
        
        # æ”»æ“Šæ¨¡å¼æª¢æ¸¬
        attack_patterns = df.filter(
            col("path").rlike(r".*(\.\.|script|sql|union|select|drop|delete|insert|update).*") |
            col("status").isin(401, 403)
        ).groupBy("ip", "path") \
         .agg(count("*").alias("attack_attempts")) \
         .orderBy(desc("attack_attempts"))
        
        print("æ½›åœ¨æ”»æ“Šæ¨¡å¼:")
        attack_patterns.show(10)
        
        return df
    
    def create_dashboard_data(self, df):
        """å‰µå»ºå„€è¡¨æ¿æ•¸æ“š"""
        
        print("\nğŸ“Š å„€è¡¨æ¿æ•¸æ“š")
        print("=" * 20)
        
        # 1. æ¦‚è¦½æŒ‡æ¨™
        overview = df.agg(
            count("*").alias("total_requests"),
            col("ip").countDistinct().alias("unique_visitors"),
            (spark_sum(when(col("status") >= 400, 1).otherwise(0)) * 100.0 / count("*")).alias("error_rate"),
            avg("size").alias("avg_response_size")
        )
        
        print("æ¦‚è¦½æŒ‡æ¨™:")
        overview.show()
        
        # 2. æ™‚é–“åºåˆ—æ•¸æ“š
        time_series = df.groupBy("date", "hour") \
                        .agg(count("*").alias("requests"),
                             spark_sum(when(col("status") >= 400, 1).otherwise(0)).alias("errors")) \
                        .orderBy("date", "hour")
        
        print("æ™‚é–“åºåˆ—æ•¸æ“š:")
        time_series.show(20)
        
        # 3. å°å‡ºæ•¸æ“š
        dashboard_path = os.path.join(self.temp_dir, "dashboard_data")
        
        # å°å‡ºå„ç¨®æ ¼å¼çš„æ•¸æ“š
        time_series.coalesce(1).write.mode("overwrite").option("header", "true").csv(f"{dashboard_path}/time_series")
        overview.coalesce(1).write.mode("overwrite").option("header", "true").csv(f"{dashboard_path}/overview")
        
        print(f"å„€è¡¨æ¿æ•¸æ“šå·²å°å‡ºåˆ°: {dashboard_path}")
        
        return df
    
    def generate_report(self, df):
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        
        print("\nğŸ“„ åˆ†æå ±å‘Š")
        print("=" * 20)
        
        # æ”¶é›†é—œéµæŒ‡æ¨™
        total_requests = df.count()
        unique_visitors = df.select("ip").distinct().count()
        error_rate = df.filter(col("status") >= 400).count() / total_requests * 100
        
        # ç†±é–€é é¢
        top_pages = df.groupBy("path") \
                      .agg(count("*").alias("requests")) \
                      .orderBy(desc("requests")) \
                      .limit(5) \
                      .collect()
        
        # æ™‚é–“ç¯„åœ
        time_range = df.select(
            spark_min("timestamp").alias("start_time"),
            spark_max("timestamp").alias("end_time")
        ).collect()[0]
        
        # ç”Ÿæˆå ±å‘Š
        report = {
            "summary": {
                "total_requests": total_requests,
                "unique_visitors": unique_visitors,
                "error_rate": round(error_rate, 2),
                "analysis_period": {
                    "start": str(time_range["start_time"]),
                    "end": str(time_range["end_time"])
                }
            },
            "top_pages": [
                {"path": row["path"], "requests": row["requests"]} 
                for row in top_pages
            ],
            "recommendations": [
                "ç›£æ§éŒ¯èª¤ç‡è¼ƒé«˜çš„é é¢",
                "å„ªåŒ–éŸ¿æ‡‰æ™‚é–“è¼ƒé•·çš„è«‹æ±‚",
                "åŠ å¼·å®‰å…¨é˜²è­·æ©Ÿåˆ¶",
                "è€ƒæ…®å¯¦æ–½CDNä»¥æ¸›å°‘ä¼ºæœå™¨è² è¼‰"
            ]
        }
        
        # å„²å­˜å ±å‘Š
        report_file = os.path.join(self.temp_dir, "analysis_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"åˆ†æå ±å‘Šå·²ç”Ÿæˆ: {report_file}")
        
        # é¡¯ç¤ºå ±å‘Šæ‘˜è¦
        print("\nå ±å‘Šæ‘˜è¦:")
        print(f"  åˆ†ææœŸé–“: {report['summary']['analysis_period']['start']} è‡³ {report['summary']['analysis_period']['end']}")
        print(f"  ç¸½è«‹æ±‚æ•¸: {report['summary']['total_requests']:,}")
        print(f"  ç¨ç«‹è¨ªå®¢: {report['summary']['unique_visitors']:,}")
        print(f"  éŒ¯èª¤ç‡: {report['summary']['error_rate']}%")
        print(f"  ç†±é–€é é¢: {', '.join([page['path'] for page in report['top_pages'][:3]])}")
        
        return report
    
    def cleanup(self):
        """æ¸…ç†è‡¨æ™‚æ–‡ä»¶"""
        import shutil
        shutil.rmtree(self.temp_dir)
        print(f"æ¸…ç†è‡¨æ™‚ç›®éŒ„: {self.temp_dir}")

def main():
    # å‰µå»º SparkSession
    spark = SparkSession.builder \
        .appName("Log Analyzer") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print("ğŸ“Š æ—¥èªŒåˆ†æç³»çµ±")
    print("=" * 40)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = LogAnalyzer(spark)
    
    try:
        # 1. ç”Ÿæˆç¤ºä¾‹æ•¸æ“š
        print("\n1ï¸âƒ£ ç”Ÿæˆç¤ºä¾‹æ•¸æ“š")
        log_file = analyzer.generate_sample_logs(50000)
        
        # 2. è§£ææ—¥èªŒ
        print("\n2ï¸âƒ£ è§£ææ—¥èªŒ")
        parsed_logs = analyzer.parse_logs(log_file)
        
        # 3. æ•¸æ“šå“è³ªæª¢æŸ¥
        parsed_logs = analyzer.data_quality_check(parsed_logs)
        
        # 4. åŸºæœ¬åˆ†æ
        parsed_logs = analyzer.basic_analysis(parsed_logs)
        
        # 5. é€²éšåˆ†æ
        parsed_logs = analyzer.advanced_analysis(parsed_logs)
        
        # 6. å¯¦æ™‚ç›£æ§
        parsed_logs = analyzer.real_time_monitoring(parsed_logs)
        
        # 7. å‰µå»ºå„€è¡¨æ¿æ•¸æ“š
        parsed_logs = analyzer.create_dashboard_data(parsed_logs)
        
        # 8. ç”Ÿæˆå ±å‘Š
        report = analyzer.generate_report(parsed_logs)
        
        print("\nâœ… æ—¥èªŒåˆ†æå®Œæˆ")
        
        # 9. å±•ç¤ºéƒ¨ç½²å»ºè­°
        print("\nğŸš€ éƒ¨ç½²å»ºè­°")
        print("=" * 15)
        
        deployment_tips = [
            "1. ä½¿ç”¨æµå¼è™•ç†è™•ç†å¯¦æ™‚æ—¥èªŒ",
            "2. è¨­ç½®é©ç•¶çš„åˆ†å€ç­–ç•¥ä»¥å„ªåŒ–æ€§èƒ½",
            "3. å¯¦æ–½æ•¸æ“šä¿ç•™ç­–ç•¥",
            "4. è¨­ç½®ç›£æ§å’Œè­¦å ±æ©Ÿåˆ¶",
            "5. è€ƒæ…®ä½¿ç”¨ Elasticsearch ä½œç‚ºæœç´¢å¼•æ“",
            "6. å¯¦æ–½æ•¸æ“šå®‰å…¨å’Œéš±ç§ä¿è­·æªæ–½",
            "7. å®šæœŸé€²è¡Œæ€§èƒ½èª¿å„ª"
        ]
        
        for tip in deployment_tips:
            print(f"  {tip}")
            
    finally:
        # 10. æ¸…ç†è³‡æº
        analyzer.cleanup()
        spark.stop()

if __name__ == "__main__":
    main()