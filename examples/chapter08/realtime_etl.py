#!/usr/bin/env python3
"""
ç¬¬8ç« ï¼šå¯¦æˆ°é …ç›® - å¯¦æ™‚ ETL ç³»çµ±
æ§‹å»ºä¸€å€‹å®Œæ•´çš„å¯¦æ™‚ ETL æ•¸æ“šç®¡é“ï¼Œå±•ç¤º Spark åœ¨æ•¸æ“šå·¥ç¨‹ä¸­çš„æ‡‰ç”¨
"""

import json
import os
import random
import tempfile
import threading
import time
from datetime import datetime, timedelta

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    avg, col, collect_list, count, current_timestamp, date_format,
    sum as spark_sum, when, window
)
from pyspark.sql.types import (
    DoubleType, IntegerType, StringType, StructField, StructType
)

class RealTimeETL:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.temp_dir = tempfile.mkdtemp()
        self.input_dir = os.path.join(self.temp_dir, "input")
        self.output_dir = os.path.join(self.temp_dir, "output")
        self.checkpoint_dir = os.path.join(self.temp_dir, "checkpoints")
        
        # å‰µå»ºç›®éŒ„
        os.makedirs(self.input_dir, exist_ok=True)
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        print(f"ETL å·¥ä½œç›®éŒ„: {self.temp_dir}")
        
    def generate_sample_data(self, duration_seconds=30):
        """ç”Ÿæˆç¤ºä¾‹æ•¸æ“šæµ"""
        
        # ç”¢å“ç›®éŒ„
        products = [
            {"id": 1, "name": "Laptop", "category": "Electronics", "price": 1200.00},
            {"id": 2, "name": "Phone", "category": "Electronics", "price": 800.00},
            {"id": 3, "name": "Tablet", "category": "Electronics", "price": 500.00},
            {"id": 4, "name": "Headphones", "category": "Electronics", "price": 150.00},
            {"id": 5, "name": "Watch", "category": "Electronics", "price": 300.00},
            {"id": 6, "name": "Book", "category": "Books", "price": 25.00},
            {"id": 7, "name": "Shoes", "category": "Clothing", "price": 80.00},
            {"id": 8, "name": "Shirt", "category": "Clothing", "price": 40.00}
        ]
        
        # ç”¨æˆ¶ä¿¡æ¯
        users = [
            {"id": 1, "name": "Alice", "email": "alice@example.com", "city": "New York"},
            {"id": 2, "name": "Bob", "email": "bob@example.com", "city": "Los Angeles"},
            {"id": 3, "name": "Charlie", "email": "charlie@example.com", "city": "Chicago"},
            {"id": 4, "name": "Diana", "email": "diana@example.com", "city": "Houston"},
            {"id": 5, "name": "Eve", "email": "eve@example.com", "city": "Phoenix"}
        ]
        
        def generate_events():
            for i in range(duration_seconds):
                # æ¯ç§’ç”Ÿæˆå¤šå€‹äº‹ä»¶
                for j in range(random.randint(3, 8)):
                    event_type = random.choice(["order", "product_view", "user_signup", "cart_add"])
                    
                    base_event = {
                        "event_id": f"evt_{i}_{j}",
                        "event_type": event_type,
                        "timestamp": datetime.now().isoformat(),
                        "user_id": random.choice(users)["id"]
                    }
                    
                    if event_type == "order":
                        product = random.choice(products)
                        quantity = random.randint(1, 5)
                        base_event.update({
                            "product_id": product["id"],
                            "quantity": quantity,
                            "amount": product["price"] * quantity,
                            "payment_method": random.choice(["credit_card", "paypal", "bank_transfer"])
                        })
                    elif event_type == "product_view":
                        base_event.update({
                            "product_id": random.choice(products)["id"],
                            "view_duration": random.randint(10, 300)
                        })
                    elif event_type == "user_signup":
                        base_event.update({
                            "signup_method": random.choice(["email", "google", "facebook"]),
                            "referral_source": random.choice(["organic", "paid", "social", "referral"])
                        })
                    elif event_type == "cart_add":
                        base_event.update({
                            "product_id": random.choice(products)["id"],
                            "quantity": random.randint(1, 3)
                        })
                    
                    # å¯«å…¥äº‹ä»¶æ–‡ä»¶
                    filename = f"event_{i}_{j}.json"
                    with open(os.path.join(self.input_dir, filename), 'w') as f:
                        json.dump(base_event, f)
                
                time.sleep(1)
        
        # åœ¨å¾Œå°ç·šç¨‹ä¸­ç”Ÿæˆæ•¸æ“š
        generator_thread = threading.Thread(target=generate_events)
        generator_thread.daemon = True
        generator_thread.start()
        
        return generator_thread
    
    def define_schemas(self):
        """å®šç¾©æ•¸æ“šæ¶æ§‹"""
        
        # äº‹ä»¶æ¶æ§‹
        event_schema = StructType([
            StructField("event_id", StringType(), True),
            StructField("event_type", StringType(), True),
            StructField("timestamp", StringType(), True),
            StructField("user_id", IntegerType(), True),
            StructField("product_id", IntegerType(), True),
            StructField("quantity", IntegerType(), True),
            StructField("amount", DoubleType(), True),
            StructField("payment_method", StringType(), True),
            StructField("view_duration", IntegerType(), True),
            StructField("signup_method", StringType(), True),
            StructField("referral_source", StringType(), True)
        ])
        
        return event_schema
    
    def create_streaming_source(self, schema):
        """å‰µå»ºæµæ•¸æ“šæº"""
        
        # å¾æ–‡ä»¶ç³»çµ±è®€å–JSONæµ
        streaming_df = self.spark.readStream \
            .format("json") \
            .schema(schema) \
            .option("path", self.input_dir) \
            .option("maxFilesPerTrigger", 10) \
            .load()
        
        return streaming_df
    
    def data_cleaning(self, df):
        """æ•¸æ“šæ¸…æ´—"""
        
        print("\nğŸ§¹ æ•¸æ“šæ¸…æ´—éšæ®µ")
        
        # 1. è™•ç†æ™‚é–“æˆ³
        from pyspark.sql.functions import to_timestamp
        cleaned_df = df.withColumn("event_timestamp", to_timestamp(col("timestamp")))
        
        # 2. è™•ç†ç©ºå€¼
        cleaned_df = cleaned_df.fillna({
            "quantity": 0,
            "amount": 0.0,
            "view_duration": 0,
            "payment_method": "unknown",
            "signup_method": "unknown",
            "referral_source": "unknown"
        })
        
        # 3. æ•¸æ“šé©—è­‰
        cleaned_df = cleaned_df.filter(
            col("user_id").isNotNull() & 
            col("event_type").isNotNull() &
            col("event_timestamp").isNotNull()
        )
        
        # 4. æ·»åŠ è™•ç†æ™‚é–“æˆ³
        cleaned_df = cleaned_df.withColumn("processing_time", current_timestamp())
        
        return cleaned_df
    
    def data_transformation(self, df):
        """æ•¸æ“šè½‰æ›"""
        
        print("\nğŸ”„ æ•¸æ“šè½‰æ›éšæ®µ")
        
        # 1. äº‹ä»¶é¡å‹ç‰¹å®šçš„è½‰æ›
        transformed_df = df.withColumn("is_purchase", 
                                     when(col("event_type") == "order", True).otherwise(False))
        
        # 2. æ·»åŠ æ™‚é–“ç¶­åº¦
        transformed_df = transformed_df.withColumn("hour", 
                                                 date_format(col("event_timestamp"), "HH")) \
                                     .withColumn("date", 
                                               date_format(col("event_timestamp"), "yyyy-MM-dd"))
        
        # 3. è¨ˆç®—è¡ç”ŸæŒ‡æ¨™
        transformed_df = transformed_df.withColumn("revenue", 
                                                 when(col("event_type") == "order", col("amount")).otherwise(0))
        
        # 4. æ·»åŠ åˆ†é¡æ¨™ç±¤
        transformed_df = transformed_df.withColumn("customer_segment",
                                                 when(col("amount") > 500, "premium")
                                                 .when(col("amount") > 100, "regular")
                                                 .otherwise("basic"))
        
        return transformed_df
    
    def data_enrichment(self, df):
        """æ•¸æ“šè±å¯ŒåŒ–"""
        
        print("\nğŸ“ˆ æ•¸æ“šè±å¯ŒåŒ–éšæ®µ")
        
        # 1. å‰µå»ºç¶­åº¦è¡¨
        user_dim = self.spark.createDataFrame([
            (1, "Alice", "alice@example.com", "New York", "Premium"),
            (2, "Bob", "bob@example.com", "Los Angeles", "Regular"),
            (3, "Charlie", "charlie@example.com", "Chicago", "Regular"),
            (4, "Diana", "diana@example.com", "Houston", "Premium"),
            (5, "Eve", "eve@example.com", "Phoenix", "Basic")
        ], ["user_id", "user_name", "email", "city", "tier"])
        
        product_dim = self.spark.createDataFrame([
            (1, "Laptop", "Electronics", 1200.00, "High"),
            (2, "Phone", "Electronics", 800.00, "High"),
            (3, "Tablet", "Electronics", 500.00, "Medium"),
            (4, "Headphones", "Electronics", 150.00, "Medium"),
            (5, "Watch", "Electronics", 300.00, "Medium"),
            (6, "Book", "Books", 25.00, "Low"),
            (7, "Shoes", "Clothing", 80.00, "Low"),
            (8, "Shirt", "Clothing", 40.00, "Low")
        ], ["product_id", "product_name", "category", "price", "price_tier"])
        
        # 2. è¯çµç¶­åº¦è¡¨
        enriched_df = df.join(user_dim, "user_id", "left") \
                        .join(product_dim, "product_id", "left")
        
        # 3. æ·»åŠ è¨ˆç®—åˆ—
        enriched_df = enriched_df.withColumn("is_high_value", 
                                           when(col("price_tier") == "High", True).otherwise(False))
        
        return enriched_df
    
    def create_aggregations(self, df):
        """å‰µå»ºèšåˆè¦–åœ–"""
        
        print("\nğŸ“Š èšåˆè¦–åœ–å‰µå»º")
        
        # 1. å¯¦æ™‚éŠ·å”®æŒ‡æ¨™
        sales_metrics = df.filter(col("event_type") == "order") \
                         .withWatermark("event_timestamp", "1 minute") \
                         .groupBy(window(col("event_timestamp"), "5 minutes")) \
                         .agg(
                             count("*").alias("order_count"),
                             spark_sum("amount").alias("total_revenue"),
                             avg("amount").alias("avg_order_value"),
                             col("user_id").countDistinct().alias("unique_customers")
                         )
        
        # 2. ç”¢å“åˆ†æ
        product_metrics = df.filter(col("product_id").isNotNull()) \
                           .withWatermark("event_timestamp", "1 minute") \
                           .groupBy(
                               window(col("event_timestamp"), "10 minutes"),
                               col("product_name"),
                               col("category")
                           ) \
                           .agg(
                               count("*").alias("interaction_count"),
                               spark_sum("quantity").alias("total_quantity"),
                               spark_sum("revenue").alias("product_revenue")
                           )
        
        # 3. ç”¨æˆ¶è¡Œç‚ºåˆ†æ
        user_behavior = df.withWatermark("event_timestamp", "1 minute") \
                         .groupBy(
                             window(col("event_timestamp"), "15 minutes"),
                             col("user_id"),
                             col("user_name")
                         ) \
                         .agg(
                             count("*").alias("event_count"),
                             spark_sum("revenue").alias("user_revenue"),
                             collect_list("event_type").alias("event_sequence")
                         )
        
        return sales_metrics, product_metrics, user_behavior
    
    def quality_checks(self, df):
        """æ•¸æ“šå“è³ªæª¢æŸ¥"""
        
        print("\nâœ… æ•¸æ“šå“è³ªæª¢æŸ¥")
        
        # 1. é‡è¤‡æª¢æŸ¥
        duplicate_check = df.groupBy("event_id").count().filter(col("count") > 1)
        
        # 2. ç©ºå€¼æª¢æŸ¥
        null_check = df.select([
            col(c).isNull().cast("int").alias(c) for c in df.columns
        ]).agg(*[
            spark_sum(col(c)).alias(f"{c}_nulls") for c in df.columns
        ])
        
        # 3. ç¯„åœæª¢æŸ¥
        range_check = df.select(
            spark_sum(when(col("amount") < 0, 1).otherwise(0)).alias("negative_amounts"),
            spark_sum(when(col("quantity") < 0, 1).otherwise(0)).alias("negative_quantities"),
            spark_sum(when(col("user_id") <= 0, 1).otherwise(0)).alias("invalid_user_ids")
        )
        
        return duplicate_check, null_check, range_check
    
    def setup_outputs(self, sales_metrics, product_metrics, user_behavior):
        """è¨­ç½®è¼¸å‡ºæµ"""
        
        print("\nğŸ“¤ è¨­ç½®è¼¸å‡ºæµ")
        
        # 1. æ§åˆ¶å°è¼¸å‡º
        console_query = sales_metrics.writeStream \
            .outputMode("append") \
            .format("console") \
            .option("truncate", "false") \
            .trigger(processingTime='10 seconds') \
            .queryName("sales_console") \
            .start()
        
        # 2. æ–‡ä»¶è¼¸å‡º
        file_query = product_metrics.writeStream \
            .outputMode("append") \
            .format("json") \
            .option("path", os.path.join(self.output_dir, "product_metrics")) \
            .option("checkpointLocation", os.path.join(self.checkpoint_dir, "product_checkpoint")) \
            .trigger(processingTime='15 seconds') \
            .queryName("product_file") \
            .start()
        
        # 3. å…§å­˜è¼¸å‡ºï¼ˆç”¨æ–¼æŸ¥è©¢ï¼‰
        memory_query = user_behavior.writeStream \
            .outputMode("append") \
            .format("memory") \
            .trigger(processingTime='20 seconds') \
            .queryName("user_behavior_memory") \
            .start()
        
        return console_query, file_query, memory_query
    
    def monitoring_and_alerting(self, df):
        """ç›£æ§å’Œè­¦å ±"""
        
        print("\nğŸš¨ ç›£æ§å’Œè­¦å ±")
        
        # 1. ç•°å¸¸æª¢æ¸¬
        anomalies = df.filter(
            (col("amount") > 2000) |  # é«˜åƒ¹å€¼è¨‚å–®
            (col("quantity") > 10) |   # å¤§é‡è¨‚å–®
            (col("view_duration") > 600)  # é•·æ™‚é–“ç€è¦½
        )
        
        # 2. æ¥­å‹™æŒ‡æ¨™ç›£æ§
        business_metrics = df.filter(col("event_type") == "order") \
                            .withWatermark("event_timestamp", "1 minute") \
                            .groupBy(window(col("event_timestamp"), "1 minute")) \
                            .agg(
                                count("*").alias("orders_per_minute"),
                                spark_sum("amount").alias("revenue_per_minute"),
                                avg("amount").alias("avg_order_value")
                            )
        
        # 3. ç³»çµ±å¥åº·æª¢æŸ¥
        health_check = df.withWatermark("event_timestamp", "1 minute") \
                        .groupBy(window(col("event_timestamp"), "5 minutes")) \
                        .agg(
                            count("*").alias("total_events"),
                            spark_sum(when(col("event_type").isNull(), 1).otherwise(0)).alias("malformed_events")
                        ) \
                        .withColumn("health_score", 
                                  when(col("malformed_events") / col("total_events") > 0.05, "Poor")
                                  .when(col("malformed_events") / col("total_events") > 0.01, "Fair")
                                  .otherwise("Good"))
        
        return anomalies, business_metrics, health_check
    
    def run_etl_pipeline(self):
        """é‹è¡Œå®Œæ•´çš„ ETL ç®¡é“"""
        
        print("\nğŸš€ å•Ÿå‹• ETL ç®¡é“")
        print("=" * 40)
        
        # 1. å®šç¾©æ¶æ§‹
        schema = self.define_schemas()
        
        # 2. å‰µå»ºæµæ•¸æ“šæº
        streaming_df = self.create_streaming_source(schema)
        
        # 3. æ•¸æ“šæ¸…æ´—
        cleaned_df = self.data_cleaning(streaming_df)
        
        # 4. æ•¸æ“šè½‰æ›
        transformed_df = self.data_transformation(cleaned_df)
        
        # 5. æ•¸æ“šè±å¯ŒåŒ–
        enriched_df = self.data_enrichment(transformed_df)
        
        # 6. å‰µå»ºèšåˆè¦–åœ–
        sales_metrics, product_metrics, user_behavior = self.create_aggregations(enriched_df)
        
        # 7. è¨­ç½®è¼¸å‡º
        console_query, file_query, memory_query = self.setup_outputs(sales_metrics, product_metrics, user_behavior)
        
        # 8. ç›£æ§å’Œè­¦å ±
        anomalies, business_metrics, health_check = self.monitoring_and_alerting(enriched_df)
        
        # 9. ç›£æ§æŸ¥è©¢
        monitoring_query = health_check.writeStream \
            .outputMode("append") \
            .format("console") \
            .option("truncate", "false") \
            .trigger(processingTime='30 seconds') \
            .queryName("health_monitoring") \
            .start()
        
        return [console_query, file_query, memory_query, monitoring_query]
    
    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        import shutil
        shutil.rmtree(self.temp_dir)
        print(f"æ¸…ç†å·¥ä½œç›®éŒ„: {self.temp_dir}")

def main():
    # å‰µå»º SparkSession
    spark = SparkSession.builder \
        .appName("Real-time ETL Pipeline") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.streaming.checkpointLocation", "/tmp/etl_checkpoints") \
        .config("spark.sql.streaming.stateStore.maintenanceInterval", "60s") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print("ğŸ”„ å¯¦æ™‚ ETL ç³»çµ±")
    print("=" * 30)
    
    # åˆå§‹åŒ– ETL ç³»çµ±
    etl = RealTimeETL(spark)
    
    try:
        # 1. å•Ÿå‹•æ•¸æ“šç”Ÿæˆå™¨
        print("\n1ï¸âƒ£ å•Ÿå‹•æ•¸æ“šç”Ÿæˆå™¨")
        generator_thread = etl.generate_sample_data(60)  # ç”Ÿæˆ60ç§’çš„æ•¸æ“š
        
        # 2. å•Ÿå‹• ETL ç®¡é“
        print("\n2ï¸âƒ£ å•Ÿå‹• ETL ç®¡é“")
        queries = etl.run_etl_pipeline()
        
        # 3. è®“ç®¡é“é‹è¡Œä¸€æ®µæ™‚é–“
        print("\n3ï¸âƒ£ ETL ç®¡é“é‹è¡Œä¸­...")
        time.sleep(90)  # é‹è¡Œ90ç§’
        
        # 4. æŸ¥çœ‹å…§å­˜è¡¨çµæœ
        print("\n4ï¸âƒ£ æŸ¥çœ‹è™•ç†çµæœ")
        try:
            user_behavior_df = spark.sql("SELECT * FROM user_behavior_memory")
            print("ç”¨æˆ¶è¡Œç‚ºåˆ†æçµæœ:")
            user_behavior_df.show(10, truncate=False)
        except Exception as e:
            print(f"æŸ¥çœ‹å…§å­˜è¡¨æ™‚å‡ºéŒ¯: {e}")
        
        # 5. æŸ¥çœ‹æŸ¥è©¢ç‹€æ…‹
        print("\n5ï¸âƒ£ æŸ¥è©¢ç‹€æ…‹")
        for i, query in enumerate(queries):
            if query.isActive:
                print(f"æŸ¥è©¢ {i+1}: {query.name} - æ´»å‹•ä¸­")
                print(f"  æœ€å¾Œé€²åº¦: {query.lastProgress}")
            else:
                print(f"æŸ¥è©¢ {i+1}: {query.name} - å·²åœæ­¢")
        
        # 6. æ€§èƒ½æŒ‡æ¨™
        print("\n6ï¸âƒ£ æ€§èƒ½æŒ‡æ¨™")
        active_queries = spark.streams.active
        print(f"æ´»å‹•æŸ¥è©¢æ•¸é‡: {len(active_queries)}")
        
        for query in active_queries:
            if query.lastProgress:
                batch_duration = query.lastProgress.get('durationMs', {}).get('triggerExecution', 0)
                input_rows = query.lastProgress.get('inputRowsPerSecond', 0)
                print(f"æŸ¥è©¢ {query.name}:")
                print(f"  æ‰¹æ¬¡åŸ·è¡Œæ™‚é–“: {batch_duration} ms")
                print(f"  è¼¸å…¥è¡Œæ•¸/ç§’: {input_rows}")
        
        # 7. åœæ­¢æ‰€æœ‰æŸ¥è©¢
        print("\n7ï¸âƒ£ åœæ­¢æŸ¥è©¢")
        for query in queries:
            if query.isActive:
                query.stop()
                print(f"å·²åœæ­¢æŸ¥è©¢: {query.name}")
        
        # 8. å±•ç¤ºæ¶æ§‹åœ–
        print("\n8ï¸âƒ£ ETL æ¶æ§‹ç¸½çµ")
        print("=" * 25)
        
        architecture = """
        æ•¸æ“šæº â†’ æ•¸æ“šæ¸…æ´— â†’ æ•¸æ“šè½‰æ› â†’ æ•¸æ“šè±å¯ŒåŒ– â†’ èšåˆè™•ç† â†’ è¼¸å‡º
           â”‚         â”‚         â”‚           â”‚           â”‚         â”‚
           â”‚         â”‚         â”‚           â”‚           â”‚         â”œâ”€ æ§åˆ¶å°
           â”‚         â”‚         â”‚           â”‚           â”‚         â”œâ”€ æ–‡ä»¶
           â”‚         â”‚         â”‚           â”‚           â”‚         â””â”€ å…§å­˜è¡¨
           â”‚         â”‚         â”‚           â”‚           â”‚
           â”‚         â”‚         â”‚           â”‚           â””â”€ ç›£æ§è­¦å ±
           â”‚         â”‚         â”‚           â”‚
           â”‚         â”‚         â”‚           â””â”€ ç¶­åº¦è¡¨è¯çµ
           â”‚         â”‚         â”‚
           â”‚         â”‚         â””â”€ æ¥­å‹™é‚è¼¯è½‰æ›
           â”‚         â”‚
           â”‚         â””â”€ å“è³ªæª¢æŸ¥
           â”‚
           â””â”€ æµå¼æ•¸æ“šæ”å–
        """
        
        print(architecture)
        
        # 9. æœ€ä½³å¯¦è¸å»ºè­°
        print("\n9ï¸âƒ£ æœ€ä½³å¯¦è¸å»ºè­°")
        print("=" * 25)
        
        best_practices = [
            "âœ… ä½¿ç”¨ Watermark è™•ç†å»¶é²æ•¸æ“š",
            "âœ… é©ç•¶è¨­ç½®æª¢æŸ¥é»ç›®éŒ„",
            "âœ… ç›£æ§æµè™•ç†æ€§èƒ½æŒ‡æ¨™",
            "âœ… å¯¦æ–½æ•¸æ“šå“è³ªæª¢æŸ¥",
            "âœ… è¨­ç½®é©ç•¶çš„è§¸ç™¼é–“éš”",
            "âœ… ä½¿ç”¨é©ç•¶çš„è¼¸å‡ºæ¨¡å¼",
            "âœ… å¯¦æ–½éŒ¯èª¤è™•ç†å’Œæ¢å¾©æ©Ÿåˆ¶",
            "âœ… å®šæœŸæ¸…ç†æ­·å²æ•¸æ“š"
        ]
        
        for practice in best_practices:
            print(f"  {practice}")
        
        print("\nâœ… å¯¦æ™‚ ETL ç³»çµ±æ¼”ç¤ºå®Œæˆ")
        
    except Exception as e:
        print(f"âŒ ETL ç®¡é“åŸ·è¡ŒéŒ¯èª¤: {e}")
        
    finally:
        # æ¸…ç†è³‡æº
        etl.cleanup()
        spark.stop()

if __name__ == "__main__":
    main()