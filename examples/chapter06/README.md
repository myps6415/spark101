# ç¬¬6ç« ï¼šMLlib æ©Ÿå™¨å­¸ç¿’

## ğŸ“š å­¸ç¿’ç›®æ¨™

- ç†è§£ Spark MLlib çš„æ ¸å¿ƒæ¦‚å¿µ
- æŒæ¡ç‰¹å¾µå·¥ç¨‹å’Œæ•¸æ“šé è™•ç†
- å­¸æœƒä½¿ç”¨å„ç¨®æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•
- äº†è§£æ¨¡å‹è©•ä¼°å’Œèª¿å„ªæŠ€å·§

## ğŸ¯ æœ¬ç« å…§å®¹

### æ ¸å¿ƒæ¦‚å¿µ
- **MLlib** - Spark æ©Ÿå™¨å­¸ç¿’åº«
- **Pipeline** - æ©Ÿå™¨å­¸ç¿’ç®¡é“
- **Feature Engineering** - ç‰¹å¾µå·¥ç¨‹
- **Model Evaluation** - æ¨¡å‹è©•ä¼°

### æª”æ¡ˆèªªæ˜
- `mllib_basics.py` - æ©Ÿå™¨å­¸ç¿’åŸºç¤æ“ä½œ

## ğŸš€ é–‹å§‹å­¸ç¿’

### åŸ·è¡Œç¯„ä¾‹

```bash
# åŸ·è¡Œæ©Ÿå™¨å­¸ç¿’åŸºç¤ç¯„ä¾‹
poetry run python examples/chapter06/mllib_basics.py

# æˆ–ä½¿ç”¨ Makefile
make run-chapter06
```

## ğŸ” æ·±å…¥ç†è§£

### MLlib æ¶æ§‹

```
åŸå§‹æ•¸æ“š â†’ ç‰¹å¾µå·¥ç¨‹ â†’ æ¨¡å‹è¨“ç·´ â†’ æ¨¡å‹è©•ä¼° â†’ æ¨¡å‹éƒ¨ç½²
    â†“         â†“         â†“         â†“         â†“
  DataFrame  Vector   Algorithm  Metrics  Pipeline
```

### ä¸»è¦çµ„ä»¶

#### 1. ç‰¹å¾µå·¥ç¨‹
```python
from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler

# å‘é‡åŒ–
assembler = VectorAssembler(
    inputCols=["age", "salary", "experience"],
    outputCol="features"
)

# å­—ä¸²ç´¢å¼•
indexer = StringIndexer(
    inputCol="category",
    outputCol="category_index"
)

# æ¨™æº–åŒ–
scaler = StandardScaler(
    inputCol="features",
    outputCol="scaled_features"
)
```

#### 2. æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•
```python
from pyspark.ml.classification import LogisticRegression, RandomForestClassifier
from pyspark.ml.regression import LinearRegression, RandomForestRegressor

# é‚è¼¯å›æ­¸
lr = LogisticRegression(
    featuresCol="features",
    labelCol="label"
)

# éš¨æ©Ÿæ£®æ—
rf = RandomForestClassifier(
    featuresCol="features",
    labelCol="label",
    numTrees=100
)
```

#### 3. æ¨¡å‹è©•ä¼°
```python
from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator

# äºŒå…ƒåˆ†é¡è©•ä¼°
evaluator = BinaryClassificationEvaluator(
    rawPredictionCol="prediction",
    labelCol="label",
    metricName="areaUnderROC"
)

# å›æ­¸è©•ä¼°
evaluator = RegressionEvaluator(
    predictionCol="prediction",
    labelCol="label",
    metricName="rmse"
)
```

## ğŸ› ï¸ æ©Ÿå™¨å­¸ç¿’ç®¡é“

### 1. å‰µå»ºç®¡é“
```python
from pyspark.ml import Pipeline

# å®šç¾©ç®¡é“éšæ®µ
pipeline = Pipeline(stages=[
    assembler,      # ç‰¹å¾µå‘é‡åŒ–
    scaler,         # æ¨™æº–åŒ–
    lr             # æ¨¡å‹è¨“ç·´
])

# è¨“ç·´ç®¡é“
model = pipeline.fit(training_data)

# é æ¸¬
predictions = model.transform(test_data)
```

### 2. ç®¡é“çš„å„ªå‹¢
- **é‡è¤‡ä½¿ç”¨** - å¯ä»¥æ‡‰ç”¨åˆ°æ–°æ•¸æ“š
- **ä¸€è‡´æ€§** - ç¢ºä¿ç›¸åŒçš„è™•ç†æ­¥é©Ÿ
- **æ¨¡çµ„åŒ–** - æ˜“æ–¼ç¶­è­·å’Œä¿®æ”¹

## ğŸ“Š å¸¸ç”¨æ¼”ç®—æ³•

### 1. åˆ†é¡ç®—æ³•

#### é‚è¼¯å›æ­¸
```python
from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(
    featuresCol="features",
    labelCol="label",
    maxIter=10,
    regParam=0.01
)
```

#### æ±ºç­–æ¨¹
```python
from pyspark.ml.classification import DecisionTreeClassifier

dt = DecisionTreeClassifier(
    featuresCol="features",
    labelCol="label",
    maxDepth=5
)
```

#### éš¨æ©Ÿæ£®æ—
```python
from pyspark.ml.classification import RandomForestClassifier

rf = RandomForestClassifier(
    featuresCol="features",
    labelCol="label",
    numTrees=100,
    maxDepth=5
)
```

### 2. å›æ­¸ç®—æ³•

#### ç·šæ€§å›æ­¸
```python
from pyspark.ml.regression import LinearRegression

lr = LinearRegression(
    featuresCol="features",
    labelCol="label",
    maxIter=10,
    regParam=0.3
)
```

#### æ±ºç­–æ¨¹å›æ­¸
```python
from pyspark.ml.regression import DecisionTreeRegressor

dt = DecisionTreeRegressor(
    featuresCol="features",
    labelCol="label",
    maxDepth=5
)
```

### 3. èšé¡ç®—æ³•

#### K-means
```python
from pyspark.ml.clustering import KMeans

kmeans = KMeans(
    featuresCol="features",
    predictionCol="cluster",
    k=3,
    seed=42
)
```

#### é«˜æ–¯æ··åˆæ¨¡å‹
```python
from pyspark.ml.clustering import GaussianMixture

gmm = GaussianMixture(
    featuresCol="features",
    predictionCol="cluster",
    k=3
)
```

## ğŸ”§ ç‰¹å¾µå·¥ç¨‹

### 1. æ•¸å€¼ç‰¹å¾µè™•ç†
```python
from pyspark.ml.feature import StandardScaler, MinMaxScaler, Normalizer

# æ¨™æº–åŒ–
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")

# æœ€å¤§æœ€å°ç¸®æ”¾
minmax_scaler = MinMaxScaler(inputCol="features", outputCol="scaled_features")

# æ­£è¦åŒ–
normalizer = Normalizer(inputCol="features", outputCol="normalized_features")
```

### 2. é¡åˆ¥ç‰¹å¾µè™•ç†
```python
from pyspark.ml.feature import StringIndexer, OneHotEncoder

# å­—ä¸²ç´¢å¼•
indexer = StringIndexer(inputCol="category", outputCol="category_index")

# ç¨ç†±ç·¨ç¢¼
encoder = OneHotEncoder(inputCol="category_index", outputCol="category_vec")
```

### 3. æ–‡æœ¬ç‰¹å¾µè™•ç†
```python
from pyspark.ml.feature import Tokenizer, HashingTF, IDF

# åˆ†è©
tokenizer = Tokenizer(inputCol="text", outputCol="words")

# å“ˆå¸Œè©é »
hashingTF = HashingTF(inputCol="words", outputCol="tf_features")

# é€†æ–‡ä»¶é »ç‡
idf = IDF(inputCol="tf_features", outputCol="tfidf_features")
```

## ğŸ“ˆ æ¨¡å‹è©•ä¼°

### 1. åˆ†é¡æŒ‡æ¨™
```python
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

evaluator = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="accuracy"
)

accuracy = evaluator.evaluate(predictions)
```

### 2. å›æ­¸æŒ‡æ¨™
```python
from pyspark.ml.evaluation import RegressionEvaluator

evaluator = RegressionEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="rmse"
)

rmse = evaluator.evaluate(predictions)
```

## ğŸ¯ è¶…åƒæ•¸èª¿å„ª

### 1. åƒæ•¸ç¶²æ ¼æœç´¢
```python
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# å‰µå»ºåƒæ•¸ç¶²æ ¼
paramGrid = ParamGridBuilder() \
    .addGrid(lr.regParam, [0.1, 0.01]) \
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \
    .build()

# äº¤å‰é©—è­‰
crossval = CrossValidator(
    estimator=lr,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    numFolds=3
)

# è¨“ç·´
cv_model = crossval.fit(training_data)
```

### 2. è¨“ç·´é©—è­‰åˆ†å‰²
```python
from pyspark.ml.tuning import TrainValidationSplit

tvs = TrainValidationSplit(
    estimator=lr,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    trainRatio=0.8
)

tvs_model = tvs.fit(training_data)
```

## ğŸ“ ç·´ç¿’å»ºè­°

### åŸºç¤ç·´ç¿’
1. å¯¦ç¾åŸºæœ¬çš„åˆ†é¡å’Œå›æ­¸ä»»å‹™
2. å˜—è©¦ä¸åŒçš„ç‰¹å¾µå·¥ç¨‹æŠ€è¡“
3. æ¯”è¼ƒä¸åŒæ¼”ç®—æ³•çš„æ€§èƒ½

### é€²éšç·´ç¿’
1. å»ºç«‹å®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’ç®¡é“
2. å¯¦æ–½äº¤å‰é©—è­‰å’Œè¶…åƒæ•¸èª¿å„ª
3. è™•ç†ä¸å¹³è¡¡æ•¸æ“šé›†

## ğŸ› ï¸ å¯¦ç”¨æŠ€å·§

### 1. æ•¸æ“šæº–å‚™
```python
# è™•ç†ç¼ºå¤±å€¼
df = df.fillna({"feature1": 0, "feature2": "unknown"})

# è™•ç†ç•°å¸¸å€¼
df = df.filter(col("feature") > 0)

# æ•¸æ“šæ¡æ¨£
sample_df = df.sample(0.1, seed=42)
```

### 2. ç‰¹å¾µé¸æ“‡
```python
from pyspark.ml.feature import ChiSqSelector

selector = ChiSqSelector(
    featuresCol="features",
    outputCol="selected_features",
    labelCol="label",
    numTopFeatures=10
)
```

### 3. æ¨¡å‹æŒä¹…åŒ–
```python
# ä¿å­˜æ¨¡å‹
model.write().overwrite().save("path/to/model")

# è¼‰å…¥æ¨¡å‹
from pyspark.ml.classification import LogisticRegressionModel
loaded_model = LogisticRegressionModel.load("path/to/model")
```

## ğŸ”§ ç–‘é›£æ’è§£

### å¸¸è¦‹å•é¡Œ

**Q: ç‰¹å¾µå‘é‡åŒ–å¤±æ•—ï¼Ÿ**
A: ç¢ºä¿æ‰€æœ‰è¼¸å…¥åˆ—éƒ½æ˜¯æ•¸å€¼å‹ï¼Œæˆ–å…ˆé€²è¡Œé©ç•¶çš„é¡å‹è½‰æ›ã€‚

**Q: æ¨¡å‹æ€§èƒ½å·®ï¼Ÿ**
A: æª¢æŸ¥ç‰¹å¾µå·¥ç¨‹ã€å˜—è©¦ä¸åŒçš„æ¼”ç®—æ³•ã€èª¿æ•´è¶…åƒæ•¸ã€‚

**Q: è¨˜æ†¶é«”ä¸è¶³ï¼Ÿ**
A: æ¸›å°‘ç‰¹å¾µç¶­åº¦ã€ä½¿ç”¨æ¡æ¨£ã€å¢åŠ åŸ·è¡Œå™¨è¨˜æ†¶é«”ã€‚

## ğŸ’¡ æœ€ä½³å¯¦è¸

1. **æ•¸æ“šæ¢ç´¢** - å……åˆ†ç†è§£æ•¸æ“šç‰¹æ€§
2. **ç‰¹å¾µå·¥ç¨‹** - æŠ•å…¥æ™‚é–“å‰µå»ºæœ‰æ„ç¾©çš„ç‰¹å¾µ
3. **æ¨¡å‹é©—è­‰** - ä½¿ç”¨äº¤å‰é©—è­‰è©•ä¼°æ¨¡å‹
4. **è¶…åƒæ•¸èª¿å„ª** - ç³»çµ±æ€§åœ°æœç´¢æœ€ä½³åƒæ•¸
5. **æ¨¡å‹è§£é‡‹** - ç†è§£æ¨¡å‹çš„æ±ºç­–é‚è¼¯

## ğŸ“– ç›¸é—œæ–‡æª”

- [MLlib Guide](https://spark.apache.org/docs/latest/ml-guide.html)
- [ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html)
- [Feature Extraction](https://spark.apache.org/docs/latest/ml-features.html)

## â¡ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬ç« å¾Œï¼Œå»ºè­°ç¹¼çºŒå­¸ç¿’ï¼š
- [ç¬¬7ç« ï¼šæ€§èƒ½èª¿å„ª](../chapter07/README.md)
- äº†è§£ Spark æ€§èƒ½å„ªåŒ–æŠ€å·§
- å­¸ç¿’é›†ç¾¤éƒ¨ç½²å’Œç›£æ§

## ğŸ¯ å­¸ç¿’æª¢æ ¸

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œä½ æ‡‰è©²èƒ½å¤ ï¼š
- [ ] ç†è§£ MLlib çš„æ ¸å¿ƒæ¦‚å¿µ
- [ ] é€²è¡Œç‰¹å¾µå·¥ç¨‹å’Œæ•¸æ“šé è™•ç†
- [ ] ä½¿ç”¨å„ç¨®æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•
- [ ] å»ºç«‹å®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’ç®¡é“
- [ ] è©•ä¼°å’Œèª¿å„ªæ¨¡å‹æ€§èƒ½

## ğŸ—‚ï¸ ç« ç¯€æ–‡ä»¶ç¸½è¦½

### mllib_basics.py
- æ©Ÿå™¨å­¸ç¿’åŸºç¤æ“ä½œ
- ç‰¹å¾µå·¥ç¨‹ç¤ºä¾‹
- å¤šç¨®æ¼”ç®—æ³•å¯¦ç¾
- æ¨¡å‹è©•ä¼°å’Œèª¿å„ª
- æ–‡æœ¬æŒ–æ˜å’Œé™ç¶­
- æ¨¡å‹æŒä¹…åŒ–