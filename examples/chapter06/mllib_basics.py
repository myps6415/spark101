#!/usr/bin/env python3
"""
ç¬¬6ç« ï¼šMLlib æ©Ÿå™¨å­¸ç¿’ - åŸºç¤Žæ©Ÿå™¨å­¸ç¿’
å­¸ç¿’ Spark MLlib çš„åŸºæœ¬æ©Ÿå™¨å­¸ç¿’ç®—æ³•å’Œç®¡é“
"""

import numpy as np
from pyspark.ml import Pipeline
from pyspark.ml.classification import (DecisionTreeClassifier,
                                       LogisticRegression,
                                       RandomForestClassifier)
from pyspark.ml.clustering import GaussianMixture, KMeans
from pyspark.ml.evaluation import (BinaryClassificationEvaluator,
                                   MulticlassClassificationEvaluator,
                                   RegressionEvaluator)
from pyspark.ml.feature import (IDF, PCA, HashingTF, MinMaxScaler,
                                OneHotEncoder, StandardScaler,
                                StopWordsRemover, StringIndexer, Tokenizer,
                                VectorAssembler)
from pyspark.ml.regression import (DecisionTreeRegressor, LinearRegression,
                                   RandomForestRegressor)
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, isnan, isnull, rand, when
from pyspark.sql.types import (DoubleType, IntegerType, StringType,
                               StructField, StructType)


def main():
    # å‰µå»º SparkSession
    spark = (
        SparkSession.builder.appName("MLlib Basics")
        .master("local[*]")
        .config("spark.sql.adaptive.enabled", "true")
        .getOrCreate()
    )

    spark.sparkContext.setLogLevel("WARN")

    print("ðŸ¤– MLlib æ©Ÿå™¨å­¸ç¿’åŸºç¤Žç¤ºç¯„")
    print("=" * 40)

    # 1. æº–å‚™æ•¸æ“š
    print("\n1ï¸âƒ£ æº–å‚™æ•¸æ“š")

    # å‰µå»ºåˆ†é¡žå•é¡Œçš„ç¤ºä¾‹æ•¸æ“š
    classification_data = [
        (1, 25, 50000, 1, 0, 1),  # age, income, experience, gender, education, target
        (2, 35, 75000, 2, 1, 1),
        (3, 45, 120000, 5, 2, 1),
        (4, 22, 35000, 0, 0, 0),
        (5, 28, 45000, 1, 1, 0),
        (6, 55, 150000, 10, 2, 1),
        (7, 19, 25000, 0, 0, 0),
        (8, 42, 95000, 8, 1, 1),
        (9, 38, 85000, 6, 2, 1),
        (10, 26, 40000, 2, 0, 0),
    ]

    # æ“´å±•æ•¸æ“šé›†
    import random

    extended_data = []
    for i in range(1000):
        age = random.randint(18, 65)
        income = random.randint(20000, 200000)
        experience = max(0, age - 22 + random.randint(-3, 3))
        gender = random.randint(0, 1)
        education = random.randint(0, 2)

        # åŸºæ–¼ç‰¹å¾µç”Ÿæˆç›®æ¨™å€¼ï¼ˆæœ‰ä¸€å®šçš„æ¨¡å¼ï¼‰
        target = (
            1
            if (income > 60000 and experience > 3)
            or (education == 2 and income > 45000)
            else 0
        )
        # æ·»åŠ ä¸€äº›éš¨æ©Ÿæ€§
        if random.random() < 0.1:
            target = 1 - target

        extended_data.append((i, age, income, experience, gender, education, target))

    schema = StructType(
        [
            StructField("id", IntegerType(), True),
            StructField("age", IntegerType(), True),
            StructField("income", IntegerType(), True),
            StructField("experience", IntegerType(), True),
            StructField("gender", IntegerType(), True),
            StructField("education", IntegerType(), True),
            StructField("target", IntegerType(), True),
        ]
    )

    df = spark.createDataFrame(extended_data, schema)
    print("åŽŸå§‹æ•¸æ“š:")
    df.show(10)
    df.printSchema()

    # æ•¸æ“šçµ±è¨ˆ
    print("æ•¸æ“šçµ±è¨ˆ:")
    df.describe().show()

    # 2. æ•¸æ“šé è™•ç†
    print("\n2ï¸âƒ£ æ•¸æ“šé è™•ç†")

    # æª¢æŸ¥ç¼ºå¤±å€¼
    print("æª¢æŸ¥ç¼ºå¤±å€¼:")
    df.select([col(c).isNull().cast("int").alias(c) for c in df.columns]).agg(
        *[col(c).sum().alias(f"{c}_nulls") for c in df.columns]
    ).show()

    # ç‰¹å¾µå·¥ç¨‹
    print("ç‰¹å¾µå·¥ç¨‹:")

    # å‰µå»ºå¹´é½¡åˆ†çµ„
    df_processed = df.withColumn(
        "age_group", when(col("age") < 30, 0).when(col("age") < 45, 1).otherwise(2)
    )

    # å‰µå»ºæ”¶å…¥åˆ†çµ„
    df_processed = df_processed.withColumn(
        "income_group",
        when(col("income") < 50000, 0).when(col("income") < 100000, 1).otherwise(2),
    )

    print("è™•ç†å¾Œçš„æ•¸æ“š:")
    df_processed.show(10)

    # 3. ç‰¹å¾µå‘é‡åŒ–
    print("\n3ï¸âƒ£ ç‰¹å¾µå‘é‡åŒ–")

    # é¸æ“‡ç‰¹å¾µåˆ—
    feature_cols = [
        "age",
        "income",
        "experience",
        "gender",
        "education",
        "age_group",
        "income_group",
    ]

    # å‰µå»ºç‰¹å¾µå‘é‡
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

    df_features = assembler.transform(df_processed)
    print("ç‰¹å¾µå‘é‡:")
    df_features.select("features", "target").show(5, truncate=False)

    # 4. æ¨™æº–åŒ–
    print("\n4ï¸âƒ£ æ¨™æº–åŒ–")

    # ç‰¹å¾µæ¨™æº–åŒ–
    scaler = StandardScaler(
        inputCol="features", outputCol="scaled_features", withStd=True, withMean=True
    )

    scaler_model = scaler.fit(df_features)
    df_scaled = scaler_model.transform(df_features)

    print("æ¨™æº–åŒ–å¾Œçš„ç‰¹å¾µ:")
    df_scaled.select("scaled_features", "target").show(5, truncate=False)

    # 5. æ•¸æ“šåˆ†å‰²
    print("\n5ï¸âƒ£ æ•¸æ“šåˆ†å‰²")

    # åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†
    train_data, test_data = df_scaled.randomSplit([0.8, 0.2], seed=42)

    print(f"è¨“ç·´é›†å¤§å°: {train_data.count()}")
    print(f"æ¸¬è©¦é›†å¤§å°: {test_data.count()}")

    # 6. é‚è¼¯å›žæ­¸
    print("\n6ï¸âƒ£ é‚è¼¯å›žæ­¸")

    # å‰µå»ºé‚è¼¯å›žæ­¸æ¨¡åž‹
    lr = LogisticRegression(
        featuresCol="scaled_features", labelCol="target", predictionCol="prediction"
    )

    # è¨“ç·´æ¨¡åž‹
    lr_model = lr.fit(train_data)

    # é æ¸¬
    lr_predictions = lr_model.transform(test_data)

    print("é‚è¼¯å›žæ­¸é æ¸¬çµæžœ:")
    lr_predictions.select("target", "prediction", "probability").show(10)

    # è©•ä¼°æ¨¡åž‹
    evaluator = BinaryClassificationEvaluator(
        labelCol="target", rawPredictionCol="rawPrediction", metricName="areaUnderROC"
    )

    lr_auc = evaluator.evaluate(lr_predictions)
    print(f"é‚è¼¯å›žæ­¸ AUC: {lr_auc:.4f}")

    # 7. æ±ºç­–æ¨¹
    print("\n7ï¸âƒ£ æ±ºç­–æ¨¹")

    # æ±ºç­–æ¨¹åˆ†é¡žå™¨
    dt = DecisionTreeClassifier(featuresCol="scaled_features", labelCol="target")

    dt_model = dt.fit(train_data)
    dt_predictions = dt_model.transform(test_data)

    dt_auc = evaluator.evaluate(dt_predictions)
    print(f"æ±ºç­–æ¨¹ AUC: {dt_auc:.4f}")

    # ç‰¹å¾µé‡è¦æ€§
    print("ç‰¹å¾µé‡è¦æ€§:")
    for i, importance in enumerate(dt_model.featureImportances.toArray()):
        print(f"  {feature_cols[i]}: {importance:.4f}")

    # 8. éš¨æ©Ÿæ£®æž—
    print("\n8ï¸âƒ£ éš¨æ©Ÿæ£®æž—")

    # éš¨æ©Ÿæ£®æž—åˆ†é¡žå™¨
    rf = RandomForestClassifier(
        featuresCol="scaled_features", labelCol="target", numTrees=10
    )

    rf_model = rf.fit(train_data)
    rf_predictions = rf_model.transform(test_data)

    rf_auc = evaluator.evaluate(rf_predictions)
    print(f"éš¨æ©Ÿæ£®æž— AUC: {rf_auc:.4f}")

    # 9. å›žæ­¸å•é¡Œ
    print("\n9ï¸âƒ£ å›žæ­¸å•é¡Œ")

    # å‰µå»ºå›žæ­¸å•é¡Œçš„æ•¸æ“šï¼ˆé æ¸¬æ”¶å…¥ï¼‰
    regression_data = df_scaled.select("scaled_features", col("income").alias("label"))

    # åˆ†å‰²æ•¸æ“š
    reg_train, reg_test = regression_data.randomSplit([0.8, 0.2], seed=42)

    # ç·šæ€§å›žæ­¸
    linear_reg = LinearRegression(featuresCol="scaled_features", labelCol="label")

    lr_reg_model = linear_reg.fit(reg_train)
    lr_reg_predictions = lr_reg_model.transform(reg_test)

    print("ç·šæ€§å›žæ­¸é æ¸¬çµæžœ:")
    lr_reg_predictions.select("label", "prediction").show(10)

    # è©•ä¼°å›žæ­¸æ¨¡åž‹
    reg_evaluator = RegressionEvaluator(
        labelCol="label", predictionCol="prediction", metricName="rmse"
    )

    lr_rmse = reg_evaluator.evaluate(lr_reg_predictions)
    print(f"ç·šæ€§å›žæ­¸ RMSE: {lr_rmse:.2f}")

    # 10. èšé¡žåˆ†æž
    print("\nðŸ”Ÿ èšé¡žåˆ†æž")

    # K-means èšé¡ž
    kmeans = KMeans(
        featuresCol="scaled_features", predictionCol="cluster", k=3, seed=42
    )

    kmeans_model = kmeans.fit(df_scaled)
    kmeans_predictions = kmeans_model.transform(df_scaled)

    print("K-means èšé¡žçµæžœ:")
    kmeans_predictions.select("cluster", "target").show(10)

    # èšé¡žä¸­å¿ƒ
    print("èšé¡žä¸­å¿ƒ:")
    centers = kmeans_model.clusterCenters()
    for i, center in enumerate(centers):
        print(f"  Cluster {i}: {center}")

    # 11. æ©Ÿå™¨å­¸ç¿’ç®¡é“
    print("\n1ï¸âƒ£1ï¸âƒ£ æ©Ÿå™¨å­¸ç¿’ç®¡é“")

    # å‰µå»ºå®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’ç®¡é“
    pipeline = Pipeline(stages=[assembler, scaler, lr])

    # è¨“ç·´ç®¡é“
    pipeline_model = pipeline.fit(train_data)

    # ä½¿ç”¨ç®¡é“é€²è¡Œé æ¸¬
    pipeline_predictions = pipeline_model.transform(test_data)

    pipeline_auc = evaluator.evaluate(pipeline_predictions)
    print(f"ç®¡é“æ¨¡åž‹ AUC: {pipeline_auc:.4f}")

    # 12. è¶…åƒæ•¸èª¿å„ª
    print("\n1ï¸âƒ£2ï¸âƒ£ è¶…åƒæ•¸èª¿å„ª")

    # å‰µå»ºåƒæ•¸ç¶²æ ¼
    paramGrid = (
        ParamGridBuilder()
        .addGrid(lr.regParam, [0.01, 0.1, 1.0])
        .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])
        .build()
    )

    # äº¤å‰é©—è­‰
    crossval = CrossValidator(
        estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3
    )

    # è¨“ç·´
    cv_model = crossval.fit(train_data)

    # æœ€ä½³æ¨¡åž‹é æ¸¬
    cv_predictions = cv_model.transform(test_data)

    cv_auc = evaluator.evaluate(cv_predictions)
    print(f"äº¤å‰é©—è­‰æœ€ä½³æ¨¡åž‹ AUC: {cv_auc:.4f}")

    # 13. æ–‡æœ¬æŒ–æŽ˜
    print("\n1ï¸âƒ£3ï¸âƒ£ æ–‡æœ¬æŒ–æŽ˜")

    # å‰µå»ºæ–‡æœ¬æ•¸æ“š
    text_data = [
        (1, "machine learning is awesome"),
        (2, "spark mllib is great for big data"),
        (3, "python and scala are popular languages"),
        (4, "data science requires good algorithms"),
        (5, "artificial intelligence is the future"),
    ]

    text_df = spark.createDataFrame(text_data, ["id", "text"])

    # æ–‡æœ¬è™•ç†ç®¡é“
    tokenizer = Tokenizer(inputCol="text", outputCol="words")
    stop_words_remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
    hashing_tf = HashingTF(inputCol="filtered_words", outputCol="tf_features")
    idf = IDF(inputCol="tf_features", outputCol="tfidf_features")

    # å»ºç«‹æ–‡æœ¬è™•ç†ç®¡é“
    text_pipeline = Pipeline(stages=[tokenizer, stop_words_remover, hashing_tf, idf])

    # è¨“ç·´ä¸¦è½‰æ›
    text_model = text_pipeline.fit(text_df)
    text_result = text_model.transform(text_df)

    print("æ–‡æœ¬ç‰¹å¾µæå–çµæžœ:")
    text_result.select("id", "text", "tfidf_features").show(truncate=False)

    # 14. é™ç¶­
    print("\n1ï¸âƒ£4ï¸âƒ£ é™ç¶­")

    # PCA é™ç¶­
    pca = PCA(k=3, inputCol="scaled_features", outputCol="pca_features")

    pca_model = pca.fit(df_scaled)
    pca_result = pca_model.transform(df_scaled)

    print("PCA é™ç¶­çµæžœ:")
    pca_result.select("pca_features").show(5, truncate=False)

    # è§£é‡‹è®Šç•°é‡
    print(f"è§£é‡‹è®Šç•°é‡: {pca_model.explainedVariance}")

    # 15. æ¨¡åž‹è©•ä¼°æŒ‡æ¨™
    print("\n1ï¸âƒ£5ï¸âƒ£ æ¨¡åž‹è©•ä¼°æŒ‡æ¨™")

    # å¤šåˆ†é¡žè©•ä¼°æŒ‡æ¨™
    multi_evaluator = MulticlassClassificationEvaluator(
        labelCol="target", predictionCol="prediction"
    )

    # æº–ç¢ºçŽ‡
    accuracy = multi_evaluator.evaluate(
        lr_predictions, {multi_evaluator.metricName: "accuracy"}
    )
    print(f"æº–ç¢ºçŽ‡: {accuracy:.4f}")

    # ç²¾ç¢ºåº¦
    precision = multi_evaluator.evaluate(
        lr_predictions, {multi_evaluator.metricName: "weightedPrecision"}
    )
    print(f"ç²¾ç¢ºåº¦: {precision:.4f}")

    # å¬å›žçŽ‡
    recall = multi_evaluator.evaluate(
        lr_predictions, {multi_evaluator.metricName: "weightedRecall"}
    )
    print(f"å¬å›žçŽ‡: {recall:.4f}")

    # F1 åˆ†æ•¸
    f1 = multi_evaluator.evaluate(lr_predictions, {multi_evaluator.metricName: "f1"})
    print(f"F1 åˆ†æ•¸: {f1:.4f}")

    # 16. ä¿å­˜å’Œè¼‰å…¥æ¨¡åž‹
    print("\n1ï¸âƒ£6ï¸âƒ£ ä¿å­˜å’Œè¼‰å…¥æ¨¡åž‹")

    # ä¿å­˜æ¨¡åž‹
    model_path = "/tmp/spark_lr_model"
    lr_model.write().overwrite().save(model_path)
    print(f"æ¨¡åž‹å·²ä¿å­˜åˆ°: {model_path}")

    # è¼‰å…¥æ¨¡åž‹
    from pyspark.ml.classification import LogisticRegressionModel

    loaded_model = LogisticRegressionModel.load(model_path)
    print("æ¨¡åž‹è¼‰å…¥æˆåŠŸ")

    # ä½¿ç”¨è¼‰å…¥çš„æ¨¡åž‹é€²è¡Œé æ¸¬
    loaded_predictions = loaded_model.transform(test_data)
    loaded_auc = evaluator.evaluate(loaded_predictions)
    print(f"è¼‰å…¥æ¨¡åž‹ AUC: {loaded_auc:.4f}")

    # åœæ­¢ SparkSession
    spark.stop()
    print("\nâœ… MLlib æ©Ÿå™¨å­¸ç¿’åŸºç¤Žç¤ºç¯„å®Œæˆ")


if __name__ == "__main__":
    main()
